\chapter{Methods}\label{chapter:method}

{ \color{red}

    about 10-30 pages (rather more I guess)

    \begin{itemize}
        \item (1/3 of thesis)
        \item start with a theoretical approach, describe the developed system/algorithm/method from a high-level point of view,
        \item go ahead in presenting your developments in more detail
    \end{itemize}
}


\begin{enumerate}
    \item Benchmark dataset dsprites
    \item adaptation with watermark and spurious-to-core feature ratio as an SCM
    \item training X models with different ratio, cutoff and learning rate on cluster
    \item computing \textit{ground-truth feature importance} of core, spurious and unbiased features: mean logit change for output, R2-score,  prediction flip
    \item baseline(?) score how much importance is generally assigned to spurious feature (bounding box?)
    \item special score for how much importance CRP assigns to concepts encoding spurious feature
    \item causal effect estimation? or something like that
\end{enumerate}


\todo{find good name for adapted benchmark dataset}
\section{Causal Benchmark Dataset DSPRITESNEWNAME}
Although this is not the first work using a toy dataset with known generating factors to evaluate attribution methods, it still uses a new adaptation of the dSprites dataset \cite{dsprites17}. This dataset was originally constructed as a means for testing the degree of disentanglement a method has achieved. It originally contains 737280 64x64 pixel binary images with rectangles, ellipses or hearts in varying positions, scales and rotations. The generating factors \verb|shape|, \verb|scale|, \verb|rotation|, \verb|x position| and \verb|y position| are known for each sample.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{pics/dsprites_examples.png}
    \caption{First row: images from the original dSprites dataset, second row: images from the new DSPRITESNEWNAME with small \textit{w} as a watermark on some images and uniform noise added.}
    \label{fig:dsprites_examples}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/simple_scm.png}
    \caption{Structural causal model generating the dataset DSPRITESNEWNAME.\\
        In the top right corner the distribution of \textit{Has Watermark} and \textit{Is Shape} are plotted against each other to explain the effect of $\rho$}
    \label{fig:simple_scm}
\end{figure}

To adapt the benchmark for our purpose, only the first two shape classes (rectangle and ellipse) are used. A watermark in the form of a small \textit{w} was initially added to the lower-left corner of some images. During initial testing with only these adaptations it became clear that even the small convolutional neural network employed here is too powerful for this task as effectively dividing the image into two parts solves the problem and most neurons became irrelevant.
To make the spurious feature, which is the watermark \textit{w} more difficult to learn, its position is therefore varied across the edges of the image. Further a small uniform noise term is added to make the problem more realistic and the saliency maps more convincing and informative.
The aim of this new dataset is, to create the simplest possible scenario with known generating factors, while keeping it as realistic or close to real world application cases of attribution methods as possible. In \autoref{fig:dsprites_examples} the resulting images are visualized.

\todo{why the heck new dataset???}
\begin{itemize}
    \item why do we need another dataset for benchmarking watermark bias??
    \item some other benchmarks that deal with similar questions are...
    \item why am i not just using 3d shapes dataset? \url{https://github.com/deepmind/3dshapes-dataset/} (C. Burgess and H. Kim)
\end{itemize}

\subsection{Causal Model}
The process with which samples are constructed from the new dataset is a structural causal model.

For the first extensive comparison the SCM as seen in \autoref{fig:simple_scm} serves as a starting point. Explicitly using a generating SCM as previously done by \cite{Parafita2019, Yang2019, Wilming2023} enables us to study the effects of interventions on the model and the explanations. The \textit{spurious-to-core ratio} variable $\rho$ adjusts how much information is shared between the true class information (shape), which we name \textit{core feature} following \cite{Singla2022} and the watermark or \textit{spurious feature} through a shared common ancestor $g$. A second parameter $p$ (= prevalence) determines how prevalent the spurious/ watermark feature is in the data. It is important to note that this particular SCM is just one of many possible ways to model how spurious features might interact with core features. It tries to follow the logic of how images are selected in real datasets: Choosing images that have a certain object/feature (here shape), without being aware of some photographers adding watermarks to their images. By some unknown confounding factor (for example personal preference) photographers who add watermarks to their images mostly upload photos of one class of objects.
The same SCM applies to many realistic cases of spurious correlations in computer vision tasks. Think of, for example, cows mostly being photographed on pastures or halloween pumpkins mostly at night, creating correlations between the object and background. However a multitude of SCMs potentially act as simplifications of real world spurious correlation scenarios. Selection bias (see bottom picture in \autoref{fig:possible_scms}), where a certain combination of features is more likely selected into the dataset and hence makes the data distribution less generalizable to the true distribution, is arguably another often occuring type of bias in computer vision datasets. The direction of causal links for photographs is highly debatable and shall not be the focus of this work. Instead, we only want to find to what degree a neural network learns and attribution method explaind a particular generating SCM.


    {\color{red}
        \begin{itemize}
            \item have latent factors - can intervene on each factor extensively, expand on usefullness of SCM
            \item for model we can also assume SCM??? all connected neurons are causally connected
            \item in given example prediction has shape AND watermark as causal ancestors
            \item but in real world example spurious feature is only selection bias?
            \item need to find good causal covering for what i am doing here
        \end{itemize}}

\begin{figure}[H]
    \centering
    \includegraphics[height=0.7\textheight]{pics/different_possible_scms.png}
    \caption{SCMs typically found in image datasets.
        1. Our SCM with counfounder $g$,
        2. spurious feature has direct causal effect on core feature,
        3. core feature has direct causal effect on spurious feature 4. Selection Bias chooses certain combinations of spurious and core feature with higher probability}
    \label{fig:possible_scms}
\end{figure}
\todo{more diverse SCMs, simplified, akin to real world scenarios}

\section{CNN Model Zoo}
\subsection{Model Architecture}
To evaluate explanations, the model to test on can neither be to simple and therefore easy to explain, nor too large for the simple dataset at hand.
Through a simple search the architecture detailed in \autoref{lst:cnnmodel} with 3 convolutional layer a 8 channels, one linear \textit{concept} layer with 6 neurons and finally the output linear layer was deemed most fitting for the task. While less convolutional channels or layers often resulted in the model not converging at all, having more neurons or potentially \textit{concepts} did not seem to add information but just redundancy.
This model reliably yielded test accuracies over 99\% when using the same spurious-to-core feature ratio $\rho$ as used for training.

\begin{lstlisting}[language=bash, label=lst:cnnmodel]

convolutional_layers: 
    0: Conv2d(in_channels=1, out_channels=8, kernel_size=3)
    1: MaxPool2d(kernel_size=2, stride=2)
    2: ReLU()
    3: Conv2d(in_channels=8, out_channels=8, kernel_size=5)
    4: MaxPool2d(kernel_size=2, stride=2)
    5: ReLU()
    6: Conv2d(in_channels=8, out_channels=8, kernel_size=7)
    7: ReLU()

linear_layers:
    0: Linear(in_features=392, out_features=6, bias=True)
    1: ReLU()
    2: Linear(in_features=6, out_features=2, bias=True)  

\end{lstlisting}
\todo{maybe show little example of what purely linear model can achieve?}

\subsection{Hyperparameter Choice}
Similar to the size of the model, other hyperparameters are optimized for accuracy.
Finally we train all models using the \verb|Adam| optimizer \todo{cite} with a learning rate of \verb|0.001| using \verb|cross-entropy| loss \todo{cite} as the objective to minimize.
It is interesting to note that the learning rate has significantly different optimal values for highly biased models than unbiased ones and we therefore have to chose a compromise. We assume this to be due to the cost function becoming less complex as the trivial watermark feature gains importance.
But importantly, those hyperparameters including the learning rate should not be changed over the course of training our set of models because it has been shown that explanation can causally depend on hyperparameters quite strongly \cite{Karimi2023}. In our experiment we want to keep hyperparameters fixed and only intervene on the spurious-to-core feature ratio $\rho$ or later on all generating factors for establishing ground-truth importance. Another note is that as we are not evaluating the learning process or the model itself but the explanation. The hyperparameters were therefore not in focus of this thesis and chosen rather quickly through some trial-and-error.

\subsection{Training and Accuracy}
We generate datasets by sampling the spurious-to-core feature ratio $\rho$ in 0.05 steps and training on the same dataset while initializing the model with 10 different seeds.
Previously, the experiment did not control the influence of the random seed, but after observing strong variations of importance depending on the seed, we decided to always use the same 10 seeds for every dataset. This way it is theoretically possible to marginalize out the effect of the seed on the importance of the spurious feature, however this was done only through averaging of the 10 models results.
In total, 210 models are trained. The training dataset contains 30\% of all samples. Experimentally, much fewer samples seemed to be enough to achieve high accuracies, however there is no need to fear overfitting as it should if anything increase importance of the most important features even more.

Due to the low complexity of this benchmark dataset, very high accuracies of over 99\% are to be expected and also occured after short training for most models.

\todo{Talk more about seed, say that importance strongly changes with seed }


\begin{itemize}
    \item training split?
    \item how many models with which different features are trained
    \item showing some examples of heatmaps and maxrel images for different bias strength
    \item accuracies for all models plot
\end{itemize}

\subsection{Computational Setup}
\label{section:setup}
\begin{itemize}
    \item computed on personal dell xps 13 with cpu
    \item and on cluster \todo{cluster specs}
    \item how long did training all models take?
    \item about 40 hours on cluster
    \item + how long did computation of measures take:
    \item about 1 minute for all measures per model so 3 and a half hours
    \item measure time for final method more accurately for one model \todo{exact timing of final method}
\end{itemize}

\section{Preliminary (Causal) Experiments} \todo{should i include model scm stuff and attribution graphs etc?}
\begin{itemize}
    \item explain and show idea of a causal model for the whole network
    \item explain why it didnt work (too high, because linear correlation)
    \item attribution graph as a causal model???
    \item ideas and experiments with relevance maximization
    \item something more along the lines of intervening on hyperparameters? \cite{Karimi2023}
\end{itemize}

\section{Establishing a Ground-Truth of Biasedness}
\begin{itemize}
    \item non-linearity: This can also be explained information-theoretically \todo{explanation for non-linear biasedness - information-theoretically}
\end{itemize}

\subsection{Accuracy for Subgroups}
\begin{itemize}
    \item is the most 'ground' ground-truth measure of biasedness
    \item is also somehow related to the others??? \todo{how is accuracy related to prediction flip etc.}
    \item not as exact as mean logit change etc. ?
\end{itemize}

\subsection{Prediction Flip, R2 Score and Mean Logit Change}
\begin{itemize}
    \item prediction flip and r2 score are different sides of same coin
    \item mean logit change is more exact, as sometimes prediction stays the same but gets less confident (logits change a bit in that direction)
    \item mean logit change shall be used as ground truth of \textit{biasedness}
\end{itemize}

\subsection{Interpreting Mean Logit Change as Causal Intervention}
\begin{itemize}
    \item it is basically the causal effect of intervening on a latent factor on the models output
    \item theoretically the intervention must have exactly the same causal effect on the explanation as on the mean logit change???
\end{itemize}

\subsection{Relevance Mass Accuracy (RMA) and Relevance Rank Accuracy (RRA)}
In \cite{Arras2022} two metrics for the analysis of importance in pixel maps are introduced. Relevance Mass Accuracy (RMA) measures the ratio of relevance within a bounding box around a feature to total relevance. Relevance Rank Accuracy (RRA) the percentage of pixels in such a bounding box that fall within the $n$ most important pixels in the heatmap.

\section{Measuring Biasedness for Heatmaps}
\begin{itemize}
    \item this is a more general approach of measuring the biasedness of a saliency methods explanation
    \item good about it: humans look at the heatmaps and only see whether the watermark is colored or not to identify its importance.
    \item problem: humans have a hard time estimating the overall importance of concepts/features if they have varying spatial extend, see \cite{Achtibat2022} about noses and fur of dog
    \item so if watermark is even just a little bit red, it will be important to humans
    \item even bigger problem: NN do not disentangle concepts strictly. therefore the concepts found could always encode watermark and shape feature at the same time. this effect is strongly visible in our benchmark
    \item question: how much is the result explained by the spurious feature?
    \item will be taken as the baseline. all other saliency based / local attribution methods can be benchmarked with this too
    \item does not take into account the splitting up into an relevance of single neurons
    \item but can in principle also be applied to each neuron/concept individually
    \item Find a way to measure how well a single heatmap can show the bias
    \item e.g.: watermark mask importance bilder mit wm general heatmap, total relevance inside mask for:
    \item A: attribution mit wm, wenn ellipse und conditioned on y:[1]
    \item B: attribution mit wm, wenn rect    und conditioned on y:[0]
    \item C: attribution ohne wm, wenn ellipse und conditioned on y:[1]
    \item D: attribution ohne wm, wenn rect    und conditioned on y:[0]
    \item (A - B) + (D - C)
    \item \textbf{LRP biasedness score sanity check:}
          This sanity test shows that while LRP assigns strong relevance to the watermark, it fails in correctly identifying the lack of a watermark as the main reason to predict for the negative class (rectangle). Superficially this confirms the criticism of missing negative relevance \cite{Sixt2020}. It is however not clear if the advantage of not cancelling out importances outweighs this factor for more complex data and applications.  \todo{confirm class-invariance for heatmaps}


\end{itemize}

\section{Concepts Biasedness Measures}
\begin{itemize}
    \item should take into account that there are multiple concepts
    \item one could be important and not assign strong relevance to watermark
    \item the other could be unimportant and assign strong relevance to watermark
    \item \textit{ground truth} idea is to again take the mean logit change for each single neuron or summed together somehow
    \item we want to be able to identify \textit{spurious} concept and \textit{core} concept automatically, so it is not a good idea to have the latent factors given
    \item one idea: take masked/bounding box approach again for neurons individual heatmaps
    \item nmf idea: somehow try to reduce the latent space to Watermark/Shape axis and measure variance in either direction
    \item centroids idea: use random DR algorithm and calculate ratio of centroid distances (needs latent factors again)
    \item causal idea??? somehow measure causal effect? - the other things are kind of causal or?
\end{itemize}

\todo{make sure to not refer to results section too much. rather leave info out if it cannot explained well without looking at the results}


\section{Baseline Explanation Importance}
most "simple" and closest to "true causal effect" measure:
summed (weighted) full difference in heatmaps for crp and one full difference of heatmap for lrp

-> is super true to ground truth, oh shit, all i did was for nothing???
but: changes might be subtle and hard to understand, i.e. the snout/fur problem: although there might be subtle reductions in importance of the shape, it is only visible when the reductions are stronger? 
- isn't it a problem that there seems to be lots of change in the whole image but not due to the part where the watermark is? 
- not applicable to "human understanding"
- try out: weighted sum of neurons heatmap causal effects vs lrp heatmap effect 
- works just as well, but overestimates effect of spurious feature for lower biases



\section{Measures Temp Latex Notation}

\begin{center}
    Average Causal Effect of Latent Factor on Output

    $\displaystyle ACE = \mathbb{E} [ y \ | \ do(x=1) ] - \mathbb{E} [ y \ | \ do(x=0) ]$
    \vspace{0.5cm}

    $y = \vec{y} = (y_0, y_1), \ \ x = \mathrm{has\_watermark} \ or \ \mathrm{is\_ellipse}$
    \vspace{0.5cm}

    Mean Absolute Logit Change

    $\displaystyle MLC = \sum_{i \in n} \frac{|\vec{y}_{i, (x=1)}- \vec{y}_{i, (x=0)} |}{n}$


\end{center}


\begin{center}
    Average Causal Effect of Latent Factor on Explanation

    $\displaystyle ACE = \mathbb{E} [ y \ | \ do(x=1) ] - \mathbb{E} [ y \ | \ do(x=0) ]$
    \vspace{0.5cm}

    $x = \mathrm{has\_watermark} \ or \ \mathrm{is\_ellipse}$ \\
    But what is explanation $y$ or explanation change $|y_{x=1} - y_{x=0}|$?  \\
    For each neuron/concept in a layer:
    \vspace{0.4cm}

    Relevance Mass Accuracy: $\displaystyle RMA = \frac{\sum rel_{watermark}}{\sum rel_{total}}$
    \vspace{0.4cm}

    Relevance Rank Accuracy: $\displaystyle RRA = \frac{\# top-k \ rel \ in \ watermark}{\# watermark}$
    \vspace{0.4cm}

    Absolute Relevance Change: $\displaystyle \sum_{i \in layer} |rel_{i, (x=1)}- rel_{i, (x=0)} | $
    \vspace{0.4cm}

    $\mathrm{crp}  \independent \rho \ | \ gt $ ?
\end{center}
\newpage

\begin{center}
    Relevance Maximization:  $\mathcal{T}_{\max}^{rel}(x) = \max_i R_i(x|\theta)$ \\
    produces reference set of k-most relevant targets: $\mathcal{X}_k^{rel}i$ \\
    Ratio of watermark to shape overlap in sample space: $\displaystyle o_i =  \{
        \frac{\#  w_a = w_b}{\#  s_a = s_b} \ | \  a, b \in \mathcal{X}_k^{rel}i
        \}   $ \\
    RMA in reference set: $\displaystyle rma_i = \sum_{a \in \mathcal{X}_k^{rel}i} \frac{\sum rel_{watermark_a}}{\sum rel_{total_a}}$ \\
    proposed CRP relevance measure: $\displaystyle \max_i \ (o_i \cdot rma_i)$
\end{center}