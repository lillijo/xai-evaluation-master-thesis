
\chapter{Related Work}\label{chapter:related_work}


{\color{red}
      about 4-6 pages

      make a distinction between methods/papers that discuss similar approaches and methods/concepts used in this thesis}

\begin{enumerate}
      \item Back-Propagation/Saliency/Attribution/Local methods name them all
      \item LRP and CRP in more detail, showing Reduans results
      \item Current XAI evaluation methods - Feature Ablation, Visual Inspection, TCAV
      \item Current Criticism of BP methods and lack of methodical evaluation
      \item \cite{Sixt2020}, \cite{Wilming2023}, \cite{Kindermans2019} select criticism to look at
      \item XAI Methods, Criticism and Evaluation methods using Causality
      \item Use of causal methods in XAI and unused potential for evaluation
      \item Other benchmark datasets that have been used for evaluation, why need a new one?
      \item dsprites dataset? or in method
      \item why do we want to look at models reaction to bias-to-core-ratio?
\end{enumerate}


\section{The Field of Explainable Artificial Intelligence}
With the field of machine learning and particularly complex deep neural network models ever expanding, so is the demand for explanations of these models.
As especially neural networks are so called \textit{black boxes} that inhibit a human understanding of their results, plenty of explanation methods have been developed, summarized under the term \textit{explainable AI} or short \textit{XAI}.
\todo{more on why XAI is necessary in general}
\todo{post-hoc vs. interpretable models vs. ?}
Those methods can generally be divided into local and global approaches. While local methods aim to explain the decision making for one specific example, typically with attribution maps assigning importance to input features, global ones make more general interpretations of a model, for example, which features are identified in the decision-making process. The first category prominently includes saliency map methods \todo{cite}, which are usually tested on computer-vision tasks, where they assign importance to pixels or regions of a sample image, creating a heatmap. The importance is most often computed through forms of backpropagation \todo{cite} or with the help of gradients \todo{cite}. These saliency maps may generate insight into the locality of important objects, however this is usually only one facet of understanding the decision-making, especially for people not familiar with the data domain.

...


\todo{gradientXinput, integrated gradients, activation maximization, feature attack (generative approaches), activation atlas, specific local and global models ... }

\section{Layer-Wise and Concept Relevance Propagation}

\todo{LRP and surroundings: also other applications like pruning (if you can "prune" certain neurons, their causal effect must be none or extremely small \cite{Yeom2019})}

Concept Relevance Propagation, a recent method by \cite{Achtibat2022}, claims to be a \textit{glocal} XAI method, extending on the established local attribution method Layerwise Relevance Propagation (LRP) \cite{Bach2015} with more global methods like activation maximization \todo{cite and more}.
Layerwise Relevance Propagation, as a local XAI method, produces saliency maps for single data samples through a modified backpropagation process further described in \autoref{chapter:background}. By filtering on subsets of latent features within the layers of the model during this modified backpropagation, CRP yields saliency maps, which could in principle produce more specific explanations. With the help of feature visualization methods CRP's authors try to go beyond the pure \textit{"where"} of saliency maps, towards a \textit{what}, explaining which (human understandable?) concepts a model has recognized in a specific image region. This more global idea is integrating into a growing field of \textit{concept-based} explanation methods \todo{cite}.
These have in common that they try to disentangle the large latent feature space of models into human-understandable concepts.


\todo{papers using, extending on, similar to, evaluating CRP}
which papers have been published on this
\begin{itemize}
      \item reveal to revise: whole framework for XAI using CRP as one of the methods for concept/bias discovery \cite{Pahde2023}
      \item using CRP to identify and unlearn bias 'Right Reason Class Artifact Compensation (RR-ClArC)' \cite{Dreyer2023a}
      \item newest summary paper \cite{Achtibat2023}
      \item disentangle representations, similar to PCA, uses LRP \cite{Chormai2022}
\end{itemize}

\todo{more overview papers and map of XAI papers}
categorization \cite{Samek2021}


\section{Evaluation of XAI Methods}

\todo{more general what evaluation methods exist, which apply to CRP etc.}


\subsection{Evaluating Back-Propagation Methods}
\label{subsection:evaluation_critique}
The research on quantification and evaluation of XAI methods has increased with their rising popularity. Recently, a plethora of benchmarks and theoretical analyses have examined the fidelity, especially of feature importance methods, to the model they are trying to explain. Evaluations commonly used by authors of new XAI methods include feature ablation and data randomization (e.g. pixel flipping) \todo{cite}. Additionally, more complex benchmarks \cite{Kim2018, Arras2022, Bau2017, Singla2022} which are often human-supervised aim at comparing XAI outputs to human-understandable concepts.

In the quantitative evaluation of \cite{Kim2018} a similar approach to ours is put into action: 
a noise parameter controls the correlation of an image and a label on it and the accuracy for images without the label is tested. If the label was not learned as an important feature, the TCAV score should be low. User study showed that saliency maps for TCAVs did not help in identifying the important feature. This is an important insight and corresponds with \cite{Sixt2022a}. It seems that saliency maps are misleading more often than not. If there is no additional insight as with CRPs relevance images or similar, one can not expect to tell spurious from core features only using saliency maps. 

\begin{itemize}
      \item differentiate between numerical evaluation and evaluation through user studies
      \item examples of often used evaluations for local attribution methods and concept-based methods:
            \begin{itemize}
                  \item feature ablation and related methods
                  \item TCAVs \cite{Kim2018} with benchmark feature set (hard and often not applicable)
                  \item clevr-xai? \cite{Arras2022} 
            \end{itemize}
      \item clever XAI artificial benchmark dataset \cite{Arras2022}
      \item NetDissect dataset with concept-segmented images \cite{Bau2017}
      \item also other concept/neuron dissection by same authors, similar idea to CRP \cite{Bau2020}
      \item creates new dataset (human-supervised) to detect core vs spurious features \cite{Singla2022}
\end{itemize}
{\color{red} outline which problems CRP solves well, draw connections between unsolved criticism and causal perspective}

\subsubsection{Recent Critique of Saliency Maps}

Although a general lack of dependence between explanations and their model \cite{Adebayo2018,Karimi2023} has so far only been studied for less complex attribution methods, current research still draws a less than ideal picture of XAI's fidelity.
Kindermans et. al. \cite{Kindermans2019} show that a constant vector shift on the input data, which is not affecting the performance of the model, can lead to misleading explanations. \cite{Sixt2020} finds the class insensitivity of some back-propagation methods to be due to their improper use of negative relevance.
While authors of new methods often underline their results with user studies \todo{cite}, Sixt et. al. \cite{Sixt2022a} among others \todo{cite} show that XAI methods do not necessarily increase humans skill at identifying relevant features.

Constructing a test which is neither too simple and therefore too far away from realistic application scenarios nor not quantifiable empirically due to its \textit{human} component or unidentifiable ground truth, poses a challenge which \cite{Clark2023} tries to tackle. This benchmark is based on recent analysis \cite{Wilming2023} of suppressor variables, which can be for example the background color of an image, that are used by the model without having an association to the core features to normalize the image and improve the prediction.
They introduce a generation process for mixing the suppressor and real features which serves as inspiraiton for the structural causal model applied here.

\begin{itemize}
      \item explanations are independent of later layers (no negative relevance) \cite{Sixt2020}
      \item suppressor variable "in practice, XAI methods do not distinguish whether a feature is a confounder or a suppressor, which can lead to misunderstandings about a model's performance and interpretation"
      \item kinda stupid, because nueral network also does not make a difference between suppressors and confounders \cite{Wilming2023}
      \item the (un-)reliability of saliency methods: should fulfill 'input invariance'
      \item saliency method mirrors sensitivity of model with respect to transformations of the input
      \item normal LRP root point (zero) not working
      \item pattern attribution reference point works (direction of variation in data, determined by covariances) \cite{Kindermans2019}
\end{itemize}

\begin{enumerate}
      \item \cite{Sixt2022a}: evaluation of heatmaps/saliency methods not enough based on actual user studies and human performance / explanation quality
            \\ task: look at explanation and rate, weather each feature is relevant or irrelevant
      \item \cite{Wilming2023}: explanation of suppressor variables (that have no statistical association with target) gives false impression that of dependency if their inclusion into the model improves it
            \\ task: linear model with 1 real and 1 suppressor variable, saliency methods mark both suppressor variable and core variable as important
      \item \cite{Sixt2020}: because matrix is converging to rank 1 in BP methods that dont use negative relevance scores appropriatly, heatmaps are not class sensitive
            \\ task: randomize more and more network parameters, look at heatmap for and against class
      \item \cite{Kindermans2019}: heatmap methods are sensitive to constant shift in input data, but should fulfill input invariance
            \\ task: add “watermark style” input shift, test if model still predicts accurately and then if heatmap does same as model
      \item \cite{Karimi2023}: explanation depends more on hyperparameters than on model weights and prediction itself
            \\ task: quantify treatment effect when changing hyperparameters in comparison to changing model weights
      \item \cite{Adebayo2018}: some saliency methods are independent to both the model and the data generating factors (not testing LRP)
            \\ task: compare explanation trained on true model with explanation trained with random labels, also compare to simple edge detector which is very similar often
      \item \cite{Parafita2019}: use generative model to identify (causal) latent factors and estimate effect they have on prediction outcome
            \\ task: use data with known latent generating factors to test effect estimation on a constructed causal graph
      \item \cite{Narendra2018}: build SCM over input-model-output -> has potential to be more accurate than saliency purely observational
      \item \cite{Chattopadhyay2019}: build SCM over last linear layer before output and attribute because of sensitivity to constant shifts as shown by Kindermans
            \\ task: treat Model as SCM and calculate interventional expectations and average causal effect
      \item \cite{Sixt2022}: deep taylor decomposition fails, when only positive relevance taken into account, matrix falls to rank 1 \\ task: theoretical analysis
\end{enumerate}

\subsection{Causality and XAI (on Evaluation and Benchmarking of XAI)}
The link an explanation and its model should have, has come under the causal lense both for developing new XAI methods and their evaluation  \todo{cite}. Counterfactual explanations


{
\color{red}
quote from \cite{Goyal2019}:

\begin{quote}
The relationship between causality and explainability has a
long history, see (Woodward, 2005 \textit{making things happen: a theory of causal explanation}) for a discussion from
a philosophy of science point of view. Halpern \& Pearl
(2005 \textit{Causes and Explanations: a structural-model approach}) give a formal causal theory of what constitutes an
explanation, in terms of what is known as "actual causality"
\end{quote}

\begin{quote}
We note that this problem does not exist as such for most
local interpretation methods: because for a given image,
the pixels deterministically cause the output of a model,
there is no notion of probability or confounding. However,
confounding might affect local models where pixels are per-
turbed based on data-dependent models....

Many interpretability methods developed to have causal-
flavor are for local explanations, such as removing and
adding pixels to generate counterfactual explanations for
images
\end{quote}

The summary of \cite{Moraffah2020a} is really good for \"general map of XAI\" but also is a good summary on the existing causal stuff for XAI! \todo{base some of my related works section of \cite{Moraffah2020a}}

\begin{quote}
Pearl [83] introduces different levels of said interpretability
and argues that generating counterfactual explanations is
the way to achieve the highest level of interpretability.

\begin{itemize}
      \item statistical: how would seeing x change my belief in y?
      \item interventional: what if?
      \item counterfactual: why?
\end{itemize}
\end{quote}

In this paper and year (2020) there were no specific datasets designed for the purpose of causal interpretability (performance evaluation)

Evaluation:

\begin{itemize}
      \item human subject: [89], [17], [91], [66]
      \item how close model is to actual causal features
      \item how locally faithful is proposed method (i.e. with masking, perturbation)
      \item how consistent are explanation? 
      \item metrics for evaluating counterfactual explanations: sparsity/size, interpretability, proximity, speed, diversity, visual-linguistic counterfactuals
\end{itemize}

CLEVR-XAI \cite{Arras2022} is a well defined benchmark dataset with extensive ground-truth information. 

similar: M. Schuessler, P. Weiß, L. Sixt, Two4two: Evaluating interpretable machine learning - a synthetic dataset for controlled experiments

good summary table of most commonly used evaluation methods:
- Pixel Perturbation
- Data Randomization Test
- Model Randomization Test
- Remove and Retrain
- Object Localization/Segmentation
- Pointing Game

Although the term \textit{causality} is not specifically mentioned in the paper, the CLEVR-XAI benchmark has potential for the causal evaluation of XAI methods. With (ground truth) generating factors available and a complex scene graph comparable to a structural causal model, it is straight-forward to measure causal effects or do counterfactual analysis. However the dataset requires quite a complex network architecture, which can read text (questions?) e.g. with a recurrent CNN and give not just a classification output but a set of output features.
}
      {\color{red} which other methods/approaches/papers are there that broadly connect explainable AI and causality }
\begin{itemize}
      \item general map of causality and XAI mixups
      \item counterfactual stuff
      \item seeing model as scm stuff \cite{Chattopadhyay2019}
      \item seeing whole process as SCM \cite{Karimi2023} - very important for my work, as i do basically the same thing just not for hyperparameters but biasedness!
      \item representation learning??? \todo{mabye ask simon, what is a good overview paper? Schoelkopf2019? }
      \item overview of \cite{Schoelkopf2019}
      \item Oanas thesis / work??
\end{itemize}

\begin{itemize}
      \item generally, mostly about counterfactuals: \cite{Moraffah2020a}
      \item causal attribution, similar to LRP but more "causally" neural networks as SCMs \cite{Chattopadhyay2019}
      \item causal concept effects (edges in mnist) \cite{Goyal2019}
      \item causal in most general sense:independent/disjoint mechanism analysis \cite{Leemann2023} \cite{Leemann2022}
      \item causal binary concepts \cite{Tran2022}
      \item basic framework/idea of interpreting NN as skeleton of SCM and using some transformation to quantify effect:\cite{Narendra2018}
\end{itemize}



