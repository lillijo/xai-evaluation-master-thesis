
\chapter{Related Work}\label{chapter:related_work}


{\color{red} 
about 4-6 pages

make a distinction between methods/papers that discuss similar approaches and methods/concepts used in this thesis}

\begin{enumerate}
      \item Back-Propagation/Saliency/Attribution/Local methods name them all
      \item LRP and CRP in more detail, showing Reduans results
      \item Current XAI evaluation methods - Feature Ablation, Visual Inspection, TCAV
      \item Current Criticism of BP methods and lack of methodical evaluation
      \item \cite{Sixt2020}, \cite{Wilming2023}, \cite{Kindermans2019} select criticism to look at
      \item XAI Methods, Criticism and Evaluation methods using Causality
      \item Use of causal methods in XAI and unused potential for evaluation
      \item Other benchmark datasets that have been used for evaluation, why need a new one?
      \item dsprites dataset? or in method
      \item why do we want to look at models reaction to bias-to-core-ratio?
\end{enumerate}


\section{The Field of Explainable Artificial Intelligence}

\todo{General Map of the field, embedding CRP}

\begin{enumerate}
      \item XAI is needed for AI because of black box other approach is making AI not black box
      \item generally divided into local and global methods
      \item name a few popular global methods
      \item local methods that rely on attribution/ saliency maps (gradientXinput, integrated gradients, LRP, ???)
      \item CRP is based on LRP, so in principle it is a loal attribution method.
            However the authors argue that it could be a \textit{glocal} method because it can be used to summarize the workings of the model over many samples
\end{enumerate}

General Overview papers to cite
\begin{itemize}
      \item focus on post-hoc explanations + theoretical foundations, test algorithms \cite{Samek2021}
      \item need some more \todo{overview papers and map of XAI papers}
\end{itemize}

\section{Layerwise Relevance Propagation (LRP)}
 {\color{red} what has LRP been used for already, in which context to set CRP}

\begin{itemize}
      \item LRP first paper \cite{Bach2015}
      \item general XAI \cite{Samek2021}
      \item mention that it has been getting a mathematical background with Deep Taylor Decomposition  \cite{Montavon2017}
      \item LRP in practice \cite{Kohlbrenner2020}
      \item disentangle representations, similar to PCA: Principal Relevant Component Analysis - uses LRP \cite{Chormai2022}
      \item LRP is also good at pruning... idea/intuition: if you can "prune" certain neurons, their causal effect must be none or extremely small \cite{Yeom2019}
      \item recent criticism of Sixt here? (deep taylor decomposition fails, when only positive relevance taken into account, matrix falls to rank 1 \cite{Sixt2022})
\end{itemize}

\section{Concept Relevance Propagation (CRP)}
which papers have been published on this
\begin{itemize}
      \item from where to what... CRP main paper \cite{Achtibat2022}
      \item reveal to revise: whole framework for XAI using CRP as one of the methods for concept/bias discovery \cite{Pahde2023}
      \item using CRP to identify and unlearn bias 'Right Reason Class Artifact Compensation (RR-ClArC)' \cite{Dreyer2023a}
      \item newest summary paper \cite{Achtibat2023}
\end{itemize}

\section{Evaluation of XAI Methods}

\todo{more general what evaluation methods exist, which apply to CRP etc.}


\subsection{Benchmarking Back-Propagation Methods}

\begin{itemize}
      \item differentiate between numerical evaluation and evaluation through user studies
      \item examples of often used evaluations for local attribution methods and concept-based methods:
            \begin{itemize}
                  \item feature ablation and related methods
                  \item TCAVs \cite{Kim2018} with benchmark feature set (hard and often not applicable)
                  \item clevr-xai? \cite{Arras2022}
            \end{itemize}
      \item clever XAI artificial benchmark dataset \cite{Arras2022}
      \item NetDissect dataset with concept-segmented images \cite{Bau2017}
      \item also other concept/neuron dissection by same authors, similar idea to CRP \cite{Bau2020}
      \item creates new dataset (human-supervised) to detect core vs spurious features \cite{Singla2022}
\end{itemize}
{\color{red} outline which problems CRP solves well, draw connections between unsolved criticism and causal perspective}

\subsection{Recent Critique of Saliency Maps}

\begin{itemize}
      \item explanations are independent of later layers (no negative relevance) \cite{Sixt2020}
      \item suppressor variable "in practice, XAI methods do not distinguish whether a feature is a confounder or a suppressor, which can lead to misunderstandings about a model’s performance and interpretation"
      \item kinda stupid, because nueral network also does not make a difference between suppressors and confounders \cite{Wilming2023}
      \item the (un-)reliability of saliency methods: should fulfill 'input invariance'
      \item saliency method mirrors sensitivity of model with respect to transformations of the input
      \item normal LRP root point (zero) not working
      \item pattern attribution reference point works (direction of variation in data, determined by covariances) \cite{Kindermans2019}
\end{itemize}

\begin{enumerate}
      \item \cite{Sixt2022a}: evaluation of heatmaps/saliency methods not enough based on actual user studies and human performance / explanation quality
            \\ task: look at explanation and rate, weather each feature is relevant or irrelevant
      \item \cite{Wilming2023}: explanation of suppressor variables (that have no statistical association with target) gives false impression that of dependency if their inclusion into the model improves it
            \\ task: linear model with 1 real and 1 suppressor variable, saliency methods mark both suppressor variable and core variable as important
      \item \cite{Sixt2020}: because matrix is converging to rank 1 in BP methods that dont use negative relevance scores appropriatly, heatmaps are not class sensitive
            \\ task: randomize more and more network parameters, look at heatmap for and against class
      \item \cite{Kindermans2019}: heatmap methods are sensitive to constant shift in input data, but should fulfill input invariance
            \\ task: add “watermark style” input shift, test if model still predicts accurately and then if heatmap does same as model
      \item \cite{Karimi2023}: explanation depends more on hyperparameters than on model weights and prediction itself
            \\ task: quantify treatment effect when changing hyperparameters in comparison to changing model weights
      \item \cite{Adebayo2018}: some saliency methods are independent to both the model and the data generating factors (not testing LRP)
            \\ task: compare explanation trained on true model with explanation trained with random labels, also compare to simple edge detector which is very similar often
      \item \cite{Parafita2019}: use generative model to identify (causal) latent factors and estimate effect they have on prediction outcome
            \\ task: use data with known latent generating factors to test effect estimation on a constructed causal graph
      \item \cite{Narendra2018}: build SCM over input-model-output -> has potential to be more accurate than saliency purely observational
      \item \cite{Chattopadhyay2019}: build SCM over last linear layer before output and attribute because of sensitivity to constant shifts as shown by Kindermans
            \\ task: treat Model as SCM and calculate interventional expectations and average causal effect
\end{enumerate}

\subsection{Causality Research on Evaluation and Benchmarking of XAI}
{\color{red} which other methods/approaches/papers are there that broadly connect explainable AI and causality }
\begin{itemize}
      \item general map of causality and XAI mixups
      \item counterfactual stuff
      \item seeing model as scm stuff \cite{Chattopadhyay2019}
      \item representation learning
      \item overview of \cite{Schoelkopf2019}
\end{itemize}

\begin{itemize}
      \item generally, mostly about counterfactuals: \cite{Moraffah2020a}
      \item causal attribution, similar to LRP but more "causally" neural networks as SCMs \cite{Chattopadhyay2019}
      \item causal concept effects (edges in mnist) \cite{Goyal2019}
      \item causal in most general sense:independent/disjoint mechanism analysis \cite{Leemann2023} \cite{Leemann2022}
      \item causal binary concepts \cite{Tran2022}
      \item basic framework/idea of interpreting NN as skeleton of SCM and using some transformation to quantify effect:\cite{Narendra2018}
\end{itemize}



