
\chapter{Related Work}\label{chapter:related_work}


{\color{red} make a distinction between methods/papers that discuss similar approaches and methods/concepts used in this thesis}

\begin{enumerate}
      \item Back-Propagation/Saliency/Attribution/Local methods name them all
      \item LRP and CRP in more detail, showing Reduans results
      \item Current XAI evaluation methods - Feature Ablation, Visual Inspection, TCAV
      \item Current Criticism of BP methods and lack of methodical evaluation
      \item \cite{Sixt2020}, \cite{Wilming2023}, \cite{Kindermans2019} select criticism to look at
      \item XAI Methods, Criticism and Evaluation methods using Causality
      \item Use of causal methods in XAI and unused potential for evaluation
      \item Other benchmark datasets that have been used for evaluation, why need a new one?
      \item dsprites dataset? or in method
      \item why do we want to look at models reaction to bias-to-core-ratio?
\end{enumerate}


\subsection{XAI in general}

\begin{itemize}
      \item focus on post-hoc explanations + theoretical foundations, test algorithms \cite{Samek2021}
      \item creates new dataset (human-supervised) to detect core vs spurious features \cite{Singla2022}
\end{itemize}

\subsection{Layerwise Relevance Propagation}
{\color{red} approach used in the thesis, tell which rules of LRP are used}

\begin{itemize}
      \item LRP first paper \cite{Bach2015}
      \item overview of propagation rules \cite{Montavon2019}
      \item general XAI \cite{Samek2021}
      \item LRP in practice \cite{Kohlbrenner2020}
      \item disentangle representations, similar to PCA: Principal Relevant Component Analysis - uses LRP \cite{Chormai2022}
      \item LRP is also good at pruning... idea/intuition: if you can "prune" certain neurons, their causal effect must be none or extremely small \cite{Yeom2019}
\end{itemize}

\subsection{Deep Taylor Decomposition}
{\color{red} maybe just mention shortly how it is related to LRP (with rules) and which limitations have been studied e.g. by Sixt and Landgraf. Think again whether deep taylor decomposition has any more ties with causality???}

\begin{itemize}
      \item theoretical explanation to how LRP/DTD works...? \cite{Montavon2017}
      \item deep taylor decomposition fails, when only positive relevance taken into account, matrix falls to rank 1 \cite{Sixt2022}
\end{itemize}

\subsection{Concept Relevance Propagation (CRP)}
{\color{red} Only state when and where it has been described developed. refer to background section for theory and definition}

\begin{itemize}
      \item from where to what... CRP main paper \cite{Achtibat2022}
      \item reveal to revise: whole framework for XAI using CRP as one of the methods for concept/bias discovery \cite{Pahde2023}
      \item using CRP to identify and unlearn bias 'Right Reason Class Artifact Compensation (RR-ClArC)' \cite{Dreyer2023a}
\end{itemize}

\subsection{Evaluation of Back-Propagation XAI Methods}
{\color{red} important to note, as 'causal' perspective could add another evaluation method to corpus}

\begin{itemize}
      \item clever XAI artificial benchmark dataset \cite{Arras2022}
      \item NetDissect dataset with concept-segmented images \cite{Bau2017}
      \item also other concept/neuron dissection by same authors, similar idea to CRP \cite{Bau2020}
\end{itemize}

\subsection{Criticism of Back-Propagation XAI Methods}
{\color{red} outline which problems CRP solves well, draw connections between unsolved criticism and causal perspective}

\begin{itemize}
      \item explanations are independent of later layers (no negative relevance) \cite{Sixt2020}
      \item suppressor variable "in practice, XAI methods do not distinguish whether a feature is a confounder or a suppressor, which can lead to misunderstandings about a model’s performance and interpretation"
      \item kinda stupid, because nueral network also does not make a difference between suppressors and confounders \cite{Wilming2023}
      \item the (un-)reliability of saliency methods: should fulfill 'input invariance'
      \item saliency method mirrors sensitivity of model with respect to transformations of the input
      \item normal LRP root point (zero) not working
      \item pattern attribution reference point works (direction of variation in data, determined by covariances) \cite{Kindermans2019}
\end{itemize}

\subsection{Causal Discovery for XAI}
{\color{red} which other methods/approaches/papers are there that broadly connect explainable AI and causality }

\begin{itemize}
      \item generally, mostly about counterfactuals: \cite{Moraffah2020a}
      \item causal attribution, similar to LRP but more "causally" neural networks as SCMs \cite{Chattopadhyay2019}
      \item causal concept effects (edges in mnist) \cite{Goyal2019}
      \item causal in most general sense:independent/disjoint mechanism analysis \cite{Leemann2023} \cite{Leemann2022}
      \item causal binary concepts \cite{Tran2022}
      \item basic framework/idea of interpreting NN as skeleton of SCM and using some transformation to quantify effect:\cite{Narendra2018}
\end{itemize}



\section{Critique on Saliency Maps Summary}
\begin{enumerate}
      \item \cite{Sixt2022a}: evaluation of heatmaps/saliency methods not enough based on actual user studies and human performance / explanation quality
            \\ task: look at explanation and rate, weather each feature is relevant or irrelevant
      \item \cite{Wilming2023}: explanation of suppressor variables (that have no statistical association with target) gives false impression that of dependency if their inclusion into the model improves it
            \\ task: linear model with 1 real and 1 suppressor variable, saliency methods mark both suppressor variable and core variable as important
      \item \cite{Sixt2020}: because matrix is converging to rank 1 in BP methods that dont use negative relevance scores appropriatly, heatmaps are not class sensitive
            \\ task: randomize more and more network parameters, look at heatmap for and against class
      \item \cite{Kindermans2019}: heatmap methods are sensitive to constant shift in input data, but should fulfill input invariance
            \\ task: add “watermark style” input shift, test if model still predicts accurately and then if heatmap does same as model
      \item \cite{Karimi2023}: explanation depends more on hyperparameters than on model weights and prediction itself
            \\ task: quantify treatment effect when changing hyperparameters in comparison to changing model weights
      \item \cite{Adebayo2018}: some saliency methods are independent to both the model and the data generating factors (not testing LRP)
            \\ task: compare explanation trained on true model with explanation trained with random labels, also compare to simple edge detector which is very similar often
      \item \cite{Parafita2019}: use generative model to identify (causal) latent factors and estimate effect they have on prediction outcome
            \\ task: use data with known latent generating factors to test effect estimation on a constructed causal graph
      \item \cite{Narendra2018}: build SCM over input-model-output -> has potential to be more accurate than saliency purely observational
      \item \cite{Chattopadhyay2019}: build SCM over last linear layer before output and attribute because of sensitivity to constant shifts as shown by Kindermans
            \\ task: treat Model as SCM and calculate interventional expectations and average causal effect
\end{enumerate}

