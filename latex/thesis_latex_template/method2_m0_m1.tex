
\section{Data Ground Truth Correlation $m_0$}
The goal of this analysis it to gather information on how a known coupling ratio of two features interacts with their importance to the model and their explained importance. 
Measure $m_0 = \rho$ is the correlation between the shape and spurious feature in our data generating model. When $\rho$ is zero, the features are not associated at all, when it is one they correlate perfectly. Conceived as a \textit{signal-to-noise} ratio between the correlated and uncorrelated parts of $S$ and $W$, it can directly be used as a measure of the coupling of spurious (watermark) and core (shape) feature. However, the data generating process introduces a small modification due to the binarization of the variables $W$ and $S$. It might therefore be more insightful to look at the actual correlation of the features in the generated data distribution as a ground-truth. Considering that we have two binary variables their correlation can be measured using the $\phi$-coefficient. It is also called \textit{Matthews} or \textit{Yule phi} coefficient and is essentially the Pearson correlation coefficient for two binary variables:

\vspace{1em}
\begin{minipage}[t]{0.45\textwidth}
\begin{tabular}{|c|c|c|c|}
    \hline
     & y= 1 & y = 0 & total  \\  \hline
    x= 1 & $n_{11}$ & $n_{10}$ & $n_{1*}$ \\ \hline
    x= 0 & $n_{01}$ & $n_{00}$ & $n_{0*}$ \\ \hline
    total& $n_{*1}$ & $n_{*0}$ & $n$ \\ \hline
\end{tabular}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
\begin{align}
& \phi = \frac{n_{11} * n_{00} - n_{10}*n_{01}}{\sqrt{n_{1*}*n_{0*}*n_{*0}*n_{*1}}}
\end{align}
\end{minipage}
\vspace{1em}

Generally, we do not want and neither expect the model to perfectly reconstruct $\rho$ or the data correlation $\phi$. After all, the strength of neural networks presumably lies in recovering the truly important feature even when other, highly correlated features are present. Though some research expects explanations to give insight into the distributions of the training data to better understand how biases might occur, even if a model has apparently learned to ignore spurious features \cite{Kindermans2017}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{thesis_latex_template/pics/gt_m0_phi_only.png}
    \caption[True Data Distribution $m_0$]{$\phi$-coefficient between $W$ and $S$ of sampled training data distributions with growing coupling ratio $\rho$}
    \label{fig:finding_rho}
\end{figure}

\section{Establishing a Ground-Truth Model Feature Importance $m_1$}\label{section:gt_measure}
In contrast to realistic application scenarios our causal framework enables us to establish the ground truth importance of features for a trained model. Similar to recent work on causal attribution \cite{Goyal2019,Parafita2019,Karimi2023} the causal effect of an intervention upstream on the output of a model can be estimated in the following way:
\begin{center}
Average Causal Effect of latent factor $W$ on output $Y$ \\
\begin{equation}
\displaystyle ACE = \mathbb{E} [ Y \ | \ do(W=1) ] - \mathbb{E} [ Y \ | \ do(W=0) ] 
\end{equation}
\end{center}

The intervention on a given latent factor is straight-forward here, as our factors of interest are both binary variables (watermark and shape). Due to our knowledge of the ground truth we can condition on all other independent latent factors and feed one image with the watermark and the same image without it through the neural network. Thereby achieving a pure intervention on our $W$.  
However it is not naturally clear how to define the output of a neural network. One can either measure the average causal effect on the binary prediction or on the output layers' logits, which change in a continuous fashion.
To account for the effect of the initialization of model weights and biases on the usage of either feature, we average each measure over multiple random seeds (see \cref{fig:gt_over_seeds}).

\subsection{Prediction Flip}
Computing the \textit{Prediction Flip}, as Sixt et al. \cite{Sixt2022a} call it in their experiments, is straight forward in our example as we can control all variables. 
This binary causal effect can be estimated as the percentage of images for which the prediction changes, when the factor is changed, which in turn is equivalent to the magnitude of the $\phi$-coefficient between prediction $Y$ and watermark $W$ in our scenario (see proof in \cref{appendix:phi_equals_pf}). Therefore, this measure is apt for the comparison with the $W,S$ correlation in the training data distribution.

For better readability we will refer to an image with the watermark $\mathrm{x}_{do(W=1)}$ as $\mathrm{x}$ and the same image without the watermark $\mathrm{x}_{do(W=0)}$ as $\mathrm{x'}$ in the rest of the thesis.
\begin{align}
\displaystyle 
& PF =\tfrac{1}{|\mathcal{X}|} \sum_{\mathrm{x} \in \mathcal{X}} |y(\mathrm{x}) - y(\mathrm{x'}) | \\
&  \equiv \frac{|\mathrm{x}_{W=1,y=1}|*|\mathrm{x}_{W=0,y=0}| - |\mathrm{x}_{W=1,y=0}|*|\mathrm{x}_{W=0,y=1}| }
{\sqrt{|\mathrm{x}_{y=1}|*|\mathrm{x}_{y=0}|*|\mathrm{x}_{W=1}|*|\mathrm{x}_{W=0}| }}
\end{align}

\subsection{Mean Logit Change}
For the W-dSprites classification task, the output vector consists of 2 logits $y_0$ and $y_1$. The model predicts \textit{rectangle} when $y_0 > y_1$ and \textit{ellipse} otherwise. To compute the mean logit change when intervening on our spurious feature $W$, we take a sufficient amount of samples $\mathcal{X}$ from our dataset and feed them through the model. First we predict images with $W=1$ (containing a watermark) then with $W=0$. 
We expect this variant of the model importance to be slightly more sensitive to the spurious watermark feature $W$ for lower values of $\rho$ than the prediction flip. The reason being, that while the continuous output vector, i.e. \textit{confidence}, might already be affected by the spurious feature for weakly biased models, the prediction will only change once the spurious feature becomes more easy to identify than the core feature. 

It is important to choose a reasonable distance measure between the two output vectors. \cite{Sixt2022a} and \cite{Goyal2019} use the mean absolute difference (or $L1$-norm). To enable better comparison we apply the soft-max function to the outputs to yield confidences, as during the training process. This keeps the relative magnitudes within the sample set intact but brings them to the range $[0,1]$. 

Mean Absolute Difference:
\begin{align}\displaystyle 
& \MLC_{\rho, m}^{abs} = \tfrac{1}{|\mathcal{X}| * 2} \sum_{\mathrm{x} \in \mathcal{X}} 
|y_0(\mathrm{x}) -y_0(\mathrm{x'})| + |y_1(\mathrm{x}) -y_1(\mathrm{x'})| 
\end{align}


There is no agreed upon distance metric for the assessment of similarity of attribution maps and vectors. 
Recently, assessing the similarity of activation and relevance vectors has been done using the cosine similarity \cite{Sixt2020,Achtibat2023,Dreyer2023a, Pahde2023}. Therefore, we include the cosine distance as a potential distance metric for the model output (as well as the explanation later). \cite{Karimi2023} test different kernels for a kernelized treatment effect on multidimensional outputs including a linear kernel which is equivalent to the squared distance metric we apply. They finally settle on a radial basis function (RBF) kernel, but also find that the relative effects are not sensitive to the choice of kernels. 
We therefore assume that the trend of effects will not be affected by the choice of a distance metric but still test this hypothesis.

Cosine Distance:
\begin{align}\displaystyle 
& \MLC_{\rho, m}^{cosine} = \tfrac{1}{|\mathcal{X}|}\sum_{\mathrm{x} \in \mathcal{X}}  
1 - \frac{\vec{y}(\mathrm{x}) \cdot \vec{y}(\mathrm{x'})}
{\lVert \vec{y}(\mathrm{x}) \rVert \lVert \vec{y}(\mathrm{x'})\rVert }
\end{align}

