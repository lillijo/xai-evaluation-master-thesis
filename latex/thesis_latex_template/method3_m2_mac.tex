
\section{CRP Explanation Importance $m_2$}\label{section:measure}
Our goal is to compare the causal effect of an upstream intervention on the true feature importance $m_1$ and then the explained feature importance $m_2$. After having defined ways to measure the models true feature importance, we need to repeat the process for the concept-based explanation produced by CRP. It is not well known how humans perceive changes in an explanation and there is no agreed upon scale of importance, so we believe it is best to construct multiple measures to test against each other. Some of the proposed metrics are derived from existing work on evaluating feature importance for local attribution methods \citep{Sixt2020, Karimi2023, Arras2022}. Each candidate should be a variation of measuring the average causal effect of intervening on $W$ on an explanation $e$:
\begin{center}
\begin{equation}
\displaystyle ACE = \mathbb{E} [e \ | \ do(W=1) ] - \mathbb{E} [ e \ | \ do(W=0) ]
\end{equation}
\end{center}

The core question is, whether this effect over different $\rho$ is comparable between the explanation and the prediction. A perfect explanation assigns just as much importance to a feature as the model. This is one way to evaluate the fidelity of the explanation to the model. Some of the proposed measures however attempt to incorporate other desirable characteristics of XAI such as \textit{compactness} \citep{Nauta2023}. This also has the practical reason that while an attribution map is visually compelling, it is not a very concise description of relevance. Measuring the causal effect purely on the saliency maps also has the disadvantage that effects invisible or non-distinguishable by humans are accounted for. 
We thus aim to estimate the explanation importance not only through the attribution maps but also in less complex forms. 

The measures introduced in the following are roughly ordered from measures being most true to the numerical effect of intervention on the explanation to measures reducing the complexity of the explanation. Although human perception is not part of our evaluation, aiming for less complex explanations seems to be more in line with a \textit{good} explanation. While the first measure would require a pixel-wise comparison of multiple heatmaps to find differences, later metrics attempt to work with reduced and hopefully more human-interpretable abstractions. 

Related work often removes negative relevances in attribution maps to enable comparison with XAI methods that do not measure negative relevance. Another reason could be that incautious aggregation of attribution results with negative and positive relevance introduces cancelling-out effects. We believe, however, that a spurious feature can be negatively as well as positively attributed for a model to be biased. Therefore, the measures equally incorporate both the magnitude of the positive and negative relevance. In our scenario with a binary classification task it is reasonable to assume that whatever is positively attributed for one class, should be negatively attributed to the other with the same magnitude, but we suppose that this should be tested by incorporating both.

\subsection{Mean Attribution Change}\label{section:measure_mac}
The likely most straight-forward way to calculate explanation importance for neurons in a layer using CRP is to measure the average causal effect of an intervention on the concept-wise attribution directly. This approach aims to emulate the \textit{mean logit change} of the prediction for the concept-based explanation. In a given layer $\ell \in L$, the relevance of each of the neurons $c \in \ell$, which we described as $R_c^{\ell}(\mathrm{x})$ in \cref{eq:normed_relevance} and the pixel-wise attribution maps $A_c(\mathrm{x})$ can be used for this.
The causal effect of intervening on the spurious feature on one neuron is then the difference between its attribution map $A_c$ or relevance $R_c^{\ell}$ of one image where $W=1$ and the same image with $W=0$. If the model has indeed learned separate concepts for each feature, the same effect should become visible both for the attribution maps and the aggregated relevances.

To look at the difference in explanation, we compare multiple types of dissimilarity between two sets of heatmaps (i.e. 2-dimensional pixel maps). Firstly, we look at a normalized absolute pixel-wise difference, or absolute error ($AE$). The mean squared error $SE$ also seems a natural approach and has been applied by \citet{Karimi2023} previously. Because we compute the $cosine$ distance for the relevance values $R_c^{\ell}$ as done among others by the authors of CRP \citep{Achtibat2023}, we also compute this metric for the high-dimensional attribution maps for completeness. This metric is especially applicable because it focuses on non-empty indices and normalizes the results independently of the dimensionality of the input. Note that we scale this metric by one half, because when negative and positive values are present, its maximal value is two.

A sensible normalization for a set of attribution maps, which is needed for the absolute and squared distance metrics, is not as trivial as for the output vector or the relevance vector. The naive approach would be, to take the maximally opposing images' difference. But it is not to be expected that any method would assign positive or negative values of full magnitude to every pixel. Instead, the results of attribution methods only sparsely assign attribution to few pixels, usually within regions of objects and not the background. 
We therefore have to find a smaller divisor which scales a maximal difference between a set of attribution maps with and without watermark to approximately 1.
To adjust for the sparsity of the attributions, we take the maximal sum of absolute values of all heatmaps over all samples for one model. This \textit{maximal absolute sum} scaling is very similar to what \citet{Achtibat2022} propose for the normalization of relevances of a layer, yet applied to more dimensions and multiple samples.

Here, we show the described distance metrics for the attribution map matrices. $A(\mathrm{x})$ represents an array of all $|\ell|$ (here 8) attribution maps of the concepts $c$ in a layer. The same metrics are computed for the relevance vector, which in our case consists of $|\ell|$ normalized relevance values $R_{c}^{\ell,(norm)}$. For these, the computation is considerably simplified as no aggregation per pixel is necessary. In principle, each of these distance metrics should produce comparable results. However, like \citet{Karimi2023}, we want to ensure that the choice of a distance metric has no adverse interaction with our results and therefore compare multiple candidates.

\begin{align}
\displaystyle 
& A_{tot}^{\ell}(\mathrm{x}) = \sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}} |A_{p,q,c}(\mathrm{x})|  \label{eq:total_absolute_relevance} & \\
& \max_{\rho, m}^E(\mathrm{x}) = \max_{c \in \ell} (A_{tot}^{\ell}(\mathrm{x}) , \  A_{tot}^{\ell}(\mathrm{x'}) ) & \\
& \MAC_{\rho, m, \mathrm{x}}^{AE} = 
\sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}}| \frac{A_{p,q,c}(\mathrm{x})}{\max_{\rho, m}^E} -\frac{A_{p,q,c}(\mathrm{x'})}{\max_{\rho, m}^E}| & \\
& \MAC_{\rho, m, \mathrm{x}}^{SE} = 
\sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}} \left( \frac{A_{p,q,c}(\mathrm{x})}{\max_{\rho, m}^E} -\frac{A_{p,q,c}(\mathrm{x'})}{\max_{\rho, m}^E}\right) ^2 & \\
& \MAC_{\rho, m,\mathrm{x}}^{cosine} = \left( 1- 
\frac{A(\mathrm{x}) \cdot A(\mathrm{x'}) }{|A(\mathrm{x})|\cdot |A(\mathrm{x'})|}\right) * \tfrac{1}{2} &
\end{align}

For each distance metric we aggregate results over a set of sample images for all models using the same value of $\rho$:

\begin{align}\label{eq:ace_metric}
& ACE_{metric} = \frac{1}{|M_\rho|\cdot |\mathcal{X}| }\sum_{m}^{M_{\rho}} \sum_{\mathrm{x,x'}}^{\mathcal{X}} Metric(\mathrm{x,x'})
\end{align}

We find it necessary to make the following distinction: A filter's relevance can change strongly when intervening on the watermark. But if the importance of the watermark is not perceivable from the associated conditional attribution map, it is not possible to decipher which concept that filter encodes without access to ground-truth concepts. Reasons why that could happen are, for example, scaling differences between two heatmaps or attribution in areas of the image that humans do not perceive as regions of interest (like the area of the watermark).
So although these metrics are constructed to measure the causal effect of intervention on the explanation as closely as possible, they down-play one of the core ideas of local attribution methods' potential usefulness. If adding or removing a watermark has a significant effect on the attribution to some pixels far away from it, this is not in line with desirable properties of a good explanation such as interpretability or even fidelity to the ground truth importance. It is likely that through the coupling of the spurious feature in our experiment, the changes of (especially pixel-wise) relevance do not only affect the watermark region itself but at least reduce the shape's importance too. 
Another potential downfall for human intuition is the way more localized concepts are understood in comparison to more global or wide-spread concepts which \citet{Achtibat2022} point out in their work. We therefore want to harness the potential of local concept importance for the application scenario with a spatially separated watermark.
In the \cref{section:region_specific} we thus introduce measures which concentrate on the attributed relevance within the ground-truth region and relative to the rest of the attribution map. This, to a degree, also reduces the complexity of what has to be interpreted from the explanation.
