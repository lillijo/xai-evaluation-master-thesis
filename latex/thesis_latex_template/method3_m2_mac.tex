
\section{CRP Explanation Importance $m_2$}\label{section:measure}
Our goal is to compare the causal effect of an upstream intervention on the true feature importance $m_1$ and then the explained feature importance $m_2$. After having defined ways to measure the models true feature importance, we need to repeat the process for the concept-based explanation produced by CRP. It is not well known how humans perceive changes in an explanation and there is no agreed upon scale of importance, so we believe it is best to construct multiple measures to test against each other. Some of the proposed metrics are derived from existing work on evaluating feature importance for local attribution methods \cite{Sixt2020, Karimi2023, Arras2022}.\\

Each candidate should be a variation of measuring the average causal effect of intervening on $W$ on an explanation $e$:
\begin{center}
\begin{equation}
\displaystyle ACE = \mathbb{E} [e \ | \ do(W=1) ] - \mathbb{E} [ e \ | \ do(W=0) ]
\end{equation}
\end{center}

The core question is, whether this effect over different $\rho$ is the same on the explanation and the prediction. A perfect explanation assigns just as much importance to a feature as the model. This is one way to describe the fidelity of the explanation to the model. Some of the proposed measures however partly incorporate other notions of goodness applied for XAI such as \textit{compactness} \cite{Nauta2023}. This also has the practical reason that while an attribution map is visually compelling, it is not a very concise description of relevance. Measuring the causal effect purely on the saliency maps also has the disadvantage that effects invisible or non-distinguishable by humans are accounted for. 
We thus aim to estimate the explanation importance not only through the attribution maps but also in more compact forms. \\

The measures introduced in the following are roughly ordered from measures being most true to the numerical effect of intervention on the explanation to measures reducing the complexity of the explanation. Although human perception is not part of our evaluation, aiming for less complex explanations seems to be more in line with a \textit{good} explanation. While the first measure would require a pixel-wise comparison of multiple heatmaps to find differences, later metrics attempt to work with reduced and hopefully more human-understandable abstractions. \\

Related work often removes negative relevances in attribution maps to enable comparison with XAI methods that do not measure negative relevance. Another reason could be that incautious aggregation of attribution results with negative and positive relevance introduces cancelling-out effects. We believe, however, that a spurious feature can be negatively as well as positively attributed for a model to be biased. Therefore, the measures equally incorporate both the magnitude of the positive and negative relevance. In our scenario with a binary classification task it is reasonable to assume that whatever is positively attributed for one class, should be negatively attributed to the other with the same magnitude, which simplifies the experiment.

\subsection{Mean Attribution Change}\label{section:measure_mac}
The likely most straight-forward way to calculate explanation importance for neurons in a layer using CRP is to measure the average causal effect of an intervention on the attribution directly. This approach aims to emulate the \textit{mean logit change} of the prediction for the concept-based explanation. In a given layer $\ell \in L$, the relevance of each of the neurons $c \in \ell$, which we described as $R_c^{\ell}(\mathrm{x})$ in \cref{section:explanations_with_crp} and the pixel-wise attribution maps $A_c(\mathrm{x})$ can be used for this.

The causal effect of intervening on the spurious watermark feature on one neuron is then the difference between its attribution map $A_c$ or relevance $R_c^{\ell}$ of one image with a watermark and the same image without a watermark. If the model has indeed learned separate concepts for each feature, the same effect should become visible both for the attribution maps and the aggregated relevances.\\

To look at the difference in explanation, we compare multiple types of dissimilarity between two heatmaps (i.e. 2-dimensional pixel maps). Firstly, we look at a normalized absolute pixel-wise difference, or absolute error ($AE$). The Euclidean distance or error ($EE$) and mean squared (euclidean) error $SE$ also seem natural approaches. Because we compute the $cosine$ distance for the relevance values $R_c^{\ell}$ as proposed by the authors of CRP \cite{Achtibat2023}, we also compute the measure for the attribution maps for completeness.
For the 2-dimensional heatmaps we facilitate the kernelized treatment effect which Karimi et al. \cite{Karimi2023} also use in their experiment. It is simplifying the computations in the distance metrics for high-dimensional data by applying 2D convolutions. For one-dimensional vectors like the predictions and the relevances, this corresponds to a simple dot product. \\ 

A sensible normalization for a set of attribution maps is not as trivial as for the output vector or the relevance vector. The naive approach would be, to take the maximally opposing images difference. But it is not to be expected that any method would assign positive or negative values of full magnitude to every pixel. Instead, the results of attribution methods only sparsely assign attribution to few pixels, usually within regions of objects and not to the background. 
We therefore have to find a smaller divisor which scales a maximal difference between a set of attribution maps with and without watermark to approximately 1.
To adjust for the sparsity of the attributions, we take the maximal sum of absolute values of all heatmaps over all samples for one model. This \textit{maximal absolute sum} scaling is very similar to what Achtibat et al. \cite{Achtibat2022} propose for the normalization of relevances of a layer, yet applied to more dimensions and multiple samples.\\

Here we show the already mentioned variants of distance metrics for the attribution map matrices. $A(\mathrm{x})$ represents an array of all $|\ell|$ attribution maps of the concepts $c$ in a layer. The same metrics are computed for the relevance vector, which in our case consists of $|\ell|$ normalized relevance values $R_{c}^{\ell,(norm)}$. For these, the computation is considerably simplified as no aggregation per pixel is necessary. In principle, each of these distance metrics should produce comparable results. However, like Karimi et al. \cite{Karimi2023} we want to ensure that the choice of a distance metric has no adverse interaction with our results and therefore compare multiple candidates.

\begin{align}
\displaystyle 
& A_{tot}^{\ell}(\mathrm{x}) = \sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}} |A_{p,q,c}(\mathrm{x})|  \label{eq:total_absolute_relevance}  \\
& \max_{\rho, m}^E = \max_{\mathrm{x} \in \mathcal{X}} (A_{tot}^{\ell}(\mathrm{x}) , \  A_{tot}^{\ell}(\mathrm{x'}) )\\
& \MAC_{\rho, m, \mathrm{x}}^{AE} = 
\sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}}| \frac{A_{p,q,c}(\mathrm{x})}{\max_{\rho, m}^E} -\frac{A_{p,q,c}(\mathrm{x'})}{\max_{\rho, m}^E}|\\
& \MAC_{\rho, m, \mathrm{x}}^{SE} = 
\sum_{c \in \ell} \sum_{(p,q) \in \mathrm{x}} \left( \frac{A_{p,q,c}(\mathrm{x})}{\max_{\rho, m}^E} -\frac{A_{p,q,c}(\mathrm{x'})}{\max_{\rho, m}^E}\right) ^2  \\
& \MAC_{\rho, m,\mathrm{x}}^{cosine} = (1- 
\frac{A(\mathrm{x}) \cdot A(\mathrm{x'}) }{|A(\mathrm{x})|\cdot |A(\mathrm{x'})|}) * \tfrac{1}{2}
\end{align}

For each distance metric we aggregate results over a set of sample images for all models using the same value of $\rho$:

\begin{align}\label{eq:ace_metric}
& ACE_{metric} = \frac{1}{|M_\rho|\cdot |\mathcal{X}| }\sum_{m}^{M_{\rho}} \sum_{\mathrm{x,x'}}^{\mathcal{X}} Metric(\mathrm{x,x'})
\end{align}

\paragraph{Concept Relevances $R_c^{\ell}$}
Without looking at the attribution map's individual pixels, the summed absolute relevance of certain neurons ($R_c^{\ell}$) should still change when intervening on the watermark ($W$) feature. If the neural network indeed follows different strategies when the spurious feature is present versus when it is not, then different concepts should be relevant depending on the feature's value. 
The relevances of concepts, i.e. filters in a layer of the network, give us no information on which (potentially human-understandable) concepts they activate, only how important they are. The authors of CRP still look at concept relevances extensively as they find them to weight abstract and disentangled concepts well for the large image datasets they look at. While for the attribution maps $A_c$ the aim was to extract the importance from the visual explanation, for $R_c^{\ell}$ we only assume that different filters should be at work depending on the presence of the watermark, if it has any significance to the model. Likewise, relevances should not change when the model is not reacting to the spurious feature. 

We find it necessary to make the following distinction: A filter's relevance could change strongly when intervening on the watermark. But if the importance of the watermark is not perceivable from the associated conditional attribution map, it is not possible to decipher which concept that filter encodes without access to ground-truth concepts. Reasons why that could happen are, for example, scaling differences between two heatmaps or attribution in areas of the image that humans do not perceive as regions of interest (like the area of the watermark).
So although these metrics are constructed to measure the causal effect of intervention on the explanation as closely as possible, they down-play one of the core ideas of local attribution methods' potential usefulness. If adding or removing a watermark has a significant effect on the attribution to some pixels far away from it, this is not in line with desirable properties of a good explanation such as interpretability or even fidelity to the ground truth importance. It is likely that through the coupling of the spurious feature in our experiment, the changes of (especially pixel-wise) relevance do not only affect the watermark region itself but at least reduce the shape's importance too. 
Another potential downfall for human intuition is the way more localized concepts are understood in comparison to more global or wide-spread concepts which Achtibat et al. point out in their work \cite{Achtibat2022}. We therefore want to harness the potential of local concept importance for the application scenario with a spatially separated watermark.

In the \cref{section:region_specific} we thus introduce measures which concentrate on the attributed relevance within the ground-truth region and relative to the rest of the attribution map. This, to a degree, also reduces the complexity of what has to be interpreted from the explanation.
