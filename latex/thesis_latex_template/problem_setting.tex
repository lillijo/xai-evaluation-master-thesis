\chapter{Problem Setting}\label{chapter:problem_setting}
{ \color{red}

    about 1-2 pages

    \begin{itemize}
        \item what is the defined goal of this thesis? what are sub-goals and what are side-products
        \item strategy
        \item coarse overview of steps
    \end{itemize}
}
% We add to the field of evaluating explanation methods for neural networks by systemically comparing the explanation importance of a feature with the ground truth importance in the trained model as well as the data generating ground truth.

The aim of this thesis is to compare the causal effect of intervening on a variable in the data generation process on the output of the trained model to the effect on the explanation. To this end, we devise a structural causal model mirroring realistic causal pathways in image data to generate a toy dataset, which is detailed in \autoref{section:causal_model} .


In distinction to others, who see importance more as a binary variable, we look at it in a continuously changing fashion. This approach enables the hypothesis that a good explanation method's curve of importance for a feature should match the curve of true model importance closely. It is motivated by the view that a truly unbiased training data is not achievable for realistic problems and instead a balance between the importance of features has to be decided. 

 
The value of the latent factor, whose effect is measured, can be thought of as a coupling ratio between the truly important feature and a spuriously correlated feature in our first experiment. We formally name this the \textit{measure} $m_0$ which is used for the comparison against the ground truth model importance measure $m_1$ and our explanation importance measure $m_2$. Measure $m_1$ is the ground truth of how strongly a model reacts to a changing latent factor and is constructed in \cref{section:gt_measure}. 


Our hypothesis is that if the similarity of $m_1$ and $m_2$ is high, we have potentially found a good measure of explanation fidelity and can also make claims to the explanation being good. However, this can only be confirmed by studying further generating causal models and interventions with other latent causal features (\cref{section:further_problems}). 


The authors of CRP claim that through looking inside of the model and computing conditional relevances of (sets of) hidden features, an explanation can not only answer \textit{where} but also \textit{what} questions \cite{Achtibat2022}. In our toy dataset both of those questions are not hard to answer. Instead we want to follow up with the question of \textit{how much} a feature influenced the decision. Because CRP can split importance into multiple concepts, which it then assigns relevance to, it can possibly answer the \textit{how much} more accurately than the general results of back-propagation methods can.  


To give justice to the potential for human intuition of the concept-based method we investigate, our goal is to develop a metric that also measures how well a feature importance can be interpreted. This objective requires us to incorporate notions of \textit{disentanglement, robustness} and \textit{interpretability} into the metric. Comparing only the total numerical change of our explanations relevance values when intervening, while it might be accurate, does not necessarily represent the understandable effect of our intervention. 
Candidates for this measure ($m_2$) of feature importance are formally introduced in \autoref{section:measure}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{thesis_latex_template/pics/test.png}
    \caption{Problem Setting/Pipeline}
    \label{fig:pipeline}
\end{figure}

{ \color{gray}


\section{Other Stuff I still somehow want to put in problem setting or related work???}


We add to the field of evaluating explanation methods for neural networks by systemically comparing the explanation importance of a feature with the ground truth importance in the trained model as well as the data generating ground truth. To achieve this, we intervene on a generating factor in an underlying causal model of a toy dataset, namely the coupling ratio between a core feature and a spurious feature. By doing this we can establish a ground truth of how much a model actually uses a spurious feature in relation to how correlated that spurious feature is to the core feature in the data. This then helps, to investigate how an explanation method follows the models importance versus how much it explains the underlying data distribution. While Sundararajan et al. \cite{Sundararajan2017} propose \textit{implementation invariance}, stating that 2 networks producing equal outputs for all inputs should attribute identically too, Kindermans et al. \cite{Kindermans2019} argue for \textit{input invariance} requiring that ''the saliency method mirror the sensitivity of the model with respect to transformations of the input''.

Does the additional information of concept importances potentially aid in estimating
relevance of spurious features  more accurately (to the ground truth importance than a single heatmap would)?

\begin{itemize}
    \item missingness as a feature is always ignored, in my example there is evidence that it is indeed used as a strategy (rectangle is red, but ellipse will be equally red when watermark is missing but identify the wrong thing: impression is that the shape itself is important, however the missingness of the watermark seems to just value the other feature higher as an artifact)
    
    \item the whole constant vector shift problem is kinda nonsensical, we cannot expect AI models to accurately predict out-of-distribution so why should we expect the explanation to be accurate. Its like saying: "explain to me why you don't like tomatoes" to a person loving tomatoes. The answer to an ill-posed question is per-se ill-defined
    \item that is actually something that might be more generalizable to saliency maps in general: maybe the question "Which pixels positively influenced your decision?" can be ill-posed too, or might not give an answer well suited as the explanation to a models decision
    \item example: when I accidentally normalized the images after adding the noise, some models seemed to just learn the class by looking at the average over the whole image or something like that. The heatmaps could not tell me what feature was important because the feature is not "visible" like that
    and again the example with the rectangle as well as ellipse shape both being seemingly positive for the rectangle class, when in reality the only telling feature is the watermark itself
\end{itemize}

\cite{Dreyer2023a}
- similar datasets, biasing in localized (i.e. watermark, tie) or unlocalized way
- localized bias can be discovered and "unlearned" quite well, while it mostly fails for unlocalized bias
- "It is still an open question, how to choose the optimal layer for bias correction."
}

