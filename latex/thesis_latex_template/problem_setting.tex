\chapter{Problem Setting}\label{chapter:problem_setting}

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/pipeline_clean.png}%
    \caption[Experiment Pipeline]{Visual Summary of the steps of our experiment (best viewed digitally)}
    \label{fig:pipeline}
\end{figure}

The aim of this thesis is to compare the reaction of model and explanation towards spuriously correlated features using a causal framework.
We want to answer the question whether the causal effect of an intervention on a meta-variable in the causal data generation process on the explanation is related to that of the model or whether the explanation reacts to a shift in the data distribution in other ways than the model. 
To this end, we devise a structural causal model seeking to mirror realistic causal pathways in image data to generate a toy dataset. This is then embedded into a broader causal process describing how model predictions and explanations are generated (see \cref{section:causal_model}).
We aspire to extract the measurable relative feature importance of a confounded, but not causally related, feature for both the trained model and its explanation. In essence, this is the causal effect of intervening on this spurious feature conditional on its coupling with the target feature. However, for the concept-based explanation method CRP we evaluate, we have to first establish a way of measuring relative feature importance. 
Throughout this thesis, we thus construct multiple hypotheses of how to measure relative feature importance within an explanation.

The authors of CRP claim that through looking inside of the model and computing conditional relevances of (sets of) hidden features, an explanation can not only answer \textit{where} but also \textit{what} questions \citep{Achtibat2022}. Hypothetically, this makes CRP more apt for explaining relative feature importance, than general local attribution methods, which is why we focus on this method in our work.
In our first toy problem with a spatially separated and easy to identify spurious feature both of the questions seem trivial to answer. Additionally we want to hence follow up with the question of \textit{how much} a feature influences the decision. Because CRP supposedly splits importance into multiple concepts, which it then assigns percentage-like relevance to, it could answer the \textit{how much} question, i.e., of relative feature importance, more intrinsically than general local attribution methods. The second toy problem mostly tests the constructed measures for a different scenario: Here, the spurious feature is spatially overlapping with the core feature and therefore the \textit{what} question is not trivial to answer anymore. Still, both \textit{what} and \textit{how much} should be just as measurable for such a case too by our metrics.

In distinction to other work, which looks at ground-truth importance mostly as a binary variable, we look at it in a continuous way. This approach enables the proposition that a good explanation method's curve of relative importance with regards to the actual coupling within data should match the curve of the models relative use of the feature closely. It is motivated by the view that features in training data for realistic problems are rarely completely independent and disentangled and instead a balance of the relative importance of multiple features has to be decided. For that, it is also necessary to create a frame of reference. There is no inherent way to define the interval in which relative feature importance resides for one data instance, explanation method and dataset. By varying the association between the spurious and target feature from completely independent (zero) to deterministically dependent (one) we hope to establish such a reference, enabling relative comparisons. 

The value of the meta variable, whose effect is measured, can be thought of as a coupling ratio ($\rho$) between the target feature and a spuriously correlated feature in the data distributions of our experiments. We name this the \textit{measure} $m_0$ which is used for the comparison against the ground truth model importance measure $m_1$ and the explanation importance measures $m_2$. Measure $m_1$ is the ground truth of how strongly a model reacts to a changing latent factor and is constructed in \cref{section:gt_measure}. For $m_2$ we examine multiple possibilities of measuring relative feature importance for explanations produced by concept relevance propagation. The candidates are introduced in \autoref{section:measure}. 

In essence, we compute the causal effect of intervention on a spurious feature $W$ on the models prediction $Y$, denoted as $m_1$, and on its explanation $E$, denoted as $m_2$, in relation to the coupling ratio $\rho$ or $m_0$:

\begin{align}\displaystyle
m_{0} & = \rho \in [0,1] \\
 m_{1} & = \tfrac{\partial}{\partial_{\rho}} \left\{ {\partial_w}\mathbb{E} [ Y \ | \ do(W=w), \rho ] \right\}  \\
 m_{2} & = \tfrac{\partial}{\partial_{\rho}} \left\{{\partial_w}\mathbb{E} [ E \ | \ do(W=w), \rho ] \right\}
\end{align}

To give justice to the potential for human intuition of the concept-based method we investigate, another goal is to take into account how well relative feature importance is extractable for humans. Comparing the total numerical change of relevance per pixel when intervening, while it might be accurate, does not represent the understandable effect of an intervention. We aim to include the complexity of what has to be extracted from the explanation into our measures. We thus test whether a reduced \textit{gist} of what humans could take from a concept-based explanation produced by CRP is similarly related to the models true relative feature importance as the whole output of the explanation method. 

Our hypothesis is on the one hand that if the similarity of $m_1$ and a manifestation of $m_2$ is high, we have potentially found a good measure of explanation fidelity and can also make claims to the explanation being faithful in terms of spurious feature importance. On the other hand, if a measure accurately follows the true relative feature importance, this does not prove that humans can read that information from the explanation. We hence analyze the limitations of the explanation and the constructed measures on a spatially separated spurious feature but especially on an overlapping spurious feature. 

In this approach lies the implicit assumption that the question ``which pixels positively or negatively influence the decision?'' can be ill-posed, or might not give an answer well suited as the explanation to a models decision. For example, when an objects texture is a Clever-Hans feature, the importance can be within the true boundary of the object to be learned, even though the wrong feature has been learned. There is evidence that even using concept-conditional explanations, like CRP does, is not fully enabling to identify such non-localized or non-separable biases \citep{Dreyer2023a}. 

