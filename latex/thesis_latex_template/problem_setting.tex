\chapter{Problem Setting}\label{chapter:problem_setting}

\begin{figure}[t!]
    \advance\leftskip-2.3cm
    \includegraphics[width=1.4\textwidth]{thesis_latex_template/pics/pipeline_clean.png}%
    \caption[Pipeline]{Visual Summary of the steps of our experiment (best viewed digitally)}
    \label{fig:pipeline}
\end{figure}

The aim of this thesis is to compare the reaction of model and explanation towards spuriously correlated features using a causal framework.
We want to answer the question whether the causal effect of an intervention on a meta-variable in the causal data generation process on the explanation is exactly that of the model or whether the explanation reacts to a shift in the data distribution in other ways than the model. 
To this end, we devise a structural causal model seeking to mirror realistic causal pathways in image data to generate a toy dataset, which is detailed in \autoref{section:causal_model}. This is then embedded into a broader causal model describing how model predictions and explanations are generated.
We aspire to extract the measurable relative feature importance of the confounded but not causal feature for both the trained model and its explanation. In essence, this is the causal effect of intervening on this spurious feature in relation to its coupling with the target feature. 
Throughout this, we construct hypotheses of what makes relative feature importance \textit{measurable} within an explanation.

The authors of CRP claim that through looking inside of the model and computing conditional relevances of (sets of) hidden features, an explanation can not only answer \textit{where} but also \textit{what} questions \cite{Achtibat2022}. Hypothetically, this makes CRP more apt for explaining relative feature importance, which is why we focus on this method in our work.
In our first toy problem with a spatially separated spurious feature both of the questions are not so hard to answer. Instead we want to follow up with the question of \textit{how much} a feature influences the decision. Because CRP supposedly splits importance into multiple concepts, which it then assigns percentage-like relevance to, it could answer the \textit{how much} question, i.e. of relative feature importance, more intrinsically than general local attribution methods. The second toy problem mostly tests the constructed measures for a different scenario: Here, the spurious feature is spatially overlapping with the core feature and therefore the \textit{what} question returns to the center of attention. The \textit{how much} should however be just as measurable for such a case.

In distinction to other work, which looks at ground-truth importance mostly as a binary variable, we look at it in a relative and continuous way. This approach enables the hypothesis that a good explanation method's curve of relative importance with regards to the actual coupling within data should match the curve of the models relative use of the feature closely. It is motivated by the view that features in training data for realistic problems are never completely independent and instead a balance of the relative importance of features has to be decided. For that, it is also necessary to create a frame of reference. There is no inherent way to define the interval in which relative feature importance resides for one data instance, explanation method and dataset. By intervening on the causal data generation process we therefore hope to establish expected values of our measures with regards to the true feature importance.

The value of the meta variable, whose effect is measured, can be thought of as a coupling ratio between the truly important feature and a spuriously correlated feature in our experiments. We name this the \textit{measure} $m_0$ which is used for the comparison against the ground truth model importance measure $m_1$ and the explanation importance measures $m_2$. Measure $m_1$ is the ground truth of how strongly a model reacts to a changing latent factor and is constructed in \cref{section:gt_measure}. For $m_2$ we examine multiple possibilities of measuring relative feature importance for explanations produced by concept relevance propagation. The candidates are formally introduced in \autoref{section:measure}. 

To give justice to the potential for human intuition of the concept-based method we investigate, the goal is to also take into account how well relative feature importance is extractable for humans. Comparing the total numerical change of relevance per pixel when intervening, while it might be accurate, does arguably not represent the understandable effect of an intervention. We aim to include the complexity of what has to be extracted from the explanation into our measures. We thus test whether a reduced \textit{gist} of what humans could take from a concept-based explanation produced by CRP is similarly related to the models true relative feature importance as the whole explanation. 

Our hypothesis is on the one hand that if the similarity of $m_1$ and an instance of $m_2$ is high, we have potentially found a good measure of explanation fidelity and can also make claims to the explanation being faithful. On the other hand, if a measure accurately follows the true relative feature importance, this does not prove that humans can extract that information from the explanation well. We hence analyze the limitations of the constructed measures on the spatially separated spurious feature but especially on the overlapping spurious feature. 

In this approach lies the implicit assumption that one simple saliency map is not enough to identify relative feature importance of spurious features. The concept-based explanation might relieve some of the grievances with general local attribution but also increases complexity of an explanation. 
The question \textit{``which pixels positively or negatively influenced your decision?''} can be ill-posed, or might not give an answer well suited as the explanation to a models decision. For example, when an objects \textit{texture} is a Clever-Hans feature, the importance can be within the true boundary of the object to be learned, even though the wrong feature has been learned. There is evidence that even using concept-conditional explanations, like CRP does, is not fully enabling to identify and remove such non-localized or non-separable biases \cite{Dreyer2023a}. 

Our findings on small benchmark problems will have to be confirmed by applying the framework to varying generating causal models and interventions with other latent causal features in the future. 
It is also necessary to compare the results not only for the concept-based explanations generated by CRP but, where applicable, also for other explanation methods.

