\chapter{Theoretical Background}\label{chapter:background}

{ \color{red} 

about 20-30 pages (rather less I guess?)

\begin{enumerate}
    \item Introduction to XAI in general
    \item Evaluation of XAI methods in general
    \item Structural Causal Models and causal framework
\end{enumerate}
 }
 \todo{write background}

\section{Neural Networks}
\begin{itemize}
    \item Explain all general concepts that are needed for understanding CRP etc
    \item layers
    \item neurons
    \item convolutional blocks/ layers
    \item activation functions (especially ReLU)
    \item other types of layers? 
    \item backpropagation / forward
\end{itemize}

\section{Layerwise Relevance Propagation}
\begin{itemize}
    \item formula(s) for LRP
    \item explanation of LRP and briefly maybe Deep Taylor Decomposition
    \item overview of propagation rules \cite{Montavon2019}  \todo{understand all rules etc of LRP/CRP}
    \item current best practices for LRP and what is used for CRP
\end{itemize}

\section{Concept Relevance Propagation}
\begin{itemize}
    \item theoretical idea of Concept Relevance Propagation and what it seeks to improve
    \item Explain formula of conditional relevance in detail
    \item some examples of usage:
    \begin{itemize}
        \item relevance scores for \textit{concepts} (=neurons)
        \item relevance maximization images
        \item conditioning on single concepts/ neurons ...? 
        \item attribution graph 
    \end{itemize}
\end{itemize}


\section{Causal Framework}
\subsection{Structural Causal Models}

\begin{itemize}
    \item Explain and define in detail Structural Causal Models
    \item neural networks could be seen as SCMs \cite{Chattopadhyay2019}
    \item but AI / neural networks in general do not care about causation and work through finding useful correlations
    \item and that is good this way, otherwise they would never find anything useful, statistics and correlations are great
    \item none-the-less the better we get at identifying spurious features the more causal methods might apply? 
    \item it doesn't matter whether the network has found the actual causal reasons for its prediction, but explanations are a distinctively causal concept.
    \item and explanation asks how and why, so we want to know the cause of model predicting Y from X
    \item causal methods have started to be used for evaluation of xai
\end{itemize}

\subsection{Interpretation as Interventions}
???

\subsection{Data Generation Process}

Other?

\begin{itemize}
    \item Short introduction to causal effects
    \item counterfactuals
    \item 
\end{itemize}


\section{Evaluation of Explanations}
\subsection{Ground Truth Importance}
\begin{itemize}
    \item What are currently used ground truth importance measures for concepts or latent factors
    \item introduce Prediction Flip with formula or application to our use case 
    \item R2 score with formula \cite{Sixt2020}
    \item mean logit change with formula
    \item make clear: human understanding is the ultimate goal, so user studies are the gold standard (but often not well done) but not feasible here
    \item relate to constant vector shift problem and how this might be measured
\end{itemize}
\subsection{CRP Concept Importance Measures}
\todo{need proper measure}
\begin{itemize}
    \item explain the measures i use to score how well the concepts are separated
    \item show theoretical basis
\end{itemize}
\subsection{Causally somehow? }