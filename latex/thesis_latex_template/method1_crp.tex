\section{Generating Explanations with Concept Relevance Propagation}\label{section:explanations_with_crp}
The previously described causal framework can be applied to a multitude of explanation methods as it is principally model- and method-agnostic. However we limit our analysis to CRP and interpretation techniques constructed with CRP. This is mostly due to the time limitations of a master thesis, but also because its authors specifically claim in their paper to be able to identify relative importance using CRP. 
Producing explanations using concept relevance propagation requires decisions on the backpropagation rule(s), on the conditioning sets and further hyperparameters. 
We follow the recommendations and default settings of CRP's authors \cite{Achtibat2022, Achtibat2023} and best practices \cite{Kohlbrenner2020} as closely as possible.
For the backpropagation we apply the $LRP_{\varepsilon -z^+- b^-}$ - rule as recommended by \cite{Kohlbrenner2020}. Due to the simplicity of our CNN model no more model canonization steps need to be applied. See \cref{appendix:lrprules} for further technical details. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{thesis_latex_template/pics/conditional_heatmaps.png}
    \caption[Comparing Attribution Maps of Layers]{Concept-conditional heatmaps for one example image. Note the Sobel-filter-like attributions in earlier layers and the more combined attributions of watermark and shape in later layers.}
    \label{fig:cc_heatmaps}
\end{figure}

% Maybe say something about how concept atlas and hierarchical attribution graphs can help us decide which concepts seem to be most disentangled (if heatmaps are indeed correct that is). THen show image below to visualize how to choose? 

%\begin{figure}[t!]
%    \centering  
%	\includegraphics[width=\textwidth]{thesis_latex_template/pics/concept_atlas_all.png}
%    \caption[Concept Atlas and Hierarchical Attribution Graph]{Concept Atlas and Hierarchical Attribution Graph for an image in our dataset, looking at how negative or positive relevance flows from the output through each layer. Here, concept 0 and 7 seem to redundantly encode the right half of the ellipse, while 5 and 6 encode the watermark in the last convolutional layer. }
%    \label{fig:attr_graph}
%\end{figure}

Principally, neurons in every layer of a model can be conditioned on using the CRP-approach. However, the resulting attribution maps are not necessarily depicting disentangled \textit{and} abstract enough concepts. When looking at \textit{concept-conditional} attribution maps from the earlier layers, one will likely see low-level features akin to Sobel-filter. In the late, fully connected layers before the output the previously disentangled concepts might get mixed together again for the final decision. In \cref{fig:cc_heatmaps} an example shows the tendency from trivial to abstract \textit{concepts}. 
According to \cite{Dreyer2023a} referring to \cite{Zeiler2013} the \textit{last convolutional layer} is ''most likely representing disentangled representation''. For comparison we investigate both the last convolutional and the first fully connected layer of the model. It is not entirely clear whether the extremely small size of our model hinders a transfer of the results to realistic scenarios. Yet, training significantly larger models would have been too computationally expensive for this principled approach, requiring many trained models.

In the following we will reiterate the steps necessary to produce the different components of CRP-explanations:

\subsubsection{Concept-Conditional Attribution Maps and Relevance for Prediction}
The concept-conditional backpropagation rule described in \cref{section:crp_background} can be applied to arbitrary sets of neurons (filters) $\theta$. In our scenario we create class-specific attribution maps conditioned on each individual neuron (termed concept $c$) in the selected layer. For this, we use the output for the ellipse class as the initialization for relevance, keep all other layers untouched and then mask out the desired neurons $c$ relevance in the layer $\ell$: 
\begin{equation}
    R^{\ell}(\mathrm{x} |\theta_{c}) = \sum_{i} R_i^{\ell}(\mathrm{x} |\{y=1\} \cup \theta_{\ell} = \{c\})
\end{equation}
Here $i$ represent all relevances in lower layers that are a part of the concept $c$ and not masked out. 
To yield the attribution map, the importance is backpropagated through all layers till the input layer $\ell = 1$ and not summed per input feature $i$, producing individual relevances  termed $R_{i}^{1}(\mathrm{x} |\theta_{c})$. In the following we will refer to this concept-conditional attribution map as $\mathcal{A}_c(\mathrm{x})$ and the class specific relevance of concept $c$ as $R_c^{\ell}(\mathrm{x})$. Due to the \textit{conservation laws} that CRP inherits from LRP (Equation 7 \cite{Achtibat2022}) the relevances $R_c^{\ell}$ within one layer can be interpreted as the \textit{percentage} of importance going through concept $c$. To enable this view also for out-of-distribution samples the authors recommend normalizing the relevances to sum to 1:
\begin{align}
    R^{\ell}_{norm}(\mathrm{x} |\theta_{c}) = \frac{R^{\ell}(\mathrm{x} |\theta_{c}) }{\sum_k |R_k^{\ell}(\mathrm{x} |\theta_{c})|}
\end{align}

\subsubsection{Local Concept Importance}
To find out how much a certain region contributes to the overall prediction but also which concepts are most highly activated within that region, the authors propose local concept importance. 
This method simply masks out the desired region (or input pixels), within the concept-conditional attribution map and sums the relevance within that mask. 

\begin{align}\label{eq:local_importance}
    R_{B}^{\ell}(\mathrm{x} | \theta_c) = \sum_{(p,q) \in B} R_{p,q}^{\ell}(\mathrm{x} | \theta_c)
\end{align}
Again, for readability we from now on refer to this local concept importance in a region $B$ as $R_B$

\subsubsection{Relevance Maximization}
As described in the background \cref{section:crp_background}, CRPs authors also use concept-conditional relevance to create prototypical reference sets of each concept. In a human subject study \cite{Achtibat2023} they find this method to be useful for the identification of Clever-Hans artifacts where only using heatmaps fails. We therefore aim to include this technique into our analysis. The reference sets consist of up to 40 images with maximal relevance per neuron. Each image can be further concentrated on the encoded concept thresholding the relevance and by cropping it to the receptive field of the neuron in question. This yields a set of cropped images which ought to describe the encoded concept. 

As mentioned by \cite{Achtibat2022} using the whole dataset might make the selected images too similar to each other. Hence we compute the maximization for a subset of only 300 images to enforce more variance. This is especially necessary for the dSprites images because the difference between images is small.  Importantly, the sample we select from does not have an association between $S$ and $W$, meaning that watermarks (or blurry patterns) are equally likely to occur on rectangle as on ellipse images. 
The provided code produces sets of 40 images for each neuron in each layer with either relevance or activation maximized using the sum or maximum (conditional) relevance/activation for an image. After preliminary experiments we use the default option of maximizing for the sum of relevance of an image for a concept. 
The implementation of this and all previously described methods is made accessible by CRPs authors at
\url{https://github.com/rachtibat/zennit-crp}. 

Relevance maximization reference sets are denoted the following way:
 \begin{align}
& \mathcal{T}_{sum}^{rel} (\mathrm{x}) = \sum_{c} A_c(\mathrm{x}|\theta) \\
& \mathcal{T}_{max}^{rel} (\mathrm{x}) = \max_{c} A_c(\mathrm{x}|\theta) \\
&\mathcal{X}^{\star} = \{ \mathrm{x}_{1}^{\star},\mathrm{x}_{2}^{\star},..., \mathrm{x}_{n}^{\star} \} = arg \sort_{\mathrm{x} \in \mathcal{X}} \mathcal{T}(\mathrm{x}) \\
& \mathcal{X}_{k}^{\star} = \{ \mathrm{x}_{1}^{\star},\mathrm{x}_{2}^{\star},..., \mathrm{x}_{k}^{\star} \} \subseteq \mathcal{X}^{\star}
 \end{align}