\chapter{Introduction}\label{chapter:introduction}

\begin{itemize}
\color{red} 
    \item (1-2 pages)
    \item Context: make sure to link where your work fits in Problem: gap in knowledge, too expensive, too slow, a deficiency, superseded technology. Strategy: the way you will address the problem
    \item Outline of the rest of the paper: "The remainder of the paper is organized as follows. In Section 2, we introduce ..Section 3 describes ... Finally, we describe future work in Section 5." (Note that Section is capitalized. Also, vary your expression between "section" being the subject of the sentence, as in "Section 2 discusses ..." and "In Section, we discuss ...".)
    \item Avoid stock and cliche phrases such as "recent advances in XYZ" or anything alluding to the growth of the Internet. 
    \item Be sure that the introduction lets the reader know what this paper is about, not just how important your general area of research is. Readers won't stick with you for three pages to find out what you are talking about.
    \item The introduction must motivate your work by pinpointing the problem you are addressing and then give an overview of your approach and/or contributions (and perhaps even a general description of your results). In this way, the intro sets up my expectations for the rest of your paper -- it provides the context, and a preview.
    \item Repeating the abstract in the introduction is a waste of space.
\end{itemize}

AGAIN: structure what i need to say:
\begin{itemize}
    \item XAI is becoming bigger, so is its evaluation
    \item therefore recently more criticisms, especially about local attribution methods
    \item lack of proper evaluation, "circular criteria (such as fidelity and faithfulness Gevaert et al. (2022))", \cite{Adebayo2018, Sixt2020, Wilming2023} etc.
    \item crp attempts to alleviate some of that, going away from "where" question, which does not seem to hold much information in some cases, to "what" question (is this actually related to recent evaluation stuff???)
    \item therefore we want to evaluate whether crp actually manages to better identify biases using numerical measure
    \item for that we use a new approach with a causally generated ground truth of a simple dataset, otherwise numerical measure of "biasedness" would not be possible
    \item try to make this scm and images as close to a realistic image dataset as possible
    \item we don't attempt to measure importance of feature which is supposed to be important (core feature) but of clever-hans feature
    \item instead of having an either biased or unbiased dataset, we use coupling ratio
    \item it makes it theoretically possible to establish a sort of threshold for feature importance? e.g. if feature more important than core feature, or a third or something?
    \item REFER MORE TO \textit{Are We Explaining The Data Or The Model?}
    \item most other work either has known bias or not. but it is not necessarily quantified how much of that bias was learned by model. 
\end{itemize}

\section{Motivation and Context}
With the explosion in popularity of deep neural networks the need to explain those opaque, black-box models has risen too. Recently, explainable AI (XAI) methods have been scrutinized more closely. Although there is still no consensus on what exactly makes a \textit{good explanation}, numerous potential metrics for the faithfulness, robustness and interpretability of explanations have been made \cite{?}. A promising branch of research seems to be the evaluation of explainable AI methods with causal methods. After all, explanations are, or at least should be, intrinsically causal constructs \cite{Woodward2004, Halpern2005, Schoelkopf2019}.\\

Due to the stronger focus on evaluation of XAI, amongst other methods also local attribution methods have come under criticism for their general lack of faithfulness to the model they are trying to explain \cite{Adebayo2018, Karimi2023}. Also, some methods class-insensitivity \cite{Sixt2020} and their failure in presence of suppressor variables \cite{Wilming2023} and in the \textit{limit of simplicity} \cite{Kindermans2017} have been examined.  Other authors have criticized the lack of comparative user-guided evaluation of explanation methods \cite{Rong2023}. \\

As local attribution methods, especially back-propagation methods, create visually compelling results through attribution maps, they have still become a staple for many AI practitioners, especially for computer vision tasks. 

The recent \textit{glocal} attribution method \textit{Concept-Relevance-Propagation (CRP)} introduced in \cite{Achtibat2022} has been developed for a more fine-grained explanation of a neural networks decisions. Instead of producing one attribution map explaining the overall prediction output such as LRP \cite{Bach2015}, each \textit{concept} in some hidden layer of the network gets assigned a conditional relevance and its own saliency map. In addition to the saliency maps, the relevance scores also act as a metric to maximize when searching representative samples for each of the concepts. According to the authors, through this more detailed explanation one can not only understand \textit{where} a model sees the most relevant features, but also \textit{what} features are relevant in this area. Their pre-assumption is, that the deeper layers of models represent concepts which are human-understandable and therefore aid in the explanation of what the model predicts. In qualitative examinations, as well as a human evaluation study \cite{Achtibat2023}, they find that \textit{Clever-Hans} (i.e. spuriously correlated) features are easier to identify with the concept-based approach and their relevance better determined in comparison to other local attribution methods. Finding whether a background or spuriously correlated feature is biasing a model is in our opinion one of the most important applications of XAI.
\\

Here, we therefore make the attempt to evaluate yet another desirable characteristic of XAI: that its sensitivity to spurious feature importance closely follows that of the network it is trying to explain. The general sensitivity or faithfulness to a model has been studied with varying methods, most often creating perturbations in the input or model at the \textit{important} feature and testing the decline in accuracy. Some recent works have also evaluated an explanations feature importance when spuriously correlated features are present \cite{Yang2019,Kim2018,Parafita2019,Reimers2020,Singla2022}. Arguably, explanations having high fidelity is especially crucial when identifying and quantifying spurious correlations a model has learned. Confirming that the true feature has some importance is not as convincing of an explanation if the importance of other features is not compared to it.
We believe that to establish the true effect of a spuriously correlated feature on a models prediction, it is interesting to also look at it in a continuous and relative fashion. 
In a realistic setting it is hard for a model to not have learned at least some bias. This is especially true because it seems impossible even for humans to know the \textit{true causal model} of how e.g. images are created. So in accordance with ideas of \textit{fairness} it seems impossible to aim for completely unbiased models, as they would need to have all knowable and unknowable knowledge of the universe to not predict 'out-of-distribution'. Instead, one needs to define a measurable threshold of biasedness, which is acceptable for a specific task.  

Therefore we will extend previous work on evaluating the explanation methods fidelity in the presence of Clever-Hans features, by using a known causal model. We extend a simple benchmark dataset with known ground truth to have a structural causal model which generates the data.
While the \textit{core} feature actually determines the class other, \textit{spurious} features are still present and correlate with the core feature in aknown way \cite{Singla2022}.
Knowing the generating factors helps to quantify the ground-truth feature importance of not only the core and the spurious feature but also irrelevant features as a baseline.


With the aim of evaluating fidelity in the presence of a spuriously correlated feature, a zoo of models is trained with varying coupling ratios. Ground-truth biasedness is calculated for each model and each feature. In expectation, the models importance for the core feature declines and for the spurious feature increases as the coupling ratio rises. 


If CRP indeed produces an accurate explanation, more concepts should assign higher relevance to the bias feature the stronger the bias impacts the prediction of the model. It is important to note, that the model might accurately predict based on the real feature even though the bias is strong, when there are enough counterexamples. \Cref{fig:tesfigure} shows the non-linear interaction between prediction accuracy and coupling ratio.
Now the question is, whether CRP can correctly identify this non-linear relationship or whether CRPs attribution to the spurious feature will more closely follow its actual presence in the data. 
In other words: Does CRP learn the causal effect of the spurious feature on the model or just the causal effect within the data? Our goal is to quantify the effect that CRP actually has on human understanding. So even if the overall importance of the watermark can be either denied or affirmed, the numeric importance might not be the same as what a user can see and find through heatmaps, relevance hierarchies and relevance maximization image sets. Therefore it is necessary to develop methods which quantify human understanding of biasedness?  

\section{Contributions}
\todo{refine strategy based on what i actually did}
\begin{itemize}
    \item Construct a causal model of the explanation generation process
    \item Construct a causal data generating model embedded into the larger model
    \item Train a succession of neural network instances by intervening on a meta variable that determines the correlations in the data distribution
    \item Establish a ground-truth of model accuracy and feature importance in relationship to the intervened factor
    \item Evaluate explanations in a concept-based approach using concept relevance propagation (CRP)
    \item Construct X metrics describing the effect of the intervention on the explanations feature importance 
    \item Compare the effect of the intervention on model importance and explanation importance
\end{itemize}

\section{Outline}
To further motivate this approach we will in the following summarize previous work on XAI, evaluation of XAI, causality and their combination \cref{chapter:background}. We will also lay down the theoretical framework of the XAI method and the causal concepts studied here. \Cref{chapter:method} introduces the causal explanation generation process, and how the benchmark dataset embeds into it. It also describes the methods used to establish a ground-truth \textit{feature importance} as well as metrics for measuring feature importance in our explanation. The architecture and training of the DNN model and other details of the conducted experiment are detailed here too. Finally the different measures are compared and visualized in \cref{chapter:results} and discussed in \cref{chapter:conclusion}.
