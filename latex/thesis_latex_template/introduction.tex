\chapter{Introduction}\label{chapter:introduction}

\section{Motivation and Context}
With the explosion in popularity of deep neural networks, the need to explain such black-box models has risen too. Recently, explainable AI (XAI) methods have been scrutinized more quantitatively. Although there is still no consensus on what exactly makes a \textit{good explanation}, numerous potential metrics for the faithfulness, robustness and interpretability of explanations have been made \citep{Nauta2023}. A promising branch of research seems to be the evaluation of XAI with causal methods \citep{Moraffah2020a}. After all, explanations are, or at least should be, intrinsically causal constructs \citep{Woodward2004, Halpern2005, Schoelkopf2019}.

Local attribution methods explain a decision of a neural network by attributing importance to local features such as pixels in a computer vision task. 
Due to the stronger focus on evaluation of XAI, also local attribution methods have come under criticism, amongst others, for their general lack of sensitivity to the model they are trying to explain \citep{Adebayo2018, Karimi2023}. Also, some methods' class-insensitivity \citep{Sixt2020} and their failure in the ``limit of simplicity'' \citep{Wilming2023} have been examined. Other authors have criticized the lack of comparative user-guided evaluation of these explanation methods \citep{Rong2023}. 
As local attribution methods, especially back-propagation methods, create visually compelling results through attribution maps, they have still become a staple for many AI practitioners, especially for computer vision tasks.  

The recent attribution method Concept Relevance Propagation (CRP) introduced in \cite{Achtibat2022} has been developed for a more fine-grained explanation of a neural network's decisions. Additionally to producing one attribution map explaining the overall prediction output such as Layer-wise Relevance Propagation (LRP) \citep{Bach2015}, each neuron, i.e., ``concept'', in some hidden layer of the network gets assigned a relevance and its own saliency map. In addition to the saliency maps, the relevance scores for each of the concepts also act as a metric to maximize when searching representative samples. According to the authors, through this more detailed and global explanation one can not only understand where a model sees the most relevant features, but also what kind of features they are. Their assumption is that the deeper layers of models represent concepts which are human-understandable and therefore aid in the explanation of what the model predicts. Discovering learned artifacts or concepts in training data which are spuriously correlated to the class to be predicted but have no actual causal relationship is a principal goal of explanation methods. CRP's parent LRP has been shown to uncover these \textit{Clever-Hans} or spurious features in many cases, often with additional analysis similar to CRP \citep{Lapuschkin2019, Anders2022}. It is therefore not surprising that the authors of CRP also particularly assess their method for the task of discovering these artifacts. 

In qualitative examinations as well as a small human subject study, \cite{Achtibat2023} find that Clever-Hans features are easier to identify with their concept-based approach and their relevance better determined in comparison to other local attribution methods. Nevertheless there have been very few quantitative or more formal evaluations of concept-based methods like CRP.
Especially the task of determining the strength of such spurious correlations has not been assessed quantitatively for the method. 
Finding whether a background or spuriously correlated feature is biasing a model is one of the most important applications of XAI, especially in applications where \textit{fairness}, \textit{robustness} and \textit{out-of-distribution prediction} are necessary. Yet the ability to accurately assign \textit{relative} importance to features in conformity with what the model encodes has not been thoroughly analyzed and formalized for concept-based methods in our knowledge. 

Here, we therefore evaluate another desirable characteristic of XAI for the method of CRP: that its sensitivity to spurious feature importance closely follows that of the network it is trying to explain. The general faithfulness or sensitivity to a model has been studied with varying methods, most often creating perturbations in the input or model at the \textit{important} feature, and testing the decline in accuracy. Some recent works have also evaluated an explanation's feature importance when spuriously correlated features are present \citep{Yang2019,Kim2018,Parafita2019,Reimers2020,Singla2022}. Arguably, high sensitivity to the model is especially crucial when identifying and quantifying spurious correlations. Confirming that the target or \textit{core} feature has some importance is not as convincing of an explanation if the importance of other features can not be rigorously analyzed and compared to it \citep{Singla2022}. \cite{Arras2022} question whether a feature can or should have 100 percent of the importance assigned to it within a saliency map, which is even more unclear for spurious features. Such features can either be localized or not and either lie within the area of the target feature or somewhere else. If these features have some significance to the model, they might have some importance attributed, but it is not clear how much importance they can or should have assigned in relation to the target feature. Adding to that, the approach of, among others, \citet{Arras2022} to expect all importance to be within the boundary of a certain object or target feature is an evaluation of coherence with human understanding rather than fidelity to the underlying model, but the two are often conflated as \cite{Nauta2023} describe. 
We test whether evaluating how explanation and model react to a growing correlation in the data distribution between a target and a Clever-Hans feature in a continuous and relative fashion might help to clarify their relationship.

The approach is further justified by the complexly intertwined and biased data we see in real applications of AI. 
In a realistic setting it is nearly impossible for a model to not have learned at least some spurious correlations. A more philosophical question here, is whether we should really expect and desire for a machine learning model to completely ignore all associations that are not based on direct causal relationships, if even humans use non-causal statistical associations in daily visual perception. Especially in image classification datasets it is not even necessarily clearly defined which causal relationships exist between the class and what is seen in the picture. While ignoring certain artifacts such as watermarks seems sensible, some features might have no direct causal relationship with the target feature but ignoring them is not per-se necessary. 
For example, a model should be able to identify a cow even if it is standing in an unexpected environment like a beach but it still is reasonable to be slightly more ``alert'' to finding cows when a green pasture is in the background. We argue that some importance could therefore reasonably be assigned to spurious features as long as the model is still able to predict accurately when they are not presenting as in the context of the data distribution it was trained on. Intervening on how strongly the distribution is biased might enable us to define a sensible threshold for the importance of spurious features in a particular application scenario. 

\section{Strategy}

We will extend previous work on evaluating explanation methods' fidelity in the presence of Clever-Hans features by constructing a causal model. We extend a simple benchmark dataset with known ground truth to generate data using a structural causal model.
While the core or \textit{target} feature actually determines the class, other spurious features are present and correlate with it through a known indirect causal pathway.
Knowing the generating factors helps to quantify the ground-truth feature importance of not only the target and spurious but also other latent features.
With the aim of evaluating fidelity in the presence of a spuriously correlated feature in a relative way, a set of models with varying coupling between the target and the spurious feature are trained. The ground-truth importance of the spurious feature is calculated for each trained model. In expectation, the model's importance for the spurious feature increases and for the target feature declines as the coupling gets stronger.

If CRP indeed produces an accurate explanation, we expect that more concepts should assign higher relevance to the spurious feature the stronger its impact on the actual prediction of the model. It is important to note that the model might accurately predict based on the target feature even though the coupling ratio is high, when there are enough unaffected examples. Non-causally correlated features embody undesirable local minima in the cost function of a network which have to be overcome. Through refined learning procedures and due to the non-linearity of deep neural networks, they are remarkably robust to these artifacts. That means that they only use spurious features if either the stochastic initialization was unfortunate and made it impossible to escape from such a local minimum or if using the spurious feature is actually information-theoretically cheaper than using the target feature. 

If CRP were genuinely faithful to a model, it would correctly follow its learned relationship, but it is also possible that CRP's attribution to the spurious feature will more closely follow the training data distribution or be otherwise disturbed. Our experiment therefore aims to illuminate and possibly answer the following question: 

\begin{quote}
\textit{Is the relative feature importance of a spurious feature explained by CRP in correspondence with the true importance assigned to it by the model, or is it more closely aligned to the associations within the data distribution?}
\end{quote} 

\filbreak
In summary we devise a strategy with the following steps to answer this question:
\begin{enumerate}
    \item We construct a causal model which has a spurious correlation to generate a benchmark dataset.
    \item We construct a causal explanation generating model which embeds this data model into the explanation context.
    \item By intervening on a variable of this explanation generation process which determines the coupling strength in the data distribution, we train a succession of neural network instances.
    \item In relationship to this coupling factor we establish a ground-truth of the models' (spurious) feature importance.
    \item We construct 3 types of metrics, each with further variations, to measure the effect of this coupling of spurious and target feature on the explanations produced by CRP.
    \item Finally we compare the effects of the intervention on model importance and explanation importance.
\end{enumerate}
\filbreak

\section{Outline}
To further motivate this approach we will summarize and put into relation previous work on XAI, evaluation of XAI, and the causality framework \cref{chapter:background}. We will lay down the formal definition of the XAI methods LRP and CRP and of the causal concepts applied here.
\Cref{chapter:method} introduces the causal explanation generation process, and how the benchmark dataset embeds into it. It also describes the methods used to establish a ground-truth \textit{feature importance} of the model in relation to the data distribution. 
The main part of this chapter introduces metrics for measuring feature importance in concept-based explanations produced by CRP in \cref{section:measure}. Finally the different measures are visually and quantitatively analyzed in \cref{chapter:results} and discussed in \cref{chapter:discussion}.

