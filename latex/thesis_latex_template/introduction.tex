\chapter{Introduction}\label{chapter:introduction}

\section{Motivation and Context}
With the explosion in popularity of deep neural networks, the need to explain such black-box models has risen too. Recently, explainable AI (XAI) methods have been scrutinized more quantitatively. Although there is still no consensus on what exactly makes a \textit{good explanation}, numerous potential metrics for the faithfulness, robustness and interpretability of explanations have been made \citep{Nauta2023}. A promising branch of research seems to be the evaluation of XAI with causal methods \citep{Moraffah2020a}. After all, explanations are, or at least should be, intrinsically causal constructs \citep{Woodward2004, Halpern2005, Schoelkopf2019}.

Local attribution methods explain a decision of a neural network by attributing importance to local features such as pixels in a computer vision task. 
Due to the stronger focus on evaluation of XAI, local attribution methods have come under criticism, amongst others, for their general lack of sensitivity to the model they are trying to explain \citep{Adebayo2018, Karimi2023}. Also, some methods' class-insensitivity \citep{Sixt2020} and their failure in presence of suppressor variables and in the ``limit of simplicity'' \citep{Wilming2023} have been examined. Other authors have criticized the lack of comparative user-guided evaluation of these explanation methods \citep{Rong2023}. 
As local attribution methods, especially back-propagation methods, create visually compelling results through attribution maps, they have still become a staple for many AI practitioners, especially for computer vision tasks.  

The recent attribution method Concept-Relevance-Propagation (CRP) introduced in \cite{Achtibat2022} has been developed for a more fine-grained explanation of a neural network's decisions. Additionally to producing one attribution map explaining the overall prediction output such as Layer-wise Relevance Propagation (LRP) \citep{Bach2015}, each neuron, i.e., ``concept'', in some hidden layer of the network gets assigned a relevance and its own saliency map. In addition to the saliency maps, the relevance scores for each of the concepts also act as a metric to maximize when searching representative samples. According to the authors, through this more detailed and global explanation one can not only understand where a model sees the most relevant features, but also what features are relevant in this area. This is why they also call their method a ``glocal'' method. Their assumption is that the deeper layers of models represent concepts which are human-understandable and therefore aid in the explanation of what the model predicts. Discovering learned artifacts or concepts in training data which are spuriously correlated to the class to be predicted but have no actual causal relationship is a principal goal of explanation methods. CRP's parent LRP has been shown to uncover these \textit{Clever-Hans} or spurious features in many cases, often with additional analysis related to CRP \citep{Lapuschkin2019, Anders2022}. It is therefore not surprising that the authors of CRP also especially assess their method for the task of discovering these artifacts. 

In qualitative examinations as well as a small human subject study \citep{Achtibat2023}, they find that Clever-Hans features are easier to identify with the concept-based approach and their relevance better determined in comparison to other local attribution methods. Nevertheless there have been very few quantitative or more formal evaluations of concept-based methods like CRP.
Especially the task of determining spurious correlations has not been assessed quantitatively for these explanation methods. 
Finding whether a background or spuriously correlated feature is biasing a model is one of the most important applications of XAI, especially in applications where \textit{fairness}, \textit{robustness} and \textit{out-of-distribution prediction} are necessary. Yet the ability to accurately assign \textit{relative} importance to features in conformity with what the model encodes has not been thoroughly analyzed and formalized for concept-based methods in our knowledge. 

Here, we therefore evaluate another desirable characteristic of XAI for the method of CRP: that its sensitivity to spurious feature importance closely follows that of the network it is trying to explain. The general faithfulness or sensitivity to a model has been studied with varying methods, most often creating perturbations in the input or model at the \textit{important} feature, and testing the decline in accuracy. Some recent works have also evaluated an explanation's feature importance when spuriously correlated features are present \citep{Yang2019,Kim2018,Parafita2019,Reimers2020,Singla2022}. Arguably, high sensitivity to the model is especially crucial when identifying and quantifying spurious correlations. Confirming that the true or \textit{core} feature has some importance is not as convincing of an explanation if the importance of other features can not be rigorously analyzed and compared to it \citep{Singla2022}. \cite{Arras2022} question whether a feature can or should have 100 percent of the importance assigned to it, which is even more unclear for spurious features. Their approach to expect all importance to be within the boundary of a certain object is an evaluation of coherence with human understanding rather than fidelity to the underlying model, but the two are often conflated as \cite{Nauta2023} describe. 

In the presence of a correlated feature, where an importance of 0 percent for the model might be desirable but is often unattainable, it is even less clear which importance really to expect for the model both for core and spurious features. The question becomes yet harder to answer when the Clever-Hans feature is not spatially separable from the core feature.
We believe that looking at how explanation and model react to a growing correlation between a core and Clever-Hans feature in a continuous and relative fashion might help understand the relationship better. 

This approach is further justified by the correlated and often contaminated data we see in real applications of AI. 
In a realistic setting it is nearly impossible for a model to not have learned at least some spurious correlations. A more philosophical question here, is whether we should really expect a machine learning model to completely ignore them, if even humans are prone to false visual perception like optical illusions. In image classification datasets it is not necessarily clear which causal relationships exist between the class and what is seen in the picture. While ignoring artifacts such as watermarks seems desirable, some features might have no direct causal relationship with the target feature but learning them is not per-se wrong. 
For example, a model should be able to identify a cow even if it is standing in an unexpected environment like a beach but it seems still reasonable to be more ``alert'' to seeing cows when a green pasture is depicted. 

So in accordance with ideas of \textit{causality} it seems infeasible to aim for completely unbiased models, as they would need to have all knowable and unknowable knowledge of the universe to not predict \textit{out-of-distribution}. 
Instead, one perhaps needs to define a measurable threshold of correlation, which is acceptable for a specific type of spuriously correlated feature. 
Embedding the task into the \textit{causality} framework therefore helps to formalize how Clever-Hans or other forms of spurious correlations are expected to be interpreted by a model and its explanation.  

We will extend previous work on evaluating explanation methods' fidelity in the presence of Clever-Hans features by constructing a causal model. We extend a simple benchmark dataset with known ground truth to generate data using a structural causal model.
While the core or \textit{target} feature actually determines the class, other spurious features are still present and correlate with the it through a known indirect causal pathway.
Knowing the generating factors helps to quantify the ground-truth feature importance of not only the core and spurious but also irrelevant features.
With the aim of evaluating fidelity in the presence of a spuriously correlated feature in a relative way, a set of models with varying coupling between the core and the spurious feature are trained. The ground-truth importance of the spurious feature is calculated for each trained model. In expectation, the model's importance for the core feature declines and for the spurious feature increases as the coupling gets stronger.

If CRP indeed produces an accurate explanation, we expect that more concepts should assign higher relevance to the spurious feature the stronger its impact on the actual prediction of the model. It is important to note that the model might accurately predict based on the target feature even though the coupling ratio is high, when there are enough unaffected examples. Non-causally correlated features embody undesirable local minima in the cost function of a network which have to be overcome. Through refined learning procedures and due to the non-linearity of deep neural networks, they are remarkably robust to these artifacts. That means that they only use spurious features if either the random initialization was very unfortunate and made it impossible to escape from such a local minimum or if using the spurious feature is actually information-theoretically cheaper than the true target feature. 

If CRP were genuinely be faithful to a model, it would correctly follow its learned relationship, but it is also possible that CRP's attribution to the spurious feature will more closely follow the training data distribution or be otherwise disturbed. Our experiment therefore aims to illuminate and possibly answer the following question: 

\begin{quote}
\textit{Is the relative feature importance of a spurious feature explained by CRP in correspondence with the true importance assigned to it by the model, or is it more closely aligned to the associations within the data distribution?}
\end{quote} 

\filbreak
In summary we devise a strategy with the following steps to answer this question:
\begin{enumerate}
    \item We construct a causal model which has a spurious correlation using a benchmark dataset.
    \item We construct a causal explanation generating model which embeds this data model into the explanation context.
    \item By intervening on a variable of this explanation generation process which determines the coupling strength in the data distribution, we train a succession of neural network instances.
    \item In relationship to this coupling factor we establish a ground-truth of the models (spurious) feature importance.
    \item We construct metrics to measure the effect of this coupling of spurious and core feature on the concept-based approach of concept relevance propagation (CRP).
    \item Finally we compare the effects of the intervention on model importance and explanation importance
\end{enumerate}
\filbreak

\section{Outline}
To further motivate this approach we will summarize and put into relation previous work on XAI, evaluation of XAI, and the causality framework \cref{chapter:background}. We will lay down the formal definition of the XAI methods LRP and CRP and of the causal concepts applied here.
\Cref{chapter:method} introduces the causal explanation generation process, and how the benchmark dataset embeds into it. It also describes the methods used to establish a ground-truth \textit{feature importance} of the model in relation to the data distribution. 
The main part introduces metrics for measuring feature importance in concept-based explanations produced by CRP in \cref{section:measure}. The overall construction of the experiment and the set of trained models is summarized in \cref{section:experiment_setup}. Finally the different measures are visually and quantitatively analyzed in \cref{chapter:results} and discussed in \cref{chapter:discussion}.
