% Encoding: UTF-8

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title     = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2016},
  series    = {KDD '16},
  pages     = {1135–1144},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  abstract  = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  comment   = {Is paper introducing LIME},
  doi       = {10.1145/2939672.2939778},
  isbn      = {9781450342322},
  keywords  = {interpretable machine learning, interpretability, explaining machine learning, black box classifier, rank3},
  location  = {San Francisco, California, USA},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2939672.2939778},
}

@InBook{Hooker2019,
  title         = {A benchmark for interpretability methods in deep neural networks},
  publisher     = {Curran Associates Inc.},
  year          = {2019},
  author        = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  address       = {Red Hook, NY, USA},
  __markedentry = {[lilli:3]},
  abstract      = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  articleno     = {873},
  booktitle     = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  keywords      = {rank5},
  numpages      = {12},
  url           = {https://ar5iv.labs.arxiv.org/html/1806.10758},
}

@InProceedings{Alvarez-Melis2017,
  author        = {Alvarez-Melis, David and Jaakkola, Tommi},
  title         = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  booktitle     = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year          = {2017},
  pages         = {412--421},
  address       = {Copenhagen, Denmark},
  month         = sep,
  publisher     = {Association for Computational Linguistics},
  __markedentry = {[lilli:1]},
  abstract      = {We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an {``}explanation{''} consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.},
  doi           = {10.18653/v1/D17-1042},
  file          = {:Alvarez-Melis2017 - A causal framework for explaining the predictions of black-box sequence-to-sequence models.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
  primaryclass  = {cs.LG},
  url           = {https://aclanthology.org/D17-1042},
}

@InProceedings{Rong2022,
  author        = {Rong, Yao and Leemann, Tobias and Borisov, Vadim and Kasneci, Gjergji and Kasneci, Enkelejda},
  title         = {A Consistent and Efficient Evaluation Strategy for Attribution Methods},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  year          = {2022},
  editor        = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume        = {162},
  series        = {Proceedings of Machine Learning Research},
  pages         = {18770--18795},
  month         = {17--23 Jul},
  publisher     = {PMLR},
  __markedentry = {[lilli:1]},
  abstract      = {With a variety of local feature attribution methods being proposed in recent years, follow-up work suggested several evaluation strategies. To assess the attribution quality across different attribution techniques, the most popular among these evaluation strategies in the image domain use pixel perturbations. However, recent advances discovered that different evaluation strategies produce conflicting rankings of attribution methods and can be prohibitively expensive to compute. In this work, we present an information-theoretic analysis of evaluation strategies based on pixel perturbations. Our findings reveal that the results are strongly affected by information leakage through the shape of the removed pixels as opposed to their actual values. Using our theoretical insights, we propose a novel evaluation framework termed Remove and Debias (ROAD) which offers two contributions: First, it mitigates the impact of the confounders, which entails higher consistency among evaluation strategies. Second, ROAD does not require the computationally expensive retraining step and saves up to 99% in computational costs compared to the state-of-the-art. We release our source code at https://github.com/tleemann/road_evaluation.},
  file          = {:Rong2022 - A Consistent and Efficient Evaluation Strategy for Attribution Methods.pdf:PDF;rong22a.pdf:https\://proceedings.mlr.press/v162/rong22a/rong22a.pdf:PDF},
  keywords      = {rank5},
  url           = {https://proceedings.mlr.press/v162/rong22a.html},
}

@InProceedings{Atanasova2020,
  author    = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  title     = {A Diagnostic Study of Explainability Techniques for Text Classification},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2020},
  editor    = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  pages     = {3256--3274},
  address   = {Online},
  month     = nov,
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models{'} predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model{'}s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.},
  doi       = {10.18653/v1/2020.emnlp-main.263},
  url       = {https://aclanthology.org/2020.emnlp-main.263},
}

@Misc{Fel2023a,
  author        = {Thomas Fel and Victor Boutin and Mazda Moayeri and Rémi Cadène and Louis Bethune and Léo andéol and Mathieu Chalvidal and Thomas Serre},
  title         = {A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation},
  year          = {2023},
  note          = {https://serre-lab.github.io/Lens/classes/hare/},
  __markedentry = {[lilli:1]},
  abstract      = {see website https://serre-lab.github.io/Lens/classes/hare/},
  archiveprefix = {arXiv},
  eprint        = {2306.07304},
  file          = {:Fel2023a - A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation.pdf:PDF},
  keywords      = {rank4},
  primaryclass  = {cs.LG},
}

@Article{Klemm2012,
  author    = {Konstantin Klemm and M. {\'{A}}ngeles Serrano and V{\'{\i}}ctor M. Egu{\'{\i}}luz and Maxi San Miguel},
  title     = {A measure of individual role in collective dynamics},
  journal   = {Scientific Reports},
  year      = {2012},
  volume    = {2},
  number    = {1},
  month     = {feb},
  doi       = {10.1038/srep00292},
  groups    = {Application Fields, socialclimate},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Sauter2022,
  author      = {Andreas Sauter and Erman Acar and Vincent François-Lavet},
  title       = {A Meta-Reinforcement Learning Algorithm for Causal Discovery},
  abstract    = {Causal discovery is a major task with the utmost importance for machine learning since causal structures can enable models to go beyond pure correlation-based inference and significantly boost their performance. However, finding causal structures from data poses a significant challenge both in computational effort and accuracy, let alone its impossibility without interventions in general. In this paper, we develop a meta-reinforcement learning algorithm that performs causal discovery by learning to perform interventions such that it can construct an explicit causal graph. Apart from being useful for possible downstream applications, the estimated causal graph also provides an explanation for the data-generating process. In this article, we show that our algorithm estimates a good graph compared to the SOTA approaches, even in environments whose underlying causal structure is previously unseen. Further, we make an ablation study that shows how learning interventions contribute to the overall performance of our approach. We conclude that interventions indeed help boost the performance, efficiently yielding an accurate estimate of the causal structure of a possibly unseen environment.},
  date        = {2022-07-18},
  eprint      = {2207.08457v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Sauter2022 - A Meta-Reinforcement Learning Algorithm for Causal Discovery.pdf:PDF},
  keywords    = {cs.LG, cs.AI, stat.ME, rank1},
}

@Article{Ramsey2016,
  author    = {Joseph Ramsey and Madelyn Glymour and Ruben Sanchez-Romero and Clark Glymour},
  title     = {A million variables and more: the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images},
  journal   = {International Journal of Data Science and Analytics},
  year      = {2016},
  volume    = {3},
  number    = {2},
  pages     = {121--129},
  month     = {dec},
  doi       = {10.1007/s41060-016-0032-z},
  file      = {:Ramsey2016 - A million variables and more_ the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images.pdf:PDF},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{Sixt2022,
  author        = {Leon Sixt and Tim Landgraf},
  title         = {A Rigorous Study Of The Deep Taylor Decomposition},
  year          = {2022},
  __markedentry = {[lilli:2]},
  archiveprefix = {arXiv},
  eprint        = {2211.08425},
  file          = {:Sixt2022 - A Rigorous Study Of The Deep Taylor Decomposition.pdf:PDF},
  groups        = {relevant},
  journal       = {Transactions on Machine Learning},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
}

@Misc{Santoro2017,
  author        = {Adam Santoro and David Raposo and David G. T. Barrett and Mateusz Malinowski and Razvan Pascanu and Peter Battaglia and Timothy Lillicrap},
  title         = {A simple neural network module for relational reasoning},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1706.01427},
  file          = {:Santoro2017 - A simple neural network module for relational reasoning.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank2},
  primaryclass  = {cs.CL},
}

@Article{Tibau2022,
  author    = {Xavier-Andoni Tibau and Christian Reimers and Andreas Gerhardus and Joachim Denzler and Veronika Eyring and Jakob Runge},
  title     = {A spatiotemporal stochastic climate model for benchmarking causal discovery methods for teleconnections},
  journal   = {Environmental Data Science},
  year      = {2022},
  volume    = {1},
  doi       = {10.1017/eds.2022.11},
  file      = {:Tibau2022 - A spatiotemporal stochastic climate model for benchmarking causal discovery methods for teleconnections.pdf:PDF},
  groups    = {Recommended},
  keywords  = {rank1},
  publisher = {Cambridge University Press ({CUP})},
}

@Article{Shih2018,
  author     = {Andy Shih and Arthur Choi and Adnan Darwiche},
  title      = {A Symbolic Approach to Explaining Bayesian Network Classifiers},
  journal    = {CoRR},
  year       = {2018},
  volume     = {abs/1805.03364},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1805-03364.bib},
  eprint     = {1805.03364},
  eprinttype = {arXiv},
  file       = {:Shih2018 - A Symbolic Approach to Explaining Bayesian Network Classifiers.pdf:PDF},
  keywords   = {rank1},
  timestamp  = {Mon, 13 Aug 2018 16:47:13 +0200},
  url        = {http://arxiv.org/abs/1805.03364},
}

@Misc{Nie2020,
  author        = {Weili Nie and Yang Zhang and Ankit Patel},
  title         = {A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1805.07039},
  primaryclass  = {cs.CV},
}

@InBook{Schoelkopf2008,
  chapter   = {1},
  title     = {A Tutorial Introduction},
  year      = {2008},
  author    = {Bernhard Schölkopf, Alexander J. Smola},
  abstract  = {This chapter describes the central ideas of Support and Vector (SV) and learning in a  and nutshell. Its and goal is to provide an overview of the basic concepts.},
  booktitle = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  file      = {:Schoelkopf2008 - A Tutorial Introduction.pdf:PDF},
  keywords  = {rank1},
}

@InProceedings{Lundberg2017,
  author        = {Lundberg, Scott M and Lee, Su-In},
  title         = {A Unified Approach to Interpreting Model Predictions},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2017},
  editor        = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  volume        = {30},
  publisher     = {Curran Associates, Inc.},
  __markedentry = {[lilli:2]},
  file          = {:Lundberg2017 - A Unified Approach to Interpreting Model Predictions.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
}

@Article{Shapley1953,
  author   = {Lloyd S. Shapley},
  title    = {A Value for N-Person Games},
  journal  = {Contributions to the Theory of Games},
  year     = {1953},
  volume   = {2(28)},
  pages    = {307–317},
  file     = {:Shapley1953 - A Value for N-Person Games.pdf:PDF},
  groups   = {relevant},
  keywords = {rank3},
}

@Article{Carter2019,
  author  = {Shan Carter and Zan Armstrong and Ludwig Schubert and Ian Johnson and Christopher Olah},
  title   = {Activation Atlas},
  journal = {Distill},
  year    = {2019},
  url     = {https://api.semanticscholar.org/CorpusID:242816628},
}

@InProceedings{Amirinezhad2020,
  author   = {Amir Amirinezhad and Saber Salehkaleybar and Matin Hashemi},
  title    = {Active Learning of Causal Structures with Deep Reinforcement Learning},
  year     = {2020},
  abstract = {We study the problem of experiment design to learn
causal structures from interventional data. We consider an active
learning setting in which the experimenter decides to intervene
on one of the variables in the system in each step and uses the
results of the intervention to recover further causal relationships
among the variables. The goal is to fully identify the causal
structures with minimum number of interventions. We present
the first deep reinforcement learning based solution for the
problem of experiment design. In the proposed method, we embed
input graphs to vectors using a graph neural network and feed
them to another neural network which outputs a variable for
performing intervention in each step. Both networks are trained
jointly via a Q-iteration algorithm. Experimental results show
that the proposed method achieves competitive performance in
recovering causal structures with respect to previous works, while
significantly reducing execution time in dense graphs},
  doi      = {ng},
  file     = {:Amirinezhad2020 - Active Learning of Causal Structures with Deep Reinforcement Learning.pdf:PDF},
  groups   = {Recommended, relevant},
  keywords = {rank2},
}

@InProceedings{Ribeiro2018,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title     = {Anchors: high-precision model-agnostic explanations},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  year      = {2018},
  series    = {AAAI'18/IAAI'18/EAAI'18},
  publisher = {AAAI Press},
  abstract  = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  articleno = {187},
  isbn      = {978-1-57735-800-8},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {9},
}

@Article{Mueller2021,
  author    = {Paul Manuel Müller and Jobst Heitzig and Jürgen Kurths and Kathy Lüdge and Marc Wiedermann},
  title     = {Anticipation-induced social tipping: can the environment be stabilised by social dynamics?},
  journal   = {The European Physical Journal Special Topics},
  year      = {2021},
  volume    = {230},
  number    = {16-17},
  pages     = {3189--3199},
  month     = {apr},
  doi       = {10.1140/epjs/s11734-021-00011-5},
  file      = {:Mueller2021 - Anticipation-induced social tipping_ can the environment be stabilised by social dynamics_.pdf:PDF},
  groups    = {Application Fields, socialclimate},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Vaswani2017,
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  title         = {Attention is all you need},
  journal       = {Advances in neural information processing systems},
  year          = {2017},
  volume        = {30},
  __markedentry = {[lilli:1]},
  file          = {:Vaswani2017 - Attention is all you need.pdf:PDF},
  keywords      = {rank1},
}

@Misc{Jain2019,
  author        = {Jain, Sarthak and Wallace, Byron C.},
  title         = {Attention is not Explanation},
  year          = {2019},
  __markedentry = {[lilli:1]},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1902.10186},
  file          = {:Jain2019 - Attention is not Explanation.pdf:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, rank1},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1902.10186},
}

@Misc{Wiegreffe2019,
  author        = {Sarah Wiegreffe and Yuval Pinter},
  title         = {Attention is not not Explanation},
  year          = {2019},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {1908.04626},
  file          = {:Wiegreffe2019 - Attention is not not Explanation.pdf:PDF},
  keywords      = {rank1},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1908.04626},
}

@InProceedings{Sundararajan2017,
  author        = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  title         = {Axiomatic Attribution for Deep Networks},
  booktitle     = {Proceedings of the 34th International Conference on Machine Learning},
  year          = {2017},
  editor        = {Precup, Doina and Teh, Yee Whye},
  volume        = {70},
  series        = {Proceedings of Machine Learning Research},
  pages         = {3319--3328},
  month         = {06--11 Aug},
  publisher     = {PMLR},
  __markedentry = {[lilli:1]},
  abstract      = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  file          = {:Sundararajan2017 - Axiomatic Attribution for Deep Networks.pdf:PDF;sundararajan17a.pdf:http\://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
  url           = {https://proceedings.mlr.press/v70/sundararajan17a.html},
}

@Article{Haegele2022,
  author      = {Alexander Hägele and Jonas Rothfuss and Lars Lorch and Vignesh Ram Somnath and Bernhard Schölkopf and Andreas Krause},
  title       = {BaCaDI: Bayesian Causal Discovery with Unknown Interventions},
  abstract    = {Inferring causal structures from experimentation is a central task in many domains. For example, in biology, recent advances allow us to obtain single-cell expression data under multiple interventions such as drugs or gene knockouts. However, the targets of the interventions are often uncertain or unknown and the number of observations limited. As a result, standard causal discovery methods can no longer be reliably used. To fill this gap, we propose a Bayesian framework (BaCaDI) for discovering and reasoning about the causal structure that underlies data generated under various unknown experimental or interventional conditions. BaCaDI is fully differentiable, which allows us to infer the complex joint posterior over the intervention targets and the causal structure via efficient gradient-based variational inference. In experiments on synthetic causal discovery tasks and simulated gene-expression data, BaCaDI outperforms related methods in identifying causal structures and intervention targets.},
  date        = {2022-06-03},
  eprint      = {2206.01665v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Haegele2022 - BaCaDI_ Bayesian Causal Discovery with Unknown Interventions.pdf:PDF},
  keywords    = {cs.LG, stat.ME, stat.ML, rank1},
}

@InProceedings{Kuegelgen2023,
  author        = {Julius von Kügelgen and Abdirisak Mohamed and Sander Beckers},
  title         = {Backtracking Counterfactuals},
  booktitle     = {Proceedings of Machine Learning Research vol TBD:1–22, 2023 2nd Conference on Causal Learning and Reasoning},
  year          = {2023},
  __markedentry = {[lilli:1]},
  file          = {:Kuegelgen2023 - Backtracking Counterfactuals.pdf:PDF},
  groups        = {Recommended, relevant},
  keywords      = {Causal reasoning, backtracking, counterfactual explanations, explainable AI, XAI, rank5},
}

@InProceedings{Bareinboim2015a,
  author    = {Bareinboim, Elias and Forney, Andrew and Pearl, Judea},
  title     = {Bandits with Unobserved Confounders: A Causal Approach},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  volume    = {28},
  publisher = {Curran Associates, Inc.},
  file      = {:Bareinboim2015a - Bandits with Unobserved Confounders_ A Causal Approach.pdf:PDF},
  keywords  = {rank1},
}

@Article{Nishikawa-Toomey2022,
  author   = {Nishikawa-Toomey, Mizu and Deleu, Tristan and Subramanian, Jithendaraa and Bengio, Yoshua and Charlin, Laurent},
  title    = {Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes},
  journal  = {arXiv preprint arXiv:2211.02763},
  year     = {2022},
  file     = {:Nishikawa-Toomey2022 - Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes.pdf:PDF},
  groups   = {Recommended},
  keywords = {rank1},
  url      = {https://arxiv.org/abs/2211.02763},
}

@Misc{Yang2019,
  author        = {Mengjiao Yang and Been Kim},
  title         = {Benchmarking Attribution Methods with Relative Feature Importance},
  year          = {2019},
  __markedentry = {[lilli:4]},
  archiveprefix = {arXiv},
  eprint        = {1907.09701},
  file          = {:Yang2019 - Benchmarking Attribution Methods with Relative Feature Importance.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1907.09701},
}

@InProceedings{Higgins2017,
  author    = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
  booktitle = {International Conference on Learning Representations},
  year      = {2017},
  file      = {:Higgins2017 - beta-VAE_ Learning Basic Visual Concepts with a Constrained Variational Framework.pdf:PDF},
  keywords  = {rank4},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
}

@Misc{Gerstenberger2022,
  author        = {Michael Gerstenberger and Sebastian Lapuschkin and Peter Eisert and Sebastian Bosse},
  title         = {But that ' s not why: Inference adjustment by interactive prototype deselection},
  year          = {2022},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2203.10087},
  file          = {:Gerstenberger2022 - But that ' s not why_ Inference adjustment by interactive prototype deselection.pdf:PDF},
  groups        = {relevant},
  journal       = {ArXiv},
  keywords      = {rank4},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2203.10087v1},
}

@Article{Kyono2020,
  author        = {Trent Kyono and Yao Zhang and Mihaela van der Schaar},
  title         = {{CASTLE:} Regularization via Auxiliary Causal Graph Discovery},
  journal       = {ArXiv},
  year          = {2020},
  volume        = {abs/2009.13180},
  __markedentry = {[lilli:1]},
  abstract      = {@inproceedings{NIPS2013_47d1e990,
 author = {Peters, Jonas and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Causal Inference on Time Series using Restricted Structural Equation Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Paper.pdf},
 volume = {26},
 year = {2013}
}},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2009-13180.bib},
  eprint        = {2009.13180},
  eprinttype    = {arXiv},
  file          = {:Kyono2020 - CASTLE_ Regularization via Auxiliary Causal Graph Discovery.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank2},
  timestamp     = {Wed, 30 Sep 2020 16:16:22 +0200},
  url           = {https://arxiv.org/abs/2009.13180},
}

@InProceedings{Moraffah2020,
  author        = {Raha Moraffah and Bahman Moraffah and Mansooreh Karami and Adrienne Raglin and Huan Liu},
  title         = {Causal Adversarial Network for Learning Conditional and Interventional Distributions},
  year          = {2020},
  __markedentry = {[lilli:1]},
  file          = {:Moraffah2020 - Causal Adversarial Network for Learning Conditional and Interventional Distributions.pdf:PDF},
  groups        = {relevant},
  journal       = {ArXiv},
  keywords      = {rank3},
}

@Article{Lattimore2016,
  author      = {Finnian Lattimore and Tor Lattimore and Mark D. Reid},
  title       = {Causal Bandits: Learning Good Interventions via Causal Inference},
  abstract    = {We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.},
  date        = {2016-06-10},
  eprint      = {1606.03203v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Lattimore2016 - Causal Bandits_ Learning Good Interventions via Causal Inference.pdf:PDF},
  groups      = {Application Fields, relevant},
  keywords    = {stat.ML, cs.LG, rank2},
}

@InProceedings{Eberhardt2008,
  author    = {Frederick Eberhardt and EBERHARDT@WUSTL.EDU and Department of Philosophy and Washington University and in St. Louis and St. Louis and MO and USA},
  title     = {Causal Discovery as a Game},
  booktitle = {JMLR Workshop and Conference Proceedings 6:87–96 NIPS 2008 workshop on causality},
  year      = {2008},
  abstract  = {This paper presents a game theoretic approach to causal discovery. The problem of causal
discovery is framed as a game of the Scientist against Nature, in which Nature attempts to
hide its secrets for as long as possible, and the Scientist makes her best effort at discovery
while minimizing cost. This approach provides a very general framework for the assessment
of different search procedures and a principled way of modeling the effect of choices between
different experiments.},
  file      = {:Eberhardt2008 - Causal Discovery as a Game.pdf:PDF},
  groups    = {Application Fields, Recommended, relevant},
  keywords  = {causal discovery, interventions, search strategy, game theory, worst and expected case analysis, rank2},
}

@InProceedings{Perry2022,
  author        = {Ronan Perry and Julius Von K{\"u}gelgen and Bernhard Sch{\"o}lkopf},
  title         = {Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2022},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  __markedentry = {[lilli:1]},
  keywords      = {rank1},
  url           = {https://openreview.net/forum?id=kFRCvpubDJo},
}

@InProceedings{Dhir2022,
  author    = {Anish Dhir and Mark van der Wilk},
  title     = {Causal Discovery using Marginal Likelihood},
  booktitle = {NeurIPS 2022 Workshop on Causality for Real-world Impact},
  year      = {2022},
  file      = {:Dhir2022 - Causal Discovery using Marginal Likelihood.pdf:PDF},
  groups    = {Recommended},
  keywords  = {rank1},
  url       = {https://openreview.net/forum?id=k0DJZXMSgH4},
}

@InProceedings{Beckers2022,
  author        = {Beckers, Sander},
  title         = {Causal Explanations and {XAI}},
  booktitle     = {Proceedings of the First Conference on Causal Learning and Reasoning},
  year          = {2022},
  editor        = {Schölkopf, Bernhard and Uhler, Caroline and Zhang, Kun},
  volume        = {177},
  series        = {Proceedings of Machine Learning Research},
  pages         = {90--109},
  month         = {11--13 Apr},
  publisher     = {PMLR},
  __markedentry = {[lilli:2]},
  abstract      = {Although standard Machine Learning models are optimized for making predictions about observations, more and more they are used for making predictions about the results of actions. An important goal of Explainable Artificial Intelligence (XAI) is to compensate for this mismatch by offering explanations about the predictions of an ML-model which ensure that they are reliably action-guiding. As action-guiding explanations are causal explanations, the literature on this topic is starting to embrace insights from the literature on causal models. Here I take a step further down this path by formally defining the causal notions of sufficient explanations and counterfactual explanations. I show how these notions relate to (and improve upon) existing work, and motivate their adequacy by illustrating how different explanations are action-guiding under different circumstances. Moreover, this work is the first to offer a formal definition of actual causation that is founded entirely in action-guiding explanations. Although the definitions are motivated by a focus on XAI, the analysis of causal explanation and actual causation applies in general. I also touch upon the significance of this work for fairness in AI by showing how actual causation can be used to improve the idea of path-specific counterfactual fairness. },
  file          = {:Beckers2022 - Causal Explanations and XAI.pdf:PDF;beckers22a.pdf:https\://proceedings.mlr.press/v177/beckers22a/beckers22a.pdf:PDF},
  groups        = {Recommended, relevant},
  keywords      = {rank5},
  url           = {https://proceedings.mlr.press/v177/beckers22a.html},
}

@Article{Chalupka2016,
  author    = {Krzysztof Chalupka and Frederick Eberhardt and Pietro Perona},
  title     = {Causal feature learning: an overview},
  journal   = {Behaviormetrika},
  year      = {2016},
  volume    = {44},
  number    = {1},
  pages     = {137--164},
  month     = {dec},
  doi       = {10.1007/s41237-016-0008-2},
  file      = {:Chalupka2016 - Causal feature learning_ an overview.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank2},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Gonzalez-Soto2019,
  author   = {Mauricio Gonzalez-Soto and Luis E. Sucar and Hugo Jair Escalante},
  title    = {Causal Games and Causal Nash Equilibrium},
  year     = {2019},
  abstract = {Classical results of Decision Theory, and its extension to a
multi-agent setting: Game Theory, operate only at the associative level
of information; this is, classical decision makers only take into account
probabilities of events; we go one step further and consider causal infor-
mation: in this work, we define Causal Decision Problems and extend
them to a multi-agent decision problem, which we call a causal game.
For such games, we study belief updating in a class of strategic games in
which any player’s action causes some consequence via a causal model,
which is unknown by all players; for this reason, the most suitable model
is Harsanyi’s Bayesian Game. We propose a probability updating for the
Bayesian Game in such a way that the knowledge of any player in terms
of probabilistic beliefs about the causal model, as well as what is caused
by her actions as well as the actions of every other player are taken into
account. Based on such probability updating we define a Nash equilibria
for Causal Games.},
  file     = {:Gonzalez-Soto2019 - Causal Games and Causal Nash Equilibrium.pdf:PDF},
  groups   = {Application Fields, socialclimate, Recommended, relevant},
  keywords = {rank2},
}

@Misc{Goudet2018,
  author        = {Olivier Goudet and Diviyan Kalainathan and Philippe Caillou and Isabelle Guyon and David Lopez-Paz and Michèle Sebag},
  title         = {Causal Generative Neural Networks},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1711.08936},
  keywords      = {rank1},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1711.08936},
}

@InProceedings{Meek1995,
  author    = {Meek, Christopher},
  title     = {Causal Inference and Causal Explanation with Background Knowledge},
  booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
  year      = {1995},
  series    = {UAI'95},
  pages     = {403–410},
  address   = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  abstract  = {This paper presents correct algorithms for answering the following two questions; (i) Does there exist a causal explanation consistent with a set of background knowledge which explains all of the observed independence facts in a sample? (ii) Given that there is such a causal explanation what are the causal relationships common to every such causal explanation?},
  file      = {:Meek1995 - Causal Inference and Causal Explanation with Background Knowledge.pdf:PDF},
  isbn      = {1558603859},
  keywords  = {rank1},
  location  = {Montr\'{e}al, Qu\'{e}, Canada},
  numpages  = {8},
}

@Article{Peters2015,
  author   = {J. Peters and Peter Buhlmann and Nicolai Meinshausen},
  title    = {Causal inference by using invariant prediction: identification and confidence intervals},
  journal  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year     = {2015},
  volume   = {78},
  keywords = {rank1},
}

@InProceedings{Peters2013,
  author    = {Peters, Jonas and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
  title     = {Causal Inference on Time Series using Restricted Structural Equation Models},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2013},
  editor    = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  volume    = {26},
  publisher = {Curran Associates, Inc.},
  abstract  = {Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide more general identifiability results than existing ones. The results cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. When the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers. The Structural Equation Model point of view allows us to extend both the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays (as may happen for fMRI data, for example). TiMINo outperforms existing methods on artificial and real data. Code is provided.},
  keywords  = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Paper.pdf},
}

@Article{Rubin2005,
  author        = {Donald B. Rubin},
  title         = {Causal Inference Using Potential Outcomes},
  journal       = {Journal of the American Statistical Association},
  year          = {2005},
  volume        = {100},
  pages         = {322 - 331},
  __markedentry = {[lilli:1]},
  url           = {https://api.semanticscholar.org/CorpusID:842793},
}

@Article{Janzing2010,
  author        = {Janzing, D. and Sch{\"o}lkopf, B.},
  title         = {Causal Inference Using the Algorithmic Markov Condition},
  journal       = {IEEE Transactions on Information Theory},
  year          = {2010},
  volume        = {56},
  number        = {10},
  pages         = {5168-5194},
  month         = oct,
  doi           = {10.1109/TIT.2010.2060095},
  keywords      = {rank1},
  month_numeric = {10},
  organization  = {Max-Planck-Gesellschaft},
  school        = {Biologische Kybernetik},
}

@Article{Moraffah2020a,
  author        = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
  title         = {Causal Interpretability for Machine Learning - Problems, Methods and Evaluation},
  journal       = {SIGKDD Explor. Newsl.},
  year          = {2020},
  volume        = {22},
  number        = {1},
  pages         = {18–33},
  month         = {may},
  issn          = {1931-0145},
  __markedentry = {[lilli:3]},
  abstract      = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
  address       = {New York, NY, USA},
  doi           = {10.1145/3400051.3400058},
  file          = {:Moraffah2020a - Causal Interpretability for Machine Learning - Problems, Methods and Evaluation.pdf:PDF},
  groups        = {relevant},
  issue_date    = {June 2020},
  keywords      = {machine learning, counterfactuals, interpratablity, causal inference, explainability, rank4},
  numpages      = {16},
  publisher     = {Association for Computing Machinery},
}

@Article{Harradon2018,
  author   = {Michael Harradon and Jeff Druce and Brian E. Ruttenberg},
  title    = {Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations},
  journal  = {ArXiv},
  year     = {2018},
  volume   = {abs/1802.00541},
  groups   = {relevant},
  keywords = {rank4},
  url      = {https://www.semanticscholar.org/reader/267bf71c7ed5e2ad895b9815dfff73658aa9f93e},
}

@Article{Sun2015,
  author    = {Jie Sun and Dane Taylor and Erik M. Bollt},
  title     = {Causal Network Inference by Optimal Causation Entropy},
  journal   = {{SIAM} Journal on Applied Dynamical Systems},
  year      = {2015},
  volume    = {14},
  number    = {1},
  pages     = {73--106},
  month     = {jan},
  doi       = {10.1137/140956166},
  keywords  = {rank1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  url       = {https://doi.org/10.1137%2F140956166},
}

@Article{Runge2018,
  author   = {Runge,J.},
  title    = {Causal network reconstruction from time series: From theoretical assumptions to practical estimation},
  journal  = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  year     = {2018},
  volume   = {28},
  number   = {7},
  pages    = {075310},
  doi      = {10.1063/1.5025050},
  eprint   = {https://doi.org/10.1063/1.5025050},
  file     = {:Runge2018 - Causal network reconstruction from time series_ From theoretical assumptions to practical estimation.pdf:PDF},
  groups   = {relevant},
  keywords = {rank2},
  url      = {https://doi.org/10.1063/1.5025050},
}

@InProceedings{Heskes2020,
  author        = {Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and Claassen, Tom},
  title         = {Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2020},
  editor        = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  volume        = {33},
  pages         = {4778--4789},
  publisher     = {Curran Associates, Inc.},
  __markedentry = {[lilli:1]},
  file          = {:Heskes2020 - Causal Shapley Values_ Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.pdf:PDF},
  groups        = {Recommended, relevant},
  keywords      = {rank5},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf},
}

@InBook{Schoelkopf2019,
  pages         = {765–804},
  title         = {Causality for Machine Learning},
  publisher     = {Association for Computing Machinery},
  year          = {2022},
  author        = {Sch\"{o}lkopf, Bernhard},
  address       = {New York, NY, USA},
  edition       = {1},
  isbn          = {9781450395861},
  __markedentry = {[lilli:3]},
  booktitle     = {Probabilistic and Causal Inference: The Works of Judea Pearl},
  file          = {:Schoelkopf2019 - Causality for Machine Learning.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  numpages      = {40},
  url           = {https://doi.org/10.1145/3501714.3501755},
}

@Article{Castro2020,
  author    = {Daniel C. Castro and Ian Walker and Ben Glocker},
  title     = {Causality matters in medical imaging},
  journal   = {Nature Communications},
  year      = {2020},
  volume    = {11},
  number    = {1},
  month     = jul,
  doi       = {10.1038/s41467-020-17478-w},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1038/s41467-020-17478-w},
}

@Book{Pearl2009,
  title         = {Causality: Models, reasoning, and inference.},
  publisher     = {Cambridge University Press},
  year          = {2009},
  author        = {Pearl, Judea},
  volume        = {29},
  month         = {01},
  isbn          = {9780521895606},
  __markedentry = {[lilli:1]},
  doi           = {10.1017/CBO9780511803161},
  file          = {:Pearl2000 - Causality.pdf:PDF},
  groups        = {Recommended, socialclimate, Application Fields, relevant},
  journal       = {Causality},
  keywords      = {rank3},
}

@InProceedings{Yang2021,
  author    = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  title     = {CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2021},
  pages     = {9588-9597},
  doi       = {10.1109/CVPR46437.2021.00947},
  file      = {:Yang2021 - CausalVAE_ Disentangled Representation Learning via Neural Structural Causal Models.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank3},
}

@Misc{Ahmed2020,
  author        = {Ossama Ahmed and Frederik Träuble and Anirudh Goyal and Alexander Neitz and Yoshua Bengio and Bernhard Schölkopf and Manuel Wüthrich and Stefan Bauer},
  title         = {CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2010.04296},
  keywords      = {rank1},
  primaryclass  = {cs.RO},
  url           = {https://arxiv.org/abs/2010.04296},
}

@Book{Spirtes1993,
  title     = {Causation, Prediction, and Search},
  publisher = {Springer New York},
  year      = {1993},
  author    = {Peter Spirtes and Clark Glymour and Richard Scheines},
  doi       = {10.1007/978-1-4612-2748-9},
  file      = {:Spirtes1993 - Causation, Prediction, and Search.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank2},
}

@Article{Halpern2005,
  author        = {Halpern, Joseph Y. and Pearl, Judea},
  title         = {Causes and Explanations: A Structural-Model Approach. Part I: Causes},
  journal       = {The British Journal for the Philosophy of Science},
  year          = {2005},
  volume        = {56},
  number        = {4},
  pages         = {843-887},
  __markedentry = {[lilli:1]},
  abstract      = {We propose a new definition of actual causes, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account. 1. Introduction2. Causal models: a review2.1Causal models2.2Syntax and semantics3. The definition of cause4. Examples5. A more refined definition6. DiscussionAAppendix: Some Technical IssuesA.1The active causal processA.2A closer look at AC2(b)A.3Causality with infinitely many variablesA.4Causality in nonrecursive models},
  doi           = {10.1093/bjps/axi147},
  eprint        = {https://doi.org/10.1093/bjps/axi147},
  file          = {:Halpern2005 - Causes and Explanations_ A Structural-Model Approach. Part I_ Causes.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  url           = {https://doi.org/10.1093/bjps/axi147},
}

@Article{Halpern2005a,
  author        = {Joseph Y. Halpern and Judea Pearl},
  title         = {Causes and Explanations: {A} Structural-Model Approach. Part {II:} Explanations},
  journal       = {ArXiv},
  year          = {2005},
  volume        = {cs.AI/0208034},
  __markedentry = {[lilli:1]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/cs-AI-0208034.bib},
  file          = {:Halpern2002 - Causes and Explanations_ A Structural-Model Approach. Part II_ Explanations.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  timestamp     = {Fri, 10 Jan 2020 12:59:27 +0100},
  url           = {https://arxiv.org/abs/cs/0208034},
}

@Article{Rieckmann2022,
  author        = {Rieckmann, Andreas and Dworzynski, Piotr and Arras, Leila and Lapuschkin, Sebastian and Samek, Wojciech and Arah, Onyebuchi Aniweta and Rod, Naja Hulvej and Ekstrøm, Claus Thorn},
  title         = {{Causes of Outcome Learning: a causal inference-inspired machine learning approach to disentangling common combinations of potential causes of a health outcome}},
  journal       = {International Journal of Epidemiology},
  year          = {2022},
  volume        = {51},
  number        = {5},
  pages         = {1622-1636},
  month         = {05},
  issn          = {0300-5771},
  __markedentry = {[lilli:1]},
  abstract      = {{Nearly all diseases are caused by different combinations of exposures. Yet, most epidemiological studies focus on estimating the effect of a single exposure on a health outcome. We present the Causes of Outcome Learning approach (CoOL), which seeks to discover combinations of exposures that lead to an increased risk of a specific outcome in parts of the population. The approach allows for exposures acting alone and in synergy with others. The road map of CoOL involves (i) a pre-computational phase used to define a causal model; (ii) a computational phase with three steps, namely (a) fitting a non-negative model on an additive scale, (b) decomposing risk contributions and (c) clustering individuals based on the risk contributions into subgroups; and (iii) a post-computational phase on hypothesis development, validation and triangulation using new data before eventually updating the causal model. The computational phase uses a tailored neural network for the non-negative model on an additive scale and layer-wise relevance propagation for the risk decomposition through this model. We demonstrate the approach on simulated and real-life data using the R package ‘CoOL’. The presentation focuses on binary exposures and outcomes but can also be extended to other measurement types. This approach encourages and enables researchers to identify combinations of exposures as potential causes of the health outcome of interest. Expanding our ability to discover complex causes could eventually result in more effective, targeted and informed interventions prioritized for their public health impact.}},
  doi           = {10.1093/ije/dyac078},
  eprint        = {https://academic.oup.com/ije/article-pdf/51/5/1622/48444694/dyac078.pdf},
  groups        = {relevant},
  keywords      = {rank3},
  url           = {https://doi.org/10.1093/ije/dyac078},
}

@Article{Locatello2018,
  author        = {Francesco Locatello and Stefan Bauer and Mario Lucic and Sylvain Gelly and Bernhard Sch{\"{o}}lkopf and Olivier Bachem},
  title         = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  journal       = {ArXiv},
  year          = {2018},
  volume        = {abs/1811.12359},
  __markedentry = {[lilli:1]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1811-12359.bib},
  eprint        = {1811.12359},
  eprinttype    = {arXiv},
  file          = {:Locatello2018 - Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank2},
  timestamp     = {Fri, 30 Nov 2018 12:44:28 +0100},
  url           = {http://arxiv.org/abs/1811.12359},
}

@InProceedings{Lippe2202,
  author    = {Phillip Lippe and Sara Magliacane and Sindy Löwe and Yuki M. Asano and Taco Cohen and Efstratios Gavves},
  title     = {CITRIS: Causal Identifiability from Temporal Intervened Sequences},
  booktitle = {Proceedings of the 39 th International Conference on Machine Learning},
  year      = {2022},
  file      = {:Lippe2202 - CITRIS_ Causal Identifiability from Temporal Intervened Sequences.pdf:PDF},
  groups    = {Recommended, relevant},
  keywords  = {rank4},
}

@Article{Arras2022,
  author        = {Leila Arras and Ahmed Osman and Wojciech Samek},
  title         = {CLEVR-XAI: A benchmark dataset for the ground truth evaluation of neural network explanations},
  journal       = {Information Fusion},
  year          = {2022},
  volume        = {81},
  pages         = {14-40},
  __markedentry = {[lilli:4]},
  doi           = {https://doi.org/10.1016/j.inffus.2021.11.008},
  file          = {:Arras2022 - CLEVR-XAI_ A benchmark dataset for the ground truth evaluation of neural network explanations.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://www.sciencedirect.com/science/article/pii/S1566253521002335?via%3Dihub},
}

@Article{Donges2017,
  author   = {Jonathan F Donges and Ricarda Winkelmann and Wolfgang Lucht and Sarah E Cornell and James G Dyke and Johan Rockström and Jobst Heitzig and Hans Joachim Schellnhuber},
  title    = {Closing the loop: Reconnecting human dynamics to Earth System science},
  journal  = {The Anthropocene Review},
  year     = {2017},
  volume   = {4},
  number   = {2},
  pages    = {151-157},
  abstract = { International commitment to the appropriately ambitious Paris climate agreement and the United Nations Sustainable Development Goals in 2015 has pulled into the limelight the urgent need for major scientific progress in understanding and modelling the Anthropocene, the tightly intertwined social-environmental planetary system that humanity now inhabits. The Anthropocene qualitatively differs from previous eras in Earth’s history in three key characteristics: (1) There is planetary-scale human agency. (2) There are social and economic networks of teleconnections spanning the globe. (3) It is dominated by planetary-scale social-ecological feedbacks. Bolting together old concepts and methodologies cannot be an adequate approach to describing this new geological era. Instead, we need a new paradigm in Earth System science that is founded equally on a deep understanding of the physical and biological Earth System – and of the economic, social and cultural forces that are now an intrinsic part of it. It is time to close the loop and bring socially mediated dynamics explicitly into theory, analysis and models that let us study the whole Earth System. },
  doi      = {10.1177/2053019617725537},
  eprint   = {https://doi.org/10.1177/2053019617725537},
  groups   = {Application Fields, socialclimate},
  keywords = {rank1},
  url      = { 
        https://doi.org/10.1177/2053019617725537
    
},
}

@InProceedings{Leemann2022,
  author        = {Tobias Leemann and Yao Rong and Stefan Kraft and Enkelejda Kasneci and Gjergji Kasneci},
  title         = {Coherence Evaluation of Visual Concepts With Objects and Language},
  booktitle     = {ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality},
  year          = {2022},
  __markedentry = {[lilli:1]},
  file          = {:Leemann2022 - Coherence Evaluation of Visual Concepts With Objects and Language.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank2},
  url           = {https://openreview.net/forum?id=rRzSSS_Ucg9},
}

@Article{Boers2019,
  author    = {Niklas Boers and Bedartha Goswami and Aljoscha Rheinwalt and Bodo Bookhagen and Brian Hoskins and Jürgen Kurths},
  title     = {Complex networks reveal global pattern of extreme-rainfall teleconnections},
  journal   = {Nature},
  year      = {2019},
  volume    = {566},
  number    = {7744},
  pages     = {373--377},
  month     = {jan},
  doi       = {10.1038/s41586-018-0872-x},
  file      = {:Boers2019 - Complex networks reveal global pattern of extreme-rainfall teleconnections.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Reimers2021,
  author        = {Reimers, Christian and Bodesheim, Paul and Runge, Jakob and Denzler, Joachim},
  title         = {Conditional Adversarial Debiasing: Towards Learning Unbiased Classifiers from Biased Data},
  booktitle     = {Pattern Recognition},
  year          = {2021},
  editor        = {Bauckhage, Christian and Gall, Juergen and Schwing, Alexander},
  pages         = {48--62},
  address       = {Cham},
  publisher     = {Springer International Publishing},
  __markedentry = {[lilli:1]},
  abstract      = {Bias in classifiers is a severe issue of modern deep learning methods, especially for their application in safety- and security-critical areas. Often, the bias of a classifier is a direct consequence of a bias in the training set, frequently caused by the co-occurrence of relevant features and irrelevant ones. To mitigate this issue, we require learning algorithms that prevent the propagation of known bias from the dataset into the classifier. We present a novel adversarial debiasing method, which addresses a feature of which we know that it is spuriously connected to the labels of training images but statistically independent of the labels for test images. The debiasing stops the classifier from falsly identifying this irrelevant feature as important. Irrelevant features co-occur with important features in a wide range of bias-related problems for many computer vision tasks, such as automatic skin cancer detection or driver assistance. We argue by a mathematical proof that our approach is superior to existing techniques for the abovementioned bias. Our experiments show that our approach performs better than the state-of-the-art on a well-known benchmark dataset with real-world images of cats and dogs.},
  file          = {:Reimers2021 - Conditional Adversarial Debiasing_ Towards Learning Unbiased Classifiers from Biased Data.pdf:PDF},
  groups        = {Application Fields, relevant},
  isbn          = {978-3-030-92659-5},
  keywords      = {rank4},
  url           = {https://link.springer.com/chapter/10.1007/978-3-030-92659-5_4},
}

@InProceedings{Reimers2021a,
  author        = {Reimers, Christian and Penzel, Niklas and Bodesheim, Paul and Runge, Jakob and Denzler, Joachim},
  title         = {Conditional Dependence Tests Reveal the Usage of ABCD Rule Features and Bias Variables in Automatic Skin Lesion Classification},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year          = {2021},
  pages         = {1810-1819},
  month         = {June},
  __markedentry = {[lilli:1]},
  file          = {:Reimers2021a - Conditional Dependence Tests Reveal the Usage of ABCD Rule Features and Bias Variables in Automatic Skin Lesion Classification.pdf:PDF},
  groups        = {Application Fields},
  keywords      = {rank1},
  url           = {https://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Reimers_Conditional_Dependence_Tests_Reveal_the_Usage_of_ABCD_Rule_Features_CVPRW_2021_paper.pdf},
}

@InProceedings{Park2021,
  author    = {Park, Junhyung and Shalit, Uri and Sch{\"o}lkopf, Bernhard and Muandet, Krikamol},
  title     = {Conditional Distributional Treatment Effect with Kernel Conditional Mean Embeddings and U-Statistic Regression},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  pages     = {8401--8412},
  month     = {18--24 Jul},
  publisher = {PMLR},
  abstract  = {We propose to analyse the conditional distributional treatment effect (CoDiTE), which, in contrast to the more common conditional average treatment effect (CATE), is designed to encode a treatment’s distributional aspects beyond the mean. We first introduce a formal definition of the CoDiTE associated with a distance function between probability measures. Then we discuss the CoDiTE associated with the maximum mean discrepancy via kernel conditional mean embeddings, which, coupled with a hypothesis test, tells us whether there is any conditional distributional effect of the treatment. Finally, we investigate what kind of conditional distributional effect the treatment has, both in an exploratory manner via the conditional witness function, and in a quantitative manner via U-statistic regression, generalising the CATE to higher-order moments. Experiments on synthetic, semi-synthetic and real datasets demonstrate the merits of our approach.},
  file      = {:Park2021 - Conditional Distributional Treatment Effect with Kernel Conditional Mean Embeddings and U-Statistic Regression.pdf:PDF;park21c.pdf:http\://proceedings.mlr.press/v139/park21c/park21c.pdf:PDF},
  url       = {https://proceedings.mlr.press/v139/park21c.html},
}

@Misc{Runge2017,
  author        = {Jakob Runge},
  title         = {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1709.01447},
  file          = {:Runge2017 - Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {stat.ML},
}

@Article{Malandri2022,
  author   = {Lorenzo Malandri and Fabio Mercorio and Mario Mezzanzanica and Navid Nobani and Andrea Seveso},
  title    = {ContrXT: Generating contrastive explanations from any text classifier},
  journal  = {Information Fusion},
  year     = {2022},
  volume   = {81},
  pages    = {103-115},
  issn     = {1566-2535},
  abstract = {The need for explanations of ML systems is growing as new models outperform their predecessors while becoming more complex and less comprehensible for their end-users. Though several XAI methods have been proposed in recent years, not enough attention was paid to explaining how models change their behaviour in contrast with previous ones (e.g., due to retraining). In such cases, an XAI system should explain why the model changes its predictions concerning past outcomes. Capturing and understanding such differences is crucial, as the need for trust is key in any application to support human-AI decision-making processes. This is the idea of ContrXT, a novel approach that (i) traces the decision criteria of a black box text classifier by encoding the changes in the decision logic through Binary Decision Diagrams. Then (ii) it provides global, model-agnostic, Time-Contrastive (T-contrast) explanations in natural language, estimating why – and to what extent – the model has modified its behaviour over time. We implemented and evaluated ContrXT over several supervised ML models trained on a benchmark dataset and a real-life application, showing it is effective in catching majorly changed classes and in explaining their variation through a user study. The approach has been implemented, and it is available to the community both as a python package and through REST API, providing contrastive explanations as a service.},
  doi      = {https://doi.org/10.1016/j.inffus.2021.11.016},
  groups   = {relevant},
  keywords = {Post-hoc explainability, Contrastive explanation methods for XAI, XAI interpretability of text classifiers, rank1},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253521002426},
}

@InProceedings{Forney2017,
  author    = {Andrew Forney and Judea Pearl and Elias Bareinboim},
  title     = {Counterfactual Data-Fusion for Online Reinforcement Learners},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1156--1164},
  month     = {06--11 Aug},
  publisher = {PMLR},
  abstract  = {The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent’s decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our findings’ efficacy with extensive simulations.},
  file      = {:Forney2017 - Counterfactual Data-Fusion for Online Reinforcement Learners.pdf:PDF},
  keywords  = {rank1},
  url       = {https://proceedings.mlr.press/v70/forney17a.html},
}

@InProceedings{Kusner2017,
  author    = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  title     = {Counterfactual Fairness},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  volume    = {30},
  publisher = {Curran Associates, Inc.},
  file      = {:Kusner2017 - Counterfactual Fairness.pdf:PDF},
  keywords  = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
}

@Misc{Popescu2021,
  author        = {Oana-Iuliana Popescu and Maha Shadaydeh and Joachim Denzler},
  title         = {Counterfactual Generation with Knockoffs},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.00951},
  file          = {:Popescu2021 - Counterfactual Generation with Knockoffs.pdf:PDF},
  primaryclass  = {cs.CV},
}

@InProceedings{Goyal2019a,
  author       = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  title        = {Counterfactual visual explanations},
  booktitle    = {International Conference on Machine Learning},
  year         = {2019},
  pages        = {2376--2384},
  organization = {PMLR},
}

@Article{Besserve2018,
  author        = {Michel Besserve and R{\'{e}}my Sun and Bernhard Sch{\"{o}}lkopf},
  title         = {Counterfactuals uncover the modular structure of deep generative models},
  journal       = {ArXiv},
  year          = {2018},
  volume        = {abs/1812.03253},
  __markedentry = {[lilli:1]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1812-03253.bib},
  eprint        = {1812.03253},
  eprinttype    = {arXiv},
  file          = {:Besserve2018 - Counterfactuals uncover the modular structure of deep generative models.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
  timestamp     = {Tue, 01 Jan 2019 15:01:25 +0100},
  url           = {http://arxiv.org/abs/1812.03253},
}

@InProceedings{Fel2023,
  author        = {Fel, Thomas and Picard, Agustin and Bethune, Louis and Boissin, Thibaut and Vigouroux, David and Colin, Julien and Cadénc, Rémi and Serre, Thomas},
  title         = {CRAFT: Concept Recursive Activation FacTorization for Explainability},
  booktitle     = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year          = {2023},
  pages         = {2711-2721},
  __markedentry = {[lilli:2]},
  doi           = {10.1109/CVPR52729.2023.00266},
  file          = {:Fel2023 - CRAFT_ Concept Recursive Activation FacTorization for Explainability.pdf:PDF},
  keywords      = {rank5},
}

@InBook{Schwab2019,
  title         = {CXPlain: Causal Explanations for Model Interpretation under Uncertainty},
  publisher     = {Curran Associates Inc.},
  year          = {2019},
  author        = {Schwab, Patrick and Karlen, Walter},
  address       = {Red Hook, NY, USA},
  __markedentry = {[lilli:2]},
  abstract      = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
  articleno     = {917},
  booktitle     = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  file          = {:Schwab2019 - CXPlain_ Causal Explanations for Model Interpretation under Uncertainty.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  numpages      = {11},
}

@InProceedings{Zheng2018,
  author    = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
  title     = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  year      = {2018},
  series    = {NIPS'18},
  pages     = {9492–9503},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  abstract  = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.},
  file      = {:Zheng2018 - DAGs with NO TEARS_ Continuous Optimization for Structure Learning.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank2},
  location  = {Montr\'{e}al, Canada},
  numpages  = {12},
  url       = {https://dl.acm.org/doi/10.5555/3327546.3327618},
}

@InProceedings{Simonyan2014,
  author        = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title         = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  year          = {2014},
  __markedentry = {[lilli:1]},
  file          = {:Simonyan2014 - Deep Inside Convolutional Networks_ Visualising Image Classification Models and Saliency Maps.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank2},
}

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{Runge2019a,
  author   = {Jakob Runge and Peer Nowack and Marlene Kretschmer and Seth Flaxman and Dino Sejdinovic},
  title    = {Detecting and quantifying causal associations in large nonlinear time series datasets},
  journal  = {Science Advances},
  year     = {2019},
  volume   = {5},
  number   = {11},
  pages    = {eaau4996},
  abstract = {THIS IS THE PCMCI PAPER 
A novel causal discovery method for estimating nonlinear interdependency networks from large time series datasets. Identifying causal relationships and quantifying their strength from observational time series data are key problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body. Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets. We validate the method on time series of well-understood physical mechanisms in the climate system and the human heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely new possibilities to discover and quantify causal networks from time series across a range of research fields.},
  doi      = {10.1126/sciadv.aau4996},
  eprint   = {https://www.science.org/doi/pdf/10.1126/sciadv.aau4996},
  keywords = {rank1},
  url      = {https://www.science.org/doi/abs/10.1126/sciadv.aau4996},
}

@Article{Hirata2016,
  author    = {Hirata, Yoshito AND Amigó, José M. AND Matsuzaka, Yoshiya AND Yokota, Ryo AND Mushiake, Hajime AND Aihara, Kazuyuki},
  title     = {Detecting Causality by Combined Use of Multiple Methods: Climate and Brain Examples},
  journal   = {PLOS ONE},
  year      = {2016},
  volume    = {11},
  number    = {7},
  pages     = {1-15},
  month     = {07},
  abstract  = {Identifying causal relations from time series is the first step to understanding the behavior of complex systems. Although many methods have been proposed, few papers have applied multiple methods together to detect causal relations based on time series generated from coupled nonlinear systems with some unobserved parts. Here we propose the combined use of three methods and a majority vote to infer causality under such circumstances. Two of these methods are proposed here for the first time, and all of the three methods can be applied even if the underlying dynamics is nonlinear and there are hidden common causes. We test our methods with coupled logistic maps, coupled Rössler models, and coupled Lorenz models. In addition, we show from ice core data how the causal relations among the temperature, the CH4 level, and the CO2 level in the atmosphere changed in the last 800,000 years, a conclusion also supported by irregularly sampled data analysis. Moreover, these methods show how three regions of the brain interact with each other during the visually cued, two-choice arm reaching task. Especially, we demonstrate that this is due to bottom up influences at the beginning of the task, while there exist mutual influences between the posterior medial prefrontal cortex and the presupplementary motor area. Based on our results, we conclude that identifying causality with an appropriate ensemble of multiple methods ensures the validity of the obtained results more firmly.},
  doi       = {10.1371/journal.pone.0158572},
  keywords  = {rank1},
  publisher = {Public Library of Science},
  url       = {https://doi.org/10.1371/journal.pone.0158572},
}

@Article{Sugihara2012,
  author   = {George Sugihara and Robert May and Hao Ye and Chih-hao Hsieh and Ethan Deyle and Michael Fogarty and Stephan Munch},
  title    = {Detecting Causality in Complex Ecosystems},
  journal  = {Science},
  year     = {2012},
  volume   = {338},
  number   = {6106},
  pages    = {496-500},
  abstract = {Three centuries ago, Bishop Berkeley's 1710 classic “A treatise on the nature of human knowledge,” first spelled out the “correlation vs. causation” dilemma. Sugihara et al. (p. 496, published online 20 September) present an approach to this conundrum, and extend current discussions about causation to dynamic systems with weak to moderate coupling (such as ecosystems). The resulting method, convergent cross mapping can detect causal linkages between time series. A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation. Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.},
  doi      = {10.1126/science.1227079},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.1227079},
  groups   = {relevant},
  keywords = {rank2},
  url      = {https://www.science.org/doi/abs/10.1126/science.1227079},
}

@InProceedings{Reimers2020,
  author        = {Reimers, Christian and Runge, Jakob and Denzler, Joachim},
  title         = {Determining the Relevance of Features for Deep Neural Networks},
  booktitle     = {Computer Vision -- ECCV 2020},
  year          = {2020},
  editor        = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages         = {330--346},
  address       = {Cham},
  publisher     = {Springer International Publishing},
  __markedentry = {[lilli:3]},
  abstract      = {Deep neural networks are tremendously successful in many applications, but end-to-end trained networks often result in hard to understand black-box classifiers or predictors. In this work, we present a novel method to identify whether a specific feature is relevant to a classifier's decision or not. This relevance is determined at the level of the learned mapping, instead of for a single example. The approach does neither need retraining of the network nor information on intermediate results or gradients. The key idea of our approach builds upon concepts from causal inference. We interpret machine learning in a structural causal model and use Reichenbach's common cause principle to infer whether a feature is relevant. We demonstrate empirically that the method is able to successfully evaluate the relevance of given features on three real-life data sets, namely MS COCO, CUB200 and HAM10000.},
  groups        = {relevant},
  isbn          = {978-3-030-58574-7},
  keywords      = {rank2},
  url           = {https://link.springer.com/chapter/10.1007/978-3-030-58574-7_20},
}

@InProceedings{Lopez-Paz2017,
  author        = {Lopez-Paz, David and Nishihara, Robert and Chintala, Soumith and Schölkopf, Bernhard and Bottou, Léon},
  title         = {Discovering Causal Signals in Images},
  booktitle     = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year          = {2017},
  pages         = {58-66},
  __markedentry = {[lilli:1]},
  doi           = {10.1109/CVPR.2017.14},
  file          = {:Lopez-Paz2017 - Discovering Causal Signals in Images.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
  primaryclass  = {stat.ML},
}

@Misc{Runge2020,
  author    = {Runge, Jakob},
  title     = {Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets},
  year      = {2020},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2003.03685},
  file      = {:Runge2020 - Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets.pdf:PDF;:bib/2003.03685.pdf:PDF},
  keywords  = {Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, rank1},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2003.03685},
}

@Misc{Chormai2022,
  author        = {Pattarawat Chormai and Jan Herrmann and Klaus-Robert Müller and Grégoire Montavon},
  title         = {Disentangled Explanations of Neural Network Predictions by Finding Relevant Subspaces},
  year          = {2022},
  __markedentry = {[lilli:3]},
  archiveprefix = {arXiv},
  eprint        = {2212.14855},
  file          = {:Chormai2022 - Disentangled Explanations of Neural Network Predictions by Finding Relevant Subspaces.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
}

@Misc{Sixt2022a,
  author        = {Leon Sixt and Martin Schuessler and Oana-Iuliana Popescu and Philipp Weiß and Tim Landgraf},
  title         = {Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset},
  year          = {2022},
  __markedentry = {[lilli:3]},
  archiveprefix = {arXiv},
  eprint        = {2204.11642},
  file          = {:Sixt2022a - Do Users Benefit From Interpretable Vision_ A User Study, Baseline, And Dataset.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
}

@Article{DiCapua2020,
  author   = {Di Capua, G. and Runge, J. and Donner, R. V. and van den Hurk, B. and Turner, A. G. and Vellore, R. and Krishnan, R. and Coumou, D.},
  title    = {Dominant patterns of interaction between the tropics and mid-latitudes in boreal summer: causal relationships and the role of timescales},
  journal  = {Weather and Climate Dynamics},
  year     = {2020},
  volume   = {1},
  number   = {2},
  pages    = {519--539},
  doi      = {10.5194/wcd-1-519-2020},
  keywords = {rank1},
  url      = {https://wcd.copernicus.org/articles/1/519/2020/},
}

@Misc{dsprites17,
  author        = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
  title         = {dSprites: Disentanglement testing Sprites dataset},
  howpublished  = {https://github.com/deepmind/dsprites-dataset/},
  year          = {2017},
  __markedentry = {[lilli:2]},
  keywords      = {rank5},
}

@InProceedings{Vowels2021,
  author   = {Matthew J. Vowels and Necati Cihan Camgoz and Richard Bowden and Cvssp and University of Surrey and U.k},
  title    = {D’ya like DAGs? A Survey on Structure Learning and Causal Discovery},
  year     = {2021},
  file     = {:Vowels2021 - D’ya like DAGs_ A Survey on Structure Learning and Causal Discovery.pdf:PDF},
  groups   = {Recommended, relevant},
  keywords = {rank2},
}

@Misc{Heitzig2020,
  author        = {Jobst Heitzig and Forest W. Simmons},
  title         = {Efficient democratic decisions via nondeterministic proportional consensus},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.06548},
  file          = {:Heitzig2020 - Efficient democratic decisions via nondeterministic proportional consensus.pdf:PDF},
  groups        = {Application Fields, socialclimate},
  keywords      = {rank1},
  primaryclass  = {econ.GN},
}

@Book{Peters2017,
  title     = {Elements of Causal Inference: Foundations and Learning Algorithms},
  publisher = {The MIT Press},
  year      = {2017},
  author    = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
  isbn      = {0262037319},
  abstract  = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning. The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.},
  file      = {:Peters2017 - Elements of Causal Inference_ Foundations and Learning Algorithms.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank3},
}

@Article{Assaad2022,
  author    = {Charles K. Assaad and Emilie Devijver and Eric Gaussier},
  title     = {Entropy-Based Discovery of Summary Causal Graphs in Time Series},
  journal   = {Entropy},
  year      = {2022},
  volume    = {24},
  number    = {8},
  pages     = {1156},
  month     = {aug},
  doi       = {10.3390/e24081156},
  file      = {:Assaad2022 - Entropy-Based Discovery of Summary Causal Graphs in Time Series.pdf:PDF},
  keywords  = {rank1},
  publisher = {{MDPI} {AG}},
}

@Article{Gevaert2022,
  author  = {Arne Gevaert and Axel-Jan Rousseau and Thijs Becker and Dirk Valkenborg and Tijl De Bie and Yvan Saeys},
  title   = {Evaluating Feature Attribution Methods in the Image Domain},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2202.12270},
  url     = {https://www.semanticscholar.org/reader/65f8cf77942dd5067ba96debc90aa009467c990d},
}

@Article{Samek2017a,
  author        = {Samek, Wojciech and Binder, Alexander and Montavon, Grégoire and Lapuschkin, Sebastian and Müller, Klaus-Robert},
  title         = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
  journal       = {IEEE Transactions on Neural Networks and Learning Systems},
  year          = {2017},
  volume        = {28},
  number        = {11},
  pages         = {2660-2673},
  __markedentry = {[lilli:3]},
  doi           = {10.1109/TNNLS.2016.2599820},
  file          = {:Samek2017a - Evaluating the Visualization of What a Deep Neural Network Has Learned.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
}

@Article{Szabo2007,
  author   = {György Szabó and Gábor Fáth},
  title    = {Evolutionary games on graphs},
  journal  = {Physics Reports},
  year     = {2007},
  volume   = {446},
  number   = {4},
  pages    = {97-216},
  issn     = {0370-1573},
  abstract = {Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first four sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fifth section surveys the topological complications implied by non-mean-field-type social network structures in general. The next three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock–Scissors–Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games.},
  doi      = {https://doi.org/10.1016/j.physrep.2007.04.004},
  keywords = {Game theory, Graphs, Networks, Evolution, rank1},
  url      = {https://www.sciencedirect.com/science/article/pii/S0370157307001810},
}

@Article{Samek2017,
  author        = {Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert},
  title         = {Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models},
  journal       = {ArXiv},
  year          = {2017},
  __markedentry = {[lilli:1]},
  file          = {:Samek2017 - Explainable artificial intelligence_ Understanding, visualizing and interpreting deep learning models.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
}

@Article{Goyal2019,
  author        = {{Goyal}, Yash and {Feder}, Amir and {Shalit}, Uri and {Kim}, Been},
  title         = {{Explaining Classifiers with Causal Concept Effect (CaCE)}},
  journal       = {ArXiv},
  year          = {2019},
  pages         = {arXiv:1907.07165},
  month         = jul,
  __markedentry = {[lilli:3]},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190707165G},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.1907.07165},
  eid           = {arXiv:1907.07165},
  eprint        = {1907.07165},
  file          = {:Goyal2019 - Explaining Classifiers with Causal Concept Effect (CaCE).pdf:PDF},
  groups        = {relevant},
  keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, rank5},
  primaryclass  = {cs.LG},
}

@Article{Narendra2018,
  author        = {Tanmayee Narendra and Anush Sankaran and Deepak Vijaykeerthy and Senthil Mani},
  title         = {Explaining Deep Learning Models using Causal Inference},
  journal       = {ArXiv},
  year          = {2018},
  volume        = {abs/1811.04376},
  __markedentry = {[lilli:2]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1811-04376.bib},
  eprint        = {1811.04376},
  eprinttype    = {arXiv},
  file          = {:Narendra2018 - Explaining Deep Learning Models using Causal Inference.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  timestamp     = {Fri, 23 Nov 2018 12:43:51 +0100},
  url           = {http://arxiv.org/abs/1811.04376},
}

@Article{Samek2021,
  author        = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  title         = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  journal       = {Proceedings of the IEEE},
  year          = {2021},
  volume        = {109},
  number        = {3},
  pages         = {247-278},
  __markedentry = {[lilli:1]},
  doi           = {10.1109/JPROC.2021.3060483},
  file          = {:Samek2021 - Explaining Deep Neural Networks and Beyond_ A Review of Methods and Applications.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://ieeexplore.ieee.org/document/9369420},
}

@Misc{Gilpin2019,
  author        = {Leilani H. Gilpin and David Bau and Ben Z. Yuan and Ayesha Bajwa and Michael Specter and Lalana Kagal},
  title         = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1806.00069},
  primaryclass  = {cs.AI},
}

@InProceedings{Chang2019,
  author    = {Chun-Hao Chang and Elliot Creager and Anna Goldenberg and David Duvenaud},
  title     = {Explaining Image Classifiers by Counterfactual Generation},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1MXz20cYQ},
}

@InProceedings{Agarwal2020,
  author    = {Chirag Agarwal and Anh Gia-Tuan Nguyen},
  title     = {Explaining Image Classifiers by Removing Input Features Using Generative Models},
  booktitle = {Asian Conference on Computer Vision},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:222142338},
}

@Article{Aas2021,
  author        = {Kjersti Aas and Martin Jullum and Anders Løland},
  title         = {Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
  journal       = {Artificial Intelligence},
  year          = {2021},
  volume        = {298},
  pages         = {103502},
  issn          = {0004-3702},
  __markedentry = {[lilli:1]},
  abstract      = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.},
  doi           = {https://doi.org/10.1016/j.artint.2021.103502},
  keywords      = {Feature attribution, Shapley values, Kernel SHAP, Dependence, rank5},
  url           = {https://www.sciencedirect.com/science/article/pii/S0004370221000539},
}

@Article{Montavon2017,
  author        = {Gr{\'{e}}goire Montavon and Sebastian Lapuschkin and Alexander Binder and Wojciech Samek and Klaus-Robert M\"{u}ller},
  title         = {Explaining nonlinear classification decisions with deep Taylor decomposition},
  journal       = {Pattern Recognition},
  year          = {2017},
  volume        = {65},
  pages         = {211--222},
  month         = may,
  __markedentry = {[lilli:1]},
  doi           = {10.1016/j.patcog.2016.11.008},
  groups        = {relevant},
  keywords      = {rank5},
  publisher     = {Elsevier {BV}},
  url           = {https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub},
}

@InProceedings{Parafita2019,
  author        = {Parafita, Alvaro and Vitria, Jordi},
  title         = {Explaining Visual Models by Causal Attribution},
  booktitle     = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  year          = {2019},
  pages         = {4167-4175},
  __markedentry = {[lilli:3]},
  doi           = {10.1109/ICCVW.2019.00512},
  file          = {:Parafita2019 - Explaining Visual Models by Causal Attribution.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  primaryclass  = {stat.ML},
}

@Article{Miller2019,
  author   = {Tim Miller},
  title    = {Explanation in artificial intelligence: Insights from the social sciences},
  journal  = {Artificial Intelligence},
  year     = {2019},
  volume   = {267},
  pages    = {1-38},
  issn     = {0004-3702},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  doi      = {https://doi.org/10.1016/j.artint.2018.07.007},
  keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
}

@Misc{Qi2023,
  author        = {Ruoxi Qi and Yueyuan Zheng and Yi Yang and Caleb Chen Cao and Janet H. Hsiao},
  title         = {Explanation Strategies for Image Classification in Humans vs. Current Explainable AI},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2304.04448},
  file          = {:Qi2023 - Explanation Strategies for Image Classification in Humans vs. Current Explainable AI.pdf:PDF},
  keywords      = {rank1},
  primaryclass  = {cs.HC},
}

@Conference{Dombrowski2019,
  author    = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and M\"{u}ller, Klaus-Robert and Kessel, Pan},
  title     = {Explanations can be manipulated and geometry is to blame},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
  volume    = {32},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf},
}

@InProceedings{Anders2020,
  author       = {Anders, Christopher and Pasliev, Plamen and Dombrowski, Ann-Kathrin and M{\"u}ller, Klaus-Robert and Kessel, Pan},
  title        = {Fairwashing explanations with off-manifold detergent},
  booktitle    = {International Conference on Machine Learning},
  year         = {2020},
  pages        = {314--323},
  organization = {PMLR},
}

@InProceedings{None2016,
  author    = {None},
  title     = {Fast Greedy Search (FGES) Algorithm for Continuous Variables},
  booktitle = {None},
  year      = {2016},
  file      = {:None2016 - Fast Greedy Search (FGES) Algorithm for Continuous Variables.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank2},
}

@Misc{Janzing2019,
  author        = {Dominik Janzing and Lenon Minorics and Patrick Blöbaum},
  title         = {Feature relevance quantification in explainable AI: A causal problem},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1910.13413},
  file          = {:Janzing2019 - Feature relevance quantification in explainable AI_ A causal problem.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {stat.ML},
}

@Article{Olah2017,
  author        = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title         = {Feature Visualization},
  journal       = {Distill},
  year          = {2017},
  note          = {https://distill.pub/2017/feature-visualization},
  __markedentry = {[lilli:1]},
  doi           = {10.23915/distill.00007},
}

@Article{Anders2022,
  author   = {Christopher J. Anders and Leander Weber and David Neumann and Wojciech Samek and Klaus-Robert Müller and Sebastian Lapuschkin},
  title    = {Finding and removing Clever Hans: Using explanation methods to debug and improve deep models},
  journal  = {Information Fusion},
  year     = {2022},
  volume   = {77},
  pages    = {261-295},
  issn     = {1566-2535},
  abstract = {Contemporary learning models for computer vision are typically trained on very large (benchmark) datasets with millions of samples. These may, however, contain biases, artifacts, or errors that have gone unnoticed and are exploitable by the model. In the worst case, the trained model does not learn a valid and generalizable strategy to solve the problem it was trained for, and becomes a “Clever Hans” predictor that bases its decisions on spurious correlations in the training data, potentially yielding an unrepresentative or unfair, and possibly even hazardous predictor. In this paper, we contribute by providing a comprehensive analysis framework based on a scalable statistical analysis of attributions from explanation methods for large data corpora. Based on a recent technique — Spectral Relevance Analysis — we propose the following technical contributions and resulting findings: (a) a scalable quantification of artifactual and poisoned classes where the machine learning models under study exhibit Clever Hans behavior, (b) several approaches we collectively denote as Class Artifact Compensation, which are able to effectively and significantly reduce a model’s Clever Hans behavior, i.e., we are able to un-Hans models trained on (poisoned) datasets, such as the popular ImageNet data corpus. We demonstrate that Class Artifact Compensation, defined in a simple theoretical framework, may be implemented as part of a neural network’s training or fine-tuning process, or in a post-hoc manner by injecting additional layers, preventing any further propagation of undesired Clever Hans features, into the network architecture. Using our proposed methods, we provide qualitative and quantitative analyses of the biases and artifacts in, e.g., the ImageNet dataset, the Adience benchmark dataset of unfiltered faces, and the ISIC 2019 skin lesion analysis dataset. We demonstrate that these insights can give rise to improved, more representative, and fairer models operating on implicitly cleaned data corpora.},
  doi      = {https://doi.org/10.1016/j.inffus.2021.07.015},
  keywords = {Deep Neural Networks, Explainable Artificial Intelligence, Clever Hans predictors, Feature unlearning, Spectral Relevance Analysis, Class Artifact Compensation},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253521001573},
}

@InProceedings{Bommer2023,
  author        = {Philine Bommer and Marlene Kretschmer and Anna Hedström and Dilyara Bareeva and Marina M. -C. Höhne},
  title         = {Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.00652},
  file          = {:Bommer2023 - Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {cs.LG},
}

@Article{Kawamoto2022,
  author      = {Yusuke Kawamoto and Tetsuya Sato and Kohei Suenaga},
  title       = {Formalizing Statistical Causality via Modal Logic},
  abstract    = {We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for specifying causal effects on random variables. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to prove and explain the correctness of statistical causal inference.},
  date        = {2022-10-30},
  eprint      = {2210.16751v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Kawamoto2022 - Formalizing Statistical Causality via Modal Logic.pdf:PDF},
  keywords    = {cs.AI, cs.LO, rank1},
}

@Misc{Achtibat2022,
  author        = {Reduan Achtibat and Maximilian Dreyer and Ilona Eisenbraun and Sebastian Bosse and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin},
  title         = {From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation},
  year          = {2022},
  __markedentry = {[lilli:5]},
  archiveprefix = {arXiv},
  eprint        = {2206.03208},
  file          = {:Achtibat2022 - From _Where_ to _What__ Towards Human-Understandable Explanations through Concept Relevance Propagation.pdf:PDF},
  groups        = {relevant},
  journal       = {ArXiv},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
}

@Article{Nauta2023,
  author     = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
  title      = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
  journal    = {ACM Comput. Surv.},
  year       = {2023},
  volume     = {55},
  number     = {13s},
  month      = {jul},
  issn       = {0360-0300},
  abstract   = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
  address    = {New York, NY, USA},
  articleno  = {295},
  doi        = {10.1145/3583558},
  issue_date = {December 2023},
  keywords   = {XAI, explainable AI, quantitative evaluation methods, interpretability, explainability, evaluation, interpretable machine learning, Explainable artificial intelligence},
  numpages   = {42},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3583558},
}

@Article{Achtibat2023,
  author        = {Achtibat, Reduan AND Dreyer, Maximilian AND Eisenbraun, Ilona AND Bosse, Sebastian AND Wiegand, Thomas AND Samek, Wojciech AND Lapuschkin, Sebastian},
  title         = {From attribution maps to human-understandable explanations through Concept Relevance Propagation},
  journal       = {Nature Machine Intelligence},
  year          = {2023},
  volume        = {5},
  number        = {9},
  pages         = {1006 - 1019},
  month         = {jul},
  issn          = {2522-5839},
  __markedentry = {[lilli:5]},
  abstract      = {The field of explainable artificial intelligence (XAI) aims to bring transparency to today’s powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying ‘where’ important features occur (but not providing information about ‘what’ they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model’s reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the ‘where’ and ‘what’ questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model’s representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.},
  doi           = {10.1038/s42256-023-00711-8},
  file          = {:Achtibat2023 - From attribution maps to human-understandable explanations through Concept Relevance Propagation.pdf:PDF},
  issue_date    = {2023/09/01},
  keywords      = {rank5},
}

@Article{Kauffmann2022,
  author        = {Jacob Kauffmann and Malte Esders and Lukas Ruff and Gregoire Montavon and Wojciech Samek and Klaus-Robert Muller},
  title         = {From Clustering to Cluster Explanations via Neural Networks},
  journal       = {{IEEE} Transactions on Neural Networks and Learning Systems},
  year          = {2022},
  pages         = {1--15},
  __markedentry = {[lilli:1]},
  doi           = {10.1109/tnnls.2022.3185901},
  file          = {:Kauffmann2022 - From Clustering to Cluster Explanations via Neural Networks.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url           = {https://doi.org/10.1109%2Ftnnls.2022.3185901},
}

@Misc{Dreyer2023a,
  author        = {Maximilian Dreyer and Frederik Pahde and Christopher J. Anders and Wojciech Samek and Sebastian Lapuschkin},
  title         = {From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space},
  year          = {2023},
  __markedentry = {[lilli:2]},
  archiveprefix = {arXiv},
  eprint        = {2308.09437},
  file          = {:Dreyer2023a - From Hope to Safety_ Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space.pdf:PDF},
  keywords      = {rank4},
  primaryclass  = {cs.LG},
}

@Book{Rasmussen2005,
  title     = {{Gaussian Processes for Machine Learning}},
  publisher = {The MIT Press},
  year      = {2005},
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  month     = {11},
  isbn      = {9780262256834},
  abstract  = {{A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.}},
  doi       = {10.7551/mitpress/3206.001.0001},
  keywords  = {rank1},
  url       = {https://doi.org/10.7551/mitpress/3206.001.0001},
}

@InProceedings{OShaughnessy2020,
  author        = {O'Shaughnessy, Matthew and Canal, Gregory and Connor, Marissa and Davenport, Mark and Rozell, Christopher},
  title         = {Generative Causal Explanations of Black-Box Classifiers},
  booktitle     = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  year          = {2020},
  series        = {NIPS'20},
  address       = {Red Hook, NY, USA},
  publisher     = {Curran Associates Inc.},
  __markedentry = {[lilli:1]},
  abstract      = {We develop a method for generating causal post-hoc explanations of black-box classifiers based on a learned low-dimensional representation of the data. The explanation is causal in the sense that changing learned latent factors produces a change in the classifier output statistics. To construct these explanations, we design a learning framework that leverages a generative model and information-theoretic measures of causal influence. Our objective function encourages both the generative model to faithfully represent the data distribution and the latent factors to have a large causal influence on the classifier output. Our method learns both global and local explanations, is compatible with any classifier that admits class probabilities and a gradient, and does not require labeled attributes or knowledge of causal structure. Using carefully controlled test cases, we provide intuition that illuminates the function of our objective. We then demonstrate the practical utility of our method on image recognition tasks.},
  articleno     = {458},
  file          = {:OShaughnessy2020 - Generative causal explanations of black-box classifiers.pdf:PDF},
  groups        = {relevant},
  isbn          = {9781713829546},
  keywords      = {rank5},
  location      = {Vancouver, BC, Canada},
  numpages      = {15},
}

@Article{Marconato2021,
  author   = {Marconato, Emanuele and Passerini, Andrea and Teso, Stefano},
  title    = {GlanceNets: Interpretable, Leak-proof Concept-based Models},
  year     = {2021},
  file     = {:Marconato2021 - GlanceNets_ Interpretable, Leak-proof Concept-based Models.pdf:PDF},
  keywords = {rank1},
}

@Article{Szegedy2014,
  author     = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott E. Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  title      = {Going Deeper with Convolutions},
  journal    = {CoRR},
  year       = {2014},
  volume     = {abs/1409.4842},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/SzegedyLJSRAEVR14.bib},
  eprint     = {1409.4842},
  eprinttype = {arXiv},
  keywords   = {rank1},
  timestamp  = {Mon, 13 Aug 2018 16:48:52 +0200},
  url        = {http://arxiv.org/abs/1409.4842},
}

@Article{Henckel2022,
  author    = {Henckel, Leonard and Perković, Emilija and Maathuis, Marloes H.},
  title     = {Graphical Criteria for Efficient Total Effect Estimation Via Adjustment in Causal Linear Models},
  journal   = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  year      = {2022},
  volume    = {84},
  number    = {2},
  pages     = {579–599},
  month     = mar,
  issn      = {1467-9868},
  doi       = {10.1111/rssb.12451},
  file      = {:Henckel2022 - Graphical Criteria for Efficient Total Effect Estimation Via Adjustment in Causal Linear Models.pdf:PDF},
  keywords  = {rank1},
  publisher = {Oxford University Press (OUP)},
}

@Misc{Grover2019,
  author        = {Aditya Grover and Aaron Zweig and Stefano Ermon},
  title         = {Graphite: Iterative Generative Modeling of Graphs},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1803.10459},
  keywords      = {rank1},
  primaryclass  = {stat.ML},
}

@Misc{Haelvae2020,
  author    = {Hälvä, Hermanni and Hyvärinen, Aapo},
  title     = {Hidden Markov Nonlinear ICA: Unsupervised Learning from Nonstationary Time Series},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2006.12107},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, rank1},
  publisher = {arXiv},
}

@Article{Schnake2022,
  author   = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Schütt, Kristof T. and Müller, Klaus-Robert and Montavon, Grégoire},
  title    = {Higher-Order Explanations of Graph Neural Networks via Relevant Walks},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2022},
  volume   = {44},
  number   = {11},
  pages    = {7581-7596},
  doi      = {10.1109/TPAMI.2021.3115452},
  file     = {:Schnake2022 - Higher-Order Explanations of Graph Neural Networks via Relevant Walks.pdf:PDF},
  groups   = {relevant},
  keywords = {rank2},
}

@Misc{Gerhardus2020,
  author    = {Gerhardus, Andreas and Runge, Jakob},
  title     = {High-recall causal discovery for autocorrelated time series with latent confounders},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2007.01884},
  keywords  = {Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, rank1},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2007.01884},
}

@Article{Runge2015,
  author    = {Jakob Runge and Vladimir Petoukhov and Jonathan F. Donges and Jaroslav Hlinka and Nikola Jajcay and Martin Vejmelka and David Hartman and Norbert Marwan and Milan Palu{\v{s}} and Jürgen Kurths},
  title     = {Identifying causal gateways and mediators in complex spatio-temporal systems},
  journal   = {Nature Communications},
  year      = {2015},
  volume    = {6},
  number    = {1},
  month     = {oct},
  doi       = {10.1038/ncomms9502},
  file      = {:Runge2015 - Identifying causal gateways and mediators in complex spatio-temporal systems.pdf:PDF},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{Kalibhat2023,
  author        = {Neha Kalibhat and Shweta Bhardwaj and Bayan Bruss and Hamed Firooz and Maziar Sanjabi and Soheil Feizi},
  title         = {Identifying Interpretable Subspaces in Image Representations},
  year          = {2023},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2307.10504},
  file          = {:Kalibhat2023 - Identifying Interpretable Subspaces in Image Representations.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {cs.CV},
}

@Article{Bilodeau2024,
  author    = {Bilodeau, Blair and Jaques, Natasha and Koh, Pang Wei and Kim, Been},
  title     = {Impossibility theorems for feature attribution},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2024},
  volume    = {121},
  number    = {2},
  month     = jan,
  issn      = {1091-6490},
  doi       = {10.1073/pnas.2304406120},
  file      = {:Bilodeau2024 - Impossibility theorems for feature attribution.pdf:PDF},
  keywords  = {adversarial attack, measurement bias, explanations bad},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Misc{Geiger2022,
  author        = {Atticus Geiger and Zhengxuan Wu and Hanson Lu and Josh Rozner and Elisa Kreiss and Thomas Icard and Noah D. Goodman and Christopher Potts},
  title         = {Inducing Causal Structure for Interpretable Neural Networks},
  year          = {2022},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2112.00826},
  file          = {:Geiger2022 - Inducing Causal Structure for Interpretable Neural Networks.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {cs.LG},
}

@Article{Runge2019,
  author    = {Jakob Runge and Sebastian Bathiany and Erik Bollt and Gustau Camps-Valls and Dim Coumou and Ethan Deyle and Clark Glymour and Marlene Kretschmer and Miguel D. Mahecha and Jordi Mu{\~{n}}oz-Mar{\'{\i}} and Egbert H. van Nes and Jonas Peters and Rick Quax and Markus Reichstein and Marten Scheffer and Bernhard Schölkopf and Peter Spirtes and George Sugihara and Jie Sun and Kun Zhang and Jakob Zscheischler},
  title     = {Inferring causation from time series in Earth system sciences},
  journal   = {Nature Communications},
  year      = {2019},
  volume    = {10},
  number    = {1},
  month     = {jun},
  doi       = {10.1038/s41467-019-10105-3},
  file      = {:Runge2019 - Inferring causation from time series in Earth system sciences.pdf:PDF},
  keywords  = {rank1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Holzinger2022,
  author   = {Andreas Holzinger and Matthias Dehmer and Frank Emmert-Streib and Rita Cucchiara and Isabelle Augenstein and Javier Del Ser and Wojciech Samek and Igor Jurisica and Natalia Díaz-Rodríguez},
  title    = {Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence},
  journal  = {Information Fusion},
  year     = {2022},
  volume   = {79},
  pages    = {263-278},
  issn     = {1566-2535},
  abstract = {Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt that AI is important to improve human health in many ways and will disrupt various medical workflows in the future. Using AI to solve problems in medicine beyond the lab, in routine environments, we need to do more than to just improve the performance of existing AI methods. Robust AI solutions must be able to cope with imprecision, missing and incorrect information, and explain both the result and the process of how it was obtained to a medical expert. Using conceptual knowledge as a guiding model of reality can help to develop more robust, explainable, and less biased machine learning models that can ideally learn from less data. Achieving these goals will require an orchestrated effort that combines three complementary Frontier Research Areas: (1) Complex Networks and their Inference, (2) Graph causal models and counterfactuals, and (3) Verification and Explainability methods. The goal of this paper is to describe these three areas from a unified view and to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI. This makes it imperative to include ethical and legal aspects as a cross-cutting discipline, because all future solutions must not only be ethically responsible, but also legally compliant.},
  doi      = {https://doi.org/10.1016/j.inffus.2021.10.007},
  groups   = {relevant},
  keywords = {Artificial intelligence, Information fusion, Medical AI, Explainable AI, Robustness, Explainability, Trust, Graph-based machine learning, Neural-symbolic learning and reasoning, rank2},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253521002050},
}

@InProceedings{VerSteeg2012,
  author    = {Ver Steeg, Greg and Galstyan, Aram},
  title     = {Information Transfer in Social Media},
  booktitle = {Proceedings of the 21st International Conference on World Wide Web},
  year      = {2012},
  series    = {WWW '12},
  pages     = {509–518},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  abstract  = {Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior, characterizing patterns of information diffusion, and identifying influential individuals. In this paper we suggest a measure of causal relationships between nodes based on the information--theoretic notion of transfer entropy, or information transfer. This theoretically grounded measure is based on dynamic information, captures fine--grain notions of influence, and admits a natural, predictive interpretation. Networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics. We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures. In addition to altering our notion of who is influential, transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups.},
  doi       = {10.1145/2187836.2187906},
  groups    = {socialclimate},
  isbn      = {9781450312295},
  keywords  = {social networks, causality, point processes, prediction, spam, entropy, rank1},
  location  = {Lyon, France},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2187836.2187906},
}

@Misc{Alber2018,
  author        = {Maximilian Alber and Sebastian Lapuschkin and Philipp Seegerer and Miriam Hägele and Kristof T. Schütt and Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller and Sven Dähne and Pieter-Jan Kindermans},
  title         = {iNNvestigate neural networks!},
  year          = {2018},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {1808.04260},
  file          = {:Alber2018 - iNNvestigate neural networks!.pdf:PDF},
  keywords      = {rank1},
  primaryclass  = {cs.LG},
}

@InProceedings{Panda2021,
  author        = {Panda, Pranoy and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N},
  title         = {Instance-wise Causal Feature Selection for Model Interpretation},
  booktitle     = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year          = {2021},
  pages         = {1756-1759},
  __markedentry = {[lilli:1]},
  doi           = {10.1109/CVPRW53098.2021.00194},
  file          = {:Panda2021 - Instance-wise Causal Feature Selection for Model Interpretation.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
}

@InProceedings{Kim2018,
  author        = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory},
  title         = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
  booktitle     = {Proceedings of the 35th International Conference on Machine Learning},
  year          = {2018},
  editor        = {Dy, Jennifer and Krause, Andreas},
  volume        = {80},
  series        = {Proceedings of Machine Learning Research},
  pages         = {2668--2677},
  month         = {10--15 Jul},
  publisher     = {PMLR},
  __markedentry = {[lilli:3]},
  abstract      = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  file          = {:Kim2018 - Interpretability Beyond Feature Attribution_ Quantitative Testing with Concept Activation Vectors (TCAV).pdf:PDF;kim18d.pdf:http\://proceedings.mlr.press/v80/kim18d/kim18d.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://proceedings.mlr.press/v80/kim18d.html},
}

@InProceedings{Fong2017,
  author    = {Fong, Ruth C. and Vedaldi, Andrea},
  title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  year      = {2017},
  pages     = {3449-3457},
  doi       = {10.1109/ICCV.2017.371},
}

@InProceedings{Ghorbani2019a,
  author        = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  title         = {Interpretation of neural networks is fragile},
  booktitle     = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
  year          = {2019},
  series        = {AAAI'19/IAAI'19/EAAI'19},
  publisher     = {AAAI Press},
  __markedentry = {[lilli:1]},
  abstract      = {In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.},
  articleno     = {452},
  doi           = {10.1609/aaai.v33i01.33013681},
  isbn          = {978-1-57735-809-1},
  location      = {Honolulu, Hawaii, USA},
  numpages      = {8},
  url           = {https://doi.org/10.1609/aaai.v33i01.33013681},
}

@Article{Bollt2018,
  author   = {Bollt,Erik M. and Sun,Jie and Runge,Jakob},
  title    = {Introduction to Focus Issue: Causation inference and information flow in dynamical systems: Theory and applications},
  journal  = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  year     = {2018},
  volume   = {28},
  number   = {7},
  pages    = {075201},
  doi      = {10.1063/1.5046848},
  eprint   = {https://doi.org/10.1063/1.5046848},
  keywords = {rank1},
  url      = { 
        https://doi.org/10.1063/1.5046848
    
},
}

@InProceedings{Zhang2021,
  author        = {Ruihan Zhang and Prashan Madumal and Tim Miller and Krista A. Ehinger and Benjamin I. P. Rubinstein},
  title         = {Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors},
  booktitle     = {AAAI Conference on Artificial Intelligence},
  year          = {2021},
  volume        = {35},
  publisher     = {AAAI Press, Palo Alto, California USA},
  __markedentry = {[lilli:2]},
  archiveprefix = {arXiv},
  eprint        = {2006.15417},
  file          = {:Zhang2021 - Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.CV},
}

@InCollection{Schoelkopf2018,
  author    = {Bernhard Schölkopf; Alexander J. Smola},
  title     = {{Kernels}},
  booktitle = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
  publisher = {The MIT Press},
  year      = {2018},
  month     = {06},
  isbn      = {9780262256933},
  doi       = {10.7551/mitpress/4175.003.0005},
  eprint    = {https://direct.mit.edu/book/chapter-pdf/154459/9780262256933\_cab.pdf},
  groups    = {relevant},
  keywords  = {rank2},
  url       = {https://doi.org/10.7551/mitpress/4175.003.0005},
}

@Article{Rozanec2022,
  author   = {Jože M. Rožanec and Blaž Fortuna and Dunja Mladenić},
  title    = {Knowledge graph-based rich and confidentiality preserving Explainable Artificial Intelligence (XAI)},
  journal  = {Information Fusion},
  year     = {2022},
  volume   = {81},
  pages    = {91-102},
  issn     = {1566-2535},
  abstract = {The paper proposes a novel architecture for explainable artificial intelligence based on semantic technologies and artificial intelligence. We tailor the architecture for the domain of demand forecasting and validate it on a real-world case study. The explanations provided result from knowledge fusion regarding concepts describing features relevant to a particular forecast, related media events, and metadata regarding external datasets of interest. The Knowledge Graph enhances the quality of explanations by informing concepts at a higher abstraction level rather than specific features. By doing so, explanations avoid exposing sensitive details regarding the demand forecasting models, thus preserving confidentiality. In addition, the Knowledge Graph enables linking domain knowledge, forecasted values, and forecast explanations while also providing insights into actionable aspects on which users can take action. The ontology and dataset we developed for this use case are publicly available for further research.},
  doi      = {https://doi.org/10.1016/j.inffus.2021.11.015},
  groups   = {relevant},
  keywords = {Explainable Artificial Intelligence, Knowledge Graph, Demand forecasting, Smart manufacturing, Confidentiality, Privacy, rank2},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253521002414},
}

@InBook{Montavon2019,
  pages         = {193--209},
  title         = {Layer-Wise Relevance Propagation: An Overview},
  publisher     = {Springer International Publishing},
  year          = {2019},
  author        = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  editor        = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  address       = {Cham},
  isbn          = {978-3-030-28954-6},
  __markedentry = {[lilli:1]},
  abstract      = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a `deep Taylor decomposition', (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
  booktitle     = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  doi           = {10.1007/978-3-030-28954-6_10},
  groups        = {relevant},
  keywords      = {rank3},
  url           = {https://doi.org/10.1007/978-3-030-28954-6_10},
}

@Misc{Chickering2013,
  author        = {David Maxwell Chickering},
  title         = {Learning Equivalence Classes of Bayesian Networks Structures},
  year          = {2013},
  archiveprefix = {arXiv},
  eprint        = {1302.3566},
  file          = {:Chickering2013 - Learning Equivalence Classes of Bayesian Networks Structures.pdf:PDF},
  keywords      = {rank1},
  primaryclass  = {cs.AI},
}

@InProceedings{Kindermans2017,
  author        = {Pieter-Jan Kindermans and Kristof T. Sch{\"u}tt and Maximilian Alber and Klaus-Robert M{\"u}ller and D. Erhan and Been Kim and Sven D{\"a}hne},
  title         = {Learning how to explain neural networks: PatternNet and PatternAttribution},
  booktitle     = {International Conference on Learning Representations},
  year          = {2017},
  __markedentry = {[lilli:2]},
  file          = {:Kindermans2017 - Learning how to explain neural networks_ PatternNet and PatternAttribution.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://api.semanticscholar.org/CorpusID:32654687},
}

@InProceedings{Chen2018,
  author        = {Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  title         = {Learning to Explain: An Information-Theoretic Perspective on Model Interpretation},
  booktitle     = {Proceedings of the 35th International Conference on Machine Learning},
  year          = {2018},
  editor        = {Dy, Jennifer and Krause, Andreas},
  volume        = {80},
  series        = {Proceedings of Machine Learning Research},
  pages         = {883--892},
  month         = {10--15 Jul},
  publisher     = {PMLR},
  __markedentry = {[lilli:1]},
  abstract      = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
  file          = {:Chen2018 - Learning to Explain_ An Information-Theoretic Perspective on Model Interpretation.pdf:PDF;chen18j.pdf:http\://proceedings.mlr.press/v80/chen18j/chen18j.pdf:PDF},
  keywords      = {rank3},
  url           = {https://proceedings.mlr.press/v80/chen18j.html},
}

@Book{Woodward2004,
  title     = {{Making Things Happen: A Theory of Causal Explanation}},
  publisher = {Oxford University Press},
  year      = {2004},
  author    = {Woodward, James},
  month     = {01},
  isbn      = {9780195155273},
  abstract  = {{This book develops a manipulationist theory of causation and explanation: causal and explanatory relationships are relationships that are potentially exploitable for purposes of manipulation and control. The resulting theory is a species of counterfactual theory that (I claim) avoids the difficulties and counterexamples that have infected alternative accounts of causation and explanation, from the Deductive-Nomological model onwards. One of the key concepts in this theory is the notion of an intervention, which is an idealization of the notion of an experimental manipulation that is stripped of its anthropocentric elements. This notion is used to provide a characterization of causal relationships that is non-reductive but also not viciously circular. Relationships that correctly tell us how the value of one variable Y would change under interventions on a second variable Y are invariant. The notion of an invariant relationship is more helpful than the notion of a law of nature (the notion on which philosophers have traditionally relied) in understanding how explanation and causal attribution work in the special sciences.}},
  doi       = {10.1093/0195155270.001.0001},
  url       = {https://doi.org/10.1093/0195155270.001.0001},
}

@Article{Koester2022,
  author   = {Köster, Felix and Yanchuk, Serhiy and Lüdge, Kathy},
  title    = {Master Memory Function for Delay-Based Reservoir Computers With Single-Variable Dynamics},
  journal  = {Ieee Transactions on Neural Networks and Learning Systems},
  year     = {2022},
  file     = {:Koester2022 - Master Memory Function for Delay-Based Reservoir Computers With Single-Variable Dynamics.pdf:PDF},
  keywords = {rank1},
}

@Article{Schreiber2000,
  author    = {Thomas Schreiber},
  title     = {Measuring Information Transfer},
  journal   = {Physical Review Letters},
  year      = {2000},
  volume    = {85},
  number    = {2},
  pages     = {461--464},
  month     = {jul},
  doi       = {10.1103/physrevlett.85.461},
  file      = {:bib/0001042.pdf:PDF},
  keywords  = {rank1},
  publisher = {American Physical Society ({APS})},
  url       = {https://doi.org/10.1103%2Fphysrevlett.85.461},
}

@Misc{Sen2017,
  author        = {Rajat Sen and Ananda Theertha Suresh and Karthikeyan Shanmugam and Alexandros G. Dimakis and Sanjay Shakkottai},
  title         = {Model-Powered Conditional Independence Test},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {https://doi.org/10.48550/arXiv.1709.06138},
  eprint        = {1709.06138},
  keywords      = {rank1},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1709.06138},
}

@Article{Vielhaben2023,
  author        = {Johanna Vielhaben and Stefan Bluecher and Nils Strodthoff},
  title         = {Multi-dimensional concept discovery ({MCD}): A unifying framework with completeness guarantees},
  journal       = {Transactions on Machine Learning Research},
  year          = {2023},
  issn          = {2835-8856},
  __markedentry = {[lilli:2]},
  file          = {:Vielhaben2023 - Multi-dimensional concept discovery (MCD)_ A unifying framework with completeness guarantees.pdf:PDF},
  keywords      = {rank5},
  url           = {https://openreview.net/forum?id=KxBQPz7HKh},
}

@Article{Chalupka2015,
  author      = {Krzysztof Chalupka and Pietro Perona and Frederick Eberhardt},
  title       = {Multi-Level Cause-Effect Systems},
  abstract    = {We present a domain-general account of causation that applies to settings in which macro-level causal relations between two systems are of interest, but the relevant causal features are poorly understood and have to be aggregated from vast arrays of micro-measurements. Our approach generalizes that of Chalupka et al. (2015) to the setting in which the macro-level effect is not specified. We formalize the connection between micro- and macro-variables in such situations and provide a coherent framework describing causal relations at multiple levels of analysis. We present an algorithm that discovers macro-variable causes and effects from micro-level measurements obtained from an experiment. We further show how to design experiments to discover macro-variables from observational micro-variable data. Finally, we show that under specific conditions, one can identify multiple levels of causal structure. Throughout the article, we use a simulated neuroscience multi-unit recording experiment to illustrate the ideas and the algorithms.},
  date        = {2015-12-25},
  eprint      = {1512.07942v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Chalupka2015 - Multi-Level Cause-Effect Systems.pdf:PDF},
  keywords    = {stat.ML, cs.AI, rank1},
}

@InProceedings{Bau2017,
  author        = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  title         = {Network Dissection: Quantifying Interpretability of Deep Visual Representations},
  booktitle     = {Computer Vision and Pattern Recognition},
  year          = {2017},
  __markedentry = {[lilli:3]},
  file          = {:Bau2017 - Network Dissection_ Quantifying Interpretability of Deep Visual Representations.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
}

@Article{Ludescher2021,
  author   = {Josef Ludescher and Maria Martin and Niklas Boers and Armin Bunde and Catrin Ciemer and Jingfang Fan and Shlomo Havlin and Marlene Kretschmer and Jürgen Kurths and Jakob Runge and Veronika Stolbova and Elena Surovyatkina and Hans Joachim Schellnhuber},
  title    = {Network-based forecasting of climate phenomena},
  journal  = {Proceedings of the National Academy of Sciences},
  year     = {2021},
  volume   = {118},
  number   = {47},
  pages    = {e1922872118},
  abstract = {Network theory, as emerging from complex systems science, can provide critical predictive power for mitigating the global warming crisis and other societal challenges. Here we discuss the main differences of this approach to classical numerical modeling and highlight several cases where the network approach substantially improved the prediction of high-impact phenomena: 1) El Niño events, 2) droughts in the central Amazon, 3) extreme rainfall in the eastern Central Andes, 4) the Indian summer monsoon, and 5) extreme stratospheric polar vortex states that influence the occurrence of wintertime cold spells in northern Eurasia. In this perspective, we argue that network-based approaches can gainfully complement numerical modeling.},
  doi      = {10.1073/pnas.1922872118},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1922872118},
  keywords = {rank1},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1922872118},
}

@Article{Chattopadhyay2019,
  author        = {Aditya Chattopadhyay and Piyushi Manupriya and Anirban Sarkar and Vineeth N. Balasubramanian},
  title         = {Neural Network Attributions: {A} Causal Perspective},
  journal       = {ArXiv},
  year          = {2019},
  volume        = {abs/1902.02302},
  __markedentry = {[lilli:4]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1902-02302.bib},
  eprint        = {1902.02302},
  eprinttype    = {arXiv},
  file          = {:Chattopadhyay2019 - Neural Network Attributions_ A Causal Perspective.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  timestamp     = {Tue, 02 Aug 2022 09:11:14 +0200},
  url           = {http://arxiv.org/abs/1902.02302},
}

@Article{Popescu2022,
  author   = {Oana Juliana Popescu},
  title    = {Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation},
  journal  = {unpublished},
  year     = {2022},
  file     = {:Popescu2022 - Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables_ A Novel Method and Numerical Evaluation.pdf:PDF;},
  keywords = {rank3},
}

@InProceedings{Yeh2020,
  author    = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan \"{O}. and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
  title     = {On completeness-aware concept-based explanations in deep neural networks},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  year      = {2020},
  series    = {NIPS'20},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  abstract  = {Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose ConceptSHAP. Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable.1},
  articleno = {1726},
  isbn      = {9781713829546},
  location  = {Vancouver, BC, Canada},
  numpages  = {12},
}

@InProceedings{Dominguez-Olmedo2023,
  author    = {Dominguez-Olmedo, Ricardo and Karimi, Amir-Hossein and Arvanitidis, Georgios and Sch\"{o}lkopf, Bernhard},
  title     = {On Data Manifolds Entailed by Structural Causal Models},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  pages     = {8188--8201},
  month     = {23--29 Jul},
  publisher = {PMLR},
  abstract  = {The geometric structure of data is an important inductive bias in machine learning. In this work, we characterize the data manifolds entailed by structural causal models. The strengths of the proposed framework are twofold: firstly, the geometric structure of the data manifolds is causally informed, and secondly, it enables causal reasoning about the data manifolds in an interventional and a counterfactual sense. We showcase the versatility of the proposed framework by applying it to the generation of causally-grounded counterfactual explanations for machine learning classifiers, measuring distances along the data manifold in a differential geometric-principled manner.},
  file      = {:Dominguez-Olmedo2023 - On Data Manifolds Entailed by Structural Causal Models.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank4},
  url       = {https://proceedings.mlr.press/v202/dominguez-olmedo23a.html},
}

@InProceedings{Traeuble2021,
  author    = {Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  title     = {On Disentangled Representations Learned from Correlated Data},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  pages     = {10401--10412},
  month     = {18--24 Jul},
  publisher = {PMLR},
  abstract  = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
  file      = {:Traeuble2021 - On Disentangled Representations Learned from Correlated Data.pdf:PDF;trauble21a.pdf:http\://proceedings.mlr.press/v139/trauble21a/trauble21a.pdf:PDF},
  keywords  = {rank4},
  url       = {https://proceedings.mlr.press/v139/trauble21a.html},
}

@Article{Bach2015,
  author        = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
  title         = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  journal       = {PLOS ONE},
  year          = {2015},
  volume        = {10},
  number        = {7},
  pages         = {1-46},
  month         = {07},
  __markedentry = {[lilli:2]},
  abstract      = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  doi           = {10.1371/journal.pone.0130140},
  file          = {:Bach2015 - On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  publisher     = {Public Library of Science},
  url           = {https://doi.org/10.1371/journal.pone.0130140},
}

@InProceedings{Karimi2023,
  author        = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and Sch\"{o}lkopf, Bernhard and Kim, Been},
  title         = {On the Relationship Between Explanation and Prediction: A Causal View},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  year          = {2023},
  editor        = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume        = {202},
  series        = {Proceedings of Machine Learning Research},
  pages         = {15861--15883},
  month         = {23--29 Jul},
  publisher     = {PMLR},
  __markedentry = {[lilli:4]},
  abstract      = {Being able to provide explanations for a model’s decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between ’ideal’ case only increase in higher-performing models — models that are likely to be deployed. Our work is a promising first step towards providing a quantitative measure of the relationship between E and Y, which could also inform the future development of methods for E with a quantitative metric.},
  file          = {:Karimi2023 - On the Relationship Between Explanation and Prediction_ A Causal View.pdf:PDF;karimi23a.pdf:https\://proceedings.mlr.press/v202/karimi23a/karimi23a.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://proceedings.mlr.press/v202/karimi23a.html},
}

@Misc{Agarwal2023,
  author        = {Chirag Agarwal and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},
  title         = {OpenXAI: Towards a Transparent Evaluation of Model Explanations},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2206.11104},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
}

@InProceedings{Chickering2002,
  author    = {David Maxwell Chickering},
  title     = {Optimal Structure and Identification With Greedy Search},
  booktitle = {Journal of Machine Learning Research 3 (2002) 507-554},
  year      = {2002},
  file      = {:Chickering2002 - Optimal Structure and Identification With Greedy Search.pdf:PDF},
  keywords  = {rank1},
}

@Book{Bishop2006,
  title     = {Pattern Recognition and Machine Learning},
  publisher = {Springer},
  year      = {2006},
  author    = {Bishop, Christopher},
  month     = {January},
  abstract  = {This leading textbook provides a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. This is the first machine learning textbook to include a comprehensive coverage of recent developments such as probabilistic graphical models and deterministic inference methods, and to emphasize a modern Bayesian perspective. It is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. This hard cover book has 738 pages in full colour, and there are 431 graded exercises.

Solutions for these exercises and extensive support for course instructors are provided on Christopher Bishop's page.

Now available to download in full as a PDF.},
  file      = {:Bishop2006 - Pattern Recognition and Machine Learning.pdf:PDF},
  keywords  = {rank1},
  url       = {https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/},
}

@Misc{Wang2017,
  author        = {Yuhao Wang and Liam Solus and Karren Dai Yang and Caroline Uhler},
  title         = {Permutation-based Causal Inference Algorithms with Interventions},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1705.10220},
  keywords      = {rank1},
  primaryclass  = {stat.ME},
}

@InProceedings{Koopman2021,
  author        = {Koopman, Tara and Renooij, Silja},
  title         = {Persuasive Contrastive Explanations for Bayesian Networks},
  booktitle     = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  year          = {2021},
  editor        = {Vejnarov{\'a}, Ji{\v{r}}ina and Wilson, Nic},
  pages         = {229--242},
  address       = {Cham},
  publisher     = {Springer International Publishing},
  __markedentry = {[lilli:1]},
  abstract      = {Explanation in Artificial Intelligence is often focused on providing reasons for why a model under consideration and its outcome are correct. Recently, research in explainable machine learning has initiated a shift in focus on including so-called counterfactual explanations. In this paper we propose to combine both types of explanation in the context of explaining Bayesian networks. To this end we introduce persuasive contrastive explanations that aim to provide an answer to the question Why outcome t instead of {\$}{\$}t^{\backslash}prime ?{\$}{\$}t{\textasciiacutex}?posed by a user. In addition, we propose an algorithm for computing persuasive contrastive explanations. Both our definition of persuasive contrastive explanation and the proposed algorithm can be employed beyond the current scope of Bayesian networks.},
  groups        = {relevant},
  isbn          = {978-3-030-86772-0},
  keywords      = {rank1},
  url           = {https://link.springer.com/chapter/10.1007/978-3-030-86772-0_17},
}

@Article{Gonzalez-Soto2018,
  author      = {M. Gonzalez-Soto and L. E. Sucar and H. J. Escalante},
  title       = {Playing against Nature: causal discovery for decision making under uncertainty},
  abstract    = {We consider decision problems under uncertainty where the options available to a decision maker and the resulting outcome are related through a causal mechanism which is unknown to the decision maker. We ask how a decision maker can learn about this causal mechanism through sequential decision making as well as using current causal knowledge inside each round in order to make better choices had she not considered causal knowledge and propose a decision making procedure in which an agent holds \textit{beliefs} about her environment which are used to make a choice and are updated using the observed outcome. As proof of concept, we present an implementation of this causal decision making model and apply it in a simple scenario. We show that the model achieves a performance similar to the classic Q-learning while it also acquires a causal model of the environment.},
  date        = {2018-07-03},
  eprint      = {1807.01268v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Gonzalez-Soto2018 - Playing against Nature_ causal discovery for decision making under uncertainty.pdf:PDF},
  groups      = {Application Fields, relevant},
  keywords    = {cs.AI, rank3},
}

@Article{Bluecher2022,
  author        = {Blücher, Stefan and Vielhaben, Johanna and Strodthoff, Nils},
  title         = {PredDiff: Explanations and interactions from conditional expectations},
  journal       = {Artificial Intelligence},
  year          = {2022},
  volume        = {312},
  pages         = {103774},
  month         = nov,
  issn          = {0004-3702},
  __markedentry = {[lilli:1]},
  doi           = {10.1016/j.artint.2022.103774},
  file          = {:Bluecher2022 - PredDiff_ Explanations and interactions from conditional expectations.pdf:PDF},
  keywords      = {rank5},
  publisher     = {Elsevier BV},
}

@Misc{Unterthiner2021,
  author        = {Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya Tolstikhin},
  title         = {Predicting Neural Network Accuracy from Weights},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2002.11448},
  file          = {:Unterthiner2021 - Predicting Neural Network Accuracy from Weights.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {stat.ML},
}

@InProceedings{Yeom2019,
  author        = {Seul{-}Ki Yeom and Philipp Seegerer and Sebastian Lapuschkin and Simon Wiedemann and Klaus{-}Robert M{\"{u}}ller and Wojciech Samek},
  title         = {Pruning by Explaining: {A} Novel Criterion for Deep Neural Network Pruning},
  year          = {2019},
  volume        = {abs/1912.08881},
  __markedentry = {[lilli:1]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1912-08881.bib},
  eprint        = {1912.08881},
  eprinttype    = {arXiv},
  file          = {:Yeom2019 - Pruning by Explaining_ A Novel Criterion for Deep Neural Network Pruning.pdf:PDF},
  groups        = {relevant},
  journal       = {ArXiv},
  keywords      = {Pruning, Layer-wise Relevance Propagation (LRP), Convolutional Neural Network (CNN), Interpretation of Models, Explainable AI (XAI), rank4},
  timestamp     = {Sat, 23 Jan 2021 01:12:43 +0100},
  url           = {http://arxiv.org/abs/1912.08881},
}

@Article{Saggioro2020,
  author   = {Saggioro,Elena and de Wiljes,Jana and Kretschmer,Marlene and Runge,Jakob},
  title    = {Reconstructing regime-dependent causal relationships from observational time series},
  journal  = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  year     = {2020},
  volume   = {30},
  number   = {11},
  pages    = {113115},
  doi      = {10.1063/5.0020538},
  eprint   = {https://doi.org/10.1063/5.0020538},
  groups   = {relevant},
  keywords = {rank2},
  url      = { 
        https://doi.org/10.1063/5.0020538
    
},
}

@Article{Bareinboim2015,
  author       = {Bareinboim, Elias and Tian, Jin},
  title        = {Recovering Causal Effects from Selection Bias},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year         = {2015},
  volume       = {29},
  number       = {1},
  month        = {Mar.},
  abstractnote = { &lt;p&gt; Controlling for selection and confounding biases are two of the most challenging problems that appear in data analysis in the empirical sciences as well as in artificial intelligence tasks. The combination of previously studied methods for each of these biases in isolation is not directly applicable to certain non-trivial cases in which selection and confounding biases are simultaneously present. In this paper, we tackle these instances non-parametrically and in full generality. We provide graphical and algorithmic conditions for recoverability of interventional distributions for when selection and confounding biases are both present. Our treatment completely characterizes the class of causal effects that are recoverable in Markovian models, and is suffi- cient for Semi-Markovian models. &lt;/p&gt; },
  doi          = {10.1609/aaai.v29i1.9679},
  keywords     = {rank1},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/9679},
}

@InProceedings{Goyal2021,
  author        = {Goyal, A. and Lamb, A. and Hoffmann, J. and Sodhani, S. and Levine, S. and Bengio, Y. and Sch{\"o}lkopf, B.},
  title         = {Recurrent Independent Mechanisms},
  booktitle     = {9th International Conference on Learning Representations (ICLR)},
  year          = {2021},
  month         = may,
  keywords      = {rank1},
  month_numeric = {5},
  url           = {https://www.semanticscholar.org/reader/67a9dde04f367efc903b6d06097df9bdd9887ae7},
}

@Article{Zecevic2021,
  author     = {Matej Zecevic and Devendra Singh Dhami and Petar Velickovic and Kristian Kersting},
  title      = {Relating Graph Neural Networks to Structural Causal Models},
  journal    = {CoRR},
  year       = {2021},
  volume     = {abs/2109.04173},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2109-04173.bib},
  eprint     = {2109.04173},
  eprinttype = {arXiv},
  file       = {:Zecevic2021 - Relating Graph Neural Networks to Structural Causal Models.pdf:PDF},
  keywords   = {rank1},
  timestamp  = {Tue, 21 Sep 2021 17:46:04 +0200},
  url        = {https://arxiv.org/abs/2109.04173},
}

@Article{Wang2016,
  author     = {Zhiguang Wang and Wei Song and Lu Liu and Fan Zhang and Junxiao Xue and Yangdong Ye and Ming Fan and Mingliang Xu},
  title      = {Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization},
  journal    = {CoRR},
  year       = {2016},
  volume     = {abs/1610.07258},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/WangSLZXYFX16.bib},
  eprint     = {1610.07258},
  eprinttype = {arXiv},
  keywords   = {rank1},
  timestamp  = {Mon, 13 Aug 2018 16:46:07 +0200},
  url        = {http://arxiv.org/abs/1610.07258},
}

@Misc{Schulz2020,
  author        = {Karl Schulz and Leon Sixt and Federico Tombari and Tim Landgraf},
  title         = {Restricting the Flow: Information Bottlenecks for Attribution},
  year          = {2020},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2001.00396},
  file          = {:Schulz2020 - Restricting the Flow_ Information Bottlenecks for Attribution.pdf:PDF},
  groups        = {relevant},
  journal       = {ICLR2020},
  keywords      = {rank5},
  primaryclass  = {stat.ML},
}

@InProceedings{Srinivas2021,
  author    = {Suraj Srinivas and Francois Fleuret},
  title     = {Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=dYeAHXnpWJ4},
}

@Misc{Pahde2023,
  author        = {Frederik Pahde and Maximilian Dreyer and Wojciech Samek and Sebastian Lapuschkin},
  title         = {Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models},
  year          = {2023},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2303.12641},
  file          = {:Pahde2023 - Reveal to Revise_ An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.CV},
}

@InProceedings{Dreyer2023,
  author        = {Dreyer, Maximilian and Achtibat, Reduan and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
  title         = {Revealing Hidden Context Bias in Segmentation and Object Detection Through Concept-Specific Explanations},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year          = {2023},
  pages         = {3828-3838},
  month         = {June},
  __markedentry = {[lilli:2]},
  file          = {:Dreyer2023 - Revealing Hidden Context Bias in Segmentation and Object Detection Through Concept-Specific Explanations.pdf:PDF},
  keywords      = {rank5},
}

@Article{Glymour2019,
  author   = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
  title    = {Review of Causal Discovery Methods Based on Graphical Models},
  journal  = {Frontiers in Genetics},
  year     = {2019},
  volume   = {10},
  issn     = {1664-8021},
  abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
  doi      = {10.3389/fgene.2019.00524},
  file     = {:Glymour2019 - Review of Causal Discovery Methods Based on Graphical Models.pdf:PDF},
  groups   = {relevant},
  keywords = {rank3},
  url      = {https://www.frontiersin.org/articles/10.3389/fgene.2019.00524},
}

@InProceedings{Ross2017,
  author    = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
  title     = {Right for the Right Reasons: Training Differentiable Models by Constraining Their Explanations},
  booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  year      = {2017},
  series    = {IJCAI'17},
  pages     = {2662–2670},
  publisher = {AAAI Press},
  abstract  = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classifiers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
  isbn      = {9780999241103},
  keywords  = {rank1},
  location  = {Melbourne, Australia},
  numpages  = {9},
  url       = {https://dl.acm.org/doi/10.5555/3172077.3172259},
}

@Article{Wang2021,
  author     = {Zifan Wang and Matt Fredrikson and Anupam Datta},
  title      = {Robust Models Are More Interpretable Because Attributions Look Normal},
  journal    = {CoRR},
  year       = {2021},
  volume     = {abs/2103.11257},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-11257.bib},
  eprint     = {2103.11257},
  eprinttype = {arXiv},
  timestamp  = {Tue, 06 Apr 2021 15:08:59 +0200},
  url        = {https://arxiv.org/abs/2103.11257},
}

@InProceedings{Singla2022,
  author        = {Sahil Singla and Soheil Feizi},
  title         = {Salient Image Net: How to Discover Spurious Features in Deep Learning?},
  booktitle     = {ICLR},
  year          = {2022},
  __markedentry = {[lilli:3]},
  abstract      = {Deep neural networks can be unreliable in the real world especially when they
heavily use spurious features for their predictions. Focusing on image classifi-
cations, we define core features as the set of visual features that are always a
part of the object definition while spurious features are the ones that are likely
to co-occur with the object but not a part of it (e.g., attribute “fingers” for class
“band aid”). Traditional methods for discovering spurious features either require
extensive human annotations (thus, not scalable), or are useful on specific models.
In this work, we introduce a general framework to discover a subset of spurious
and core visual features used in inferences of a general model and localize them
on a large number of images with minimal human supervision. Our methodol-
ogy is based on this key idea: to identify spurious or core visual features used
in model predictions, we identify spurious or core neural features (penultimate
layer neurons of a robust model) via limited human supervision (e.g., using top
5 activating images per feature). We then show that these neural feature anno-
tations generalize extremely well to many more images without any human su-
pervision. We use the activation maps for these neural features as the soft masks
to highlight spurious or core visual features. Using this methodology, we intro-
duce the Salient Imagenet dataset containing core and spurious masks for a large
set of samples from Imagenet. Using this dataset, we show that several popular
Imagenet models rely heavily on various spurious features in their predictions, in-
dicating the standard accuracy alone is not sufficient to fully assess model performance. Code and dataset for reproducing all experiments in the paper is available
at https://github.com/singlasahil14/salient_imagenet.},
  file          = {:Singla2022 - Salient Image Net_ How to Discover Spurious Features in Deep Learning_.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
}

@InProceedings{Adebayo2018,
  author        = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  title         = {Sanity Checks for Saliency Maps},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2018},
  editor        = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  volume        = {31},
  publisher     = {Curran Associates, Inc.},
  __markedentry = {[lilli:2]},
  file          = {:Adebayo2018 - Sanity Checks for Saliency Maps.pdf:PDF},
  keywords      = {rank5},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf},
}

@Article{Wilming2022,
  author  = {Wilming, Rick and Budding, Céline and Müller, Klaus-Robert and Haufe, Stefan},
  title   = {Scrutinizing XAI using linear ground-truth data with suppressor variables},
  journal = {Machine Learning},
  year    = {2022},
  volume  = {111},
  pages   = {1-21},
  month   = {05},
  doi     = {10.1007/s10994-022-06167-y},
}

@Article{SaranyaGanesh2023,
  author   = {Saranya Ganesh S. and Tom Beucler and Frederick Iat-Hin Tam and Milton S. Gomez and Jakob Runge and Andreas Gerhardus},
  title    = {Selecting Robust Features for Machine Learning Applications Using Multidata Causal Discovery},
  year     = {2023},
  file     = {:SaranyaGanesh2023 - Selecting Robust Features for Machine Learning Applications Using Multidata Causal Discovery.pdf:PDF},
  groups   = {Recommended},
  keywords = {rank1},
}

@Misc{Smilkov2017,
  author        = {Daniel Smilkov and Nikhil Thorat and Been Kim and Fernanda Viégas and Martin Wattenberg},
  title         = {SmoothGrad: removing noise by adding noise},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1706.03825},
  primaryclass  = {cs.LG},
}

@Article{Winkelmann2022,
  author   = {Ricarda Winkelmann and Jonathan F. Donges and E. Keith Smith and Manjana Milkoreit and Christina Eder and Jobst Heitzig and Alexia Katsanidou and Marc Wiedermann and Nico Wunderling and Timothy M. Lenton},
  title    = {Social tipping processes towards climate action: A conceptual framework},
  journal  = {Ecological Economics},
  year     = {2022},
  volume   = {192},
  pages    = {107242},
  issn     = {0921-8009},
  abstract = {Societal transformations are necessary to address critical global challenges, such as mitigation of anthropogenic climate change and reaching UN sustainable development goals. Recently, social tipping processes have received increased attention, as they present a form of social change whereby a small change can shift a sensitive social system into a qualitatively different state due to strongly self-amplifying (mathematically positive) feedback mechanisms. Social tipping processes with respect to technological and energy systems, political mobilization, financial markets and sociocultural norms and behaviors have been suggested as potential key drivers towards climate action. Drawing from expert insights and comprehensive literature review, we develop a framework to identify and characterize social tipping processes critical to facilitating rapid social transformations. We find that social tipping processes are distinguishable from those of already more widely studied climate and ecological tipping dynamics. In particular, we identify human agency, social-institutional network structures, different spatial and temporal scales and increased complexity as key distinctive features underlying social tipping processes. Building on these characteristics, we propose a formal definition for social tipping processes and filtering criteria for those processes that could be decisive for future trajectories towards climate action. We illustrate this definition with the European political system as an example of potential social tipping processes, highlighting the prospective role of the FridaysForFuture movement. Accordingly, this conceptual framework for social tipping processes can be utilized to illuminate mechanisms for necessary transformative climate change mitigation policies and actions.},
  doi      = {https://doi.org/10.1016/j.ecolecon.2021.107242},
  file     = {:Winkelmann2022 - Social tipping processes towards climate action_ A conceptual framework.pdf:PDF},
  groups   = {Application Fields, socialclimate},
  keywords = {Social tipping dynamics, Social change, Sustainability, Critical states, Network structures, FridaysForFuture, rank1},
  url      = {https://www.sciencedirect.com/science/article/pii/S0921800921003013},
}

@Article{Anders2021,
  author  = {Anders, Christopher J. and Neumann, David and Samek, Wojciech and Müller, Klaus-Robert and Lapuschkin, Sebastian},
  title   = {Software for Dataset-wide XAI: From Local Explanations to Global Insights with {Zennit}, {CoRelAy}, and {ViRelAy}},
  journal = {CoRR},
  year    = {2021},
  volume  = {abs/2106.13200},
}

@Misc{Anders2023,
  author        = {Christopher J. Anders and David Neumann and Wojciech Samek and Klaus-Robert Müller and Sebastian Lapuschkin},
  title         = {Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2106.13200},
  file          = {:Anders2023 - Software for Dataset-wide XAI_ From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy.pdf:PDF},
  primaryclass  = {cs.LG},
}

@Misc{Vielhaben2022,
  author        = {Johanna Vielhaben and Stefan Bl{\"u}cher and Nils Strodthoff},
  title         = {Sparse Subspace Clustering for Concept Discovery (SSCCD)},
  year          = {2022},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2203.06043},
  file          = {:Vielhaben2022 - Sparse Subspace Clustering for Concept Discovery (SSCCD).pdf:PDF},
  groups        = {relevant},
  journal       = {ArXiv},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2203.06043},
}

@InBook{Tibau2021,
  chapter   = {13},
  pages     = {186-203},
  title     = {Spatio-temporal Autoencoders in Weather and Climate Research},
  publisher = {John Wiley \& Sons, Ltd},
  year      = {2021},
  author    = {Tibau, Xavier-Andoni and Reimers, Christian and Requena-Mesa, Christian and Runge, Jakob},
  isbn      = {9781119646181},
  abstract  = {Summary The need to understand and predict weather and climate has always been a concern of humanity. A main current challenge derives from the Earth's complex nonlinear dynamics, high dimensionality of observational as well as model datasets, and the property that the relevant processes often are emergent dynamical phenomena that are challenging to extract from the data. Deep learning bears the promise to be well suited for such challenges, but the absence of enough labeled datasets for supervised learning and the particularities of Earth science datasets make it difficult to directly use off-the-shelf deep learning methods. In this context, autoencoders are a promising tool because they extract features and patterns from complex high-dimensional data without the need for labels. This chapter presents an overlook of deep autoencoders and their use in weather and climate research. We cover the basic theoretical foundations, selected applications, and review recent research and future directions.},
  booktitle = {Deep Learning for the Earth Sciences},
  doi       = {https://doi.org/10.1002/9781119646181.ch13},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119646181.ch13},
  file      = {:Tibau2021 - Spatio-temporal Autoencoders in Weather and Climate Research.pdf:PDF},
  groups    = {relevant},
  keywords  = {climate research, deep learning, earth data challenges, spatio-temporal nature, variational autoencoders, weather research, rank3},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119646181.ch13},
}

@InProceedings{Maxwell2020,
  author   = {David Maxwell and Chickering and Microsoft Research},
  title    = {Statistically Efficient Greedy Equivalence Search},
  year     = {2020},
  abstract = {Meek (2015) solved this problem by introducing Selec-
tive Greedy Equivalence Search (SGES), which is an im-},
  file     = {:Maxwell2020 - Statistically Efficient Greedy Equivalence Search.pdf:PDF},
  keywords = {rank1},
}

@Article{Hohman2020,
  author        = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Polo Chau, Duen Horng},
  title         = {Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  year          = {2020},
  volume        = {26},
  number        = {1},
  pages         = {1096-1106},
  __markedentry = {[lilli:1]},
  doi           = {10.1109/TVCG.2019.2934659},
  file          = {:Hohman2020 - Summit_ Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.pdf:PDF},
  keywords      = {rank3},
}

@Misc{Tibau2018,
  author       = {Xavier-Andoni Tibau and Christian Requena-Mesa and Christian Reimers and Joachim Denzler and Veronika Eyring and Markus Reichstein and Jakob Runge},
  title        = {SupernoVAE: Using deep learning to find spatio-temporal dynamics in Earth system data},
  howpublished = {American Geophysical Union Fall Meeting (AGU): Abstract + Poster Presentation},
  year         = {2018},
  abstract     = {Exploring and understanding spatio-temporal patterns on Earth system datasets is one of the principal goals of the climate and geo-science communities. In this direction, Empirical Orthogonal Functions (EOFs) have been used to characterize phenomena such as the El Nino Southern Oscillation, the Arctic jet stream or the Indian Monsoon. However, EOF analysis has several limitations, for example, it can only identify linear and orthogonal patterns.  We present a framework that makes use of a convolutional variational autoencoder (VAE) as a learnable feature function to extract spatio-temporal dynamics via PCA. The VAE encodes the information in an abstract space of higher order features representing different patterns. Over this space, PCA is performed to obtain a spatial representation of related temporal dynamics.  We have used three datasets, two artificial datasets where the dynamics are ruled by a hidden spatially varying parameter and an observational reanalysis dataset of monthly sea surface temperature from 1898 to 2014. The artificial datasets have chaotic and, chaotic and stochastic dynamics depending on the spatial hidden parameter. As baseline methods, EOF analysis and Kernel PCA were performed over the original spaces.  For the two artificial datasets, we found a high correlation between some of the first Principal Components on the feature space and the spatial hidden parameter. This correlation was not found using baseline methods in the original space. In the reanalysis dataset, the method was able to find known modes, such as ENSO, as well as other patterns that baseline methods did not reveal that might have inmmediate effect on how we understand the earth system after expert interpretation.  These results provide a proof of concept: SupernoVAE is not only able to extract well-known climate patterns previously characterized with linear EOF analysis, but also allows to extract non-linear and non-orthogonal patterns that can help in analyzing Earth system dynamics that could not be characterized before.},
  key          = {tibau2018supernovae2},
  keywords     = {rank1},
  type         = {misc},
  url          = {https://agu.confex.com/agu/fm18/meetingapp.cgi/Paper/403663},
}

@Article{Nguyen2016,
  author     = {Anh Mai Nguyen and Alexey Dosovitskiy and Jason Yosinski and Thomas Brox and Jeff Clune},
  title      = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
  journal    = {CoRR},
  year       = {2016},
  volume     = {abs/1605.09304},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/NguyenDYBC16.bib},
  eprint     = {1605.09304},
  eprinttype = {arXiv},
  timestamp  = {Mon, 13 Aug 2018 16:46:53 +0200},
  url        = {http://arxiv.org/abs/1605.09304},
}

@Article{Fulton2021,
  author    = {D. James Fulton and Gabriele C. Hegerl},
  title     = {Testing Methods of Pattern Extraction for Climate Data Using Synthetic Modes},
  journal   = {Journal of Climate},
  year      = {2021},
  volume    = {34},
  number    = {18},
  pages     = {7645 - 7660},
  address   = {Boston MA, USA},
  doi       = {https://doi.org/10.1175/JCLI-D-20-0871.1},
  keywords  = {rank1},
  publisher = {American Meteorological Society},
  url       = {https://journals.ametsoc.org/view/journals/clim/34/18/JCLI-D-20-0871.1.xml},
}

@InProceedings{Kindermans2019,
  author        = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T. and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  title         = {The (Un)reliability of Saliency Methods},
  booktitle     = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  year          = {2019},
  editor        = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  pages         = {267--280},
  address       = {Cham},
  publisher     = {Springer International Publishing},
  __markedentry = {[lilli:2]},
  abstract      = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily---adding a constant shift to the input data---to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will embrace the development of additional tests.},
  doi           = {10.1007/978-3-030-28954-6_14},
  file          = {:Kindermans2019 - The (Un)reliability of Saliency Methods.pdf:PDF},
  isbn          = {978-3-030-28954-6},
  url           = {https://doi.org/10.1007/978-3-030-28954-6_14},
}

@Article{Schaefer2015,
  author     = {Sch\"{a}fer, Patrick},
  title      = {The BOSS is Concerned with Time Series Classification in the Presence of Noise},
  journal    = {Data Min. Knowl. Discov.},
  year       = {2015},
  volume     = {29},
  number     = {6},
  pages      = {1505–1530},
  month      = {nov},
  issn       = {1384-5810},
  abstract   = {Similarity search is one of the most important and probably best studied methods for data mining. In the context of time series analysis it reaches its limits when it comes to mining raw datasets. The raw time series data may be recorded at variable lengths, be noisy, or are composed of repetitive substructures. These build a foundation for state of the art search algorithms. However, noise has been paid surprisingly little attention to and is assumed to be filtered as part of a preprocessing step carried out by a human. Our Bag-of-SFA-Symbols (BOSS) model combines the extraction of substructures with the tolerance to extraneous and erroneous data using a noise reducing representation of the time series. We show that our BOSS ensemble classifier improves the best published classification accuracies in diverse application areas and on the official UCR classification benchmark datasets by a large margin.},
  address    = {USA},
  doi        = {10.1007/s10618-014-0377-7},
  issue_date = {November 2015},
  keywords   = {Noise, Time series, Classification, Fourier transform, Similarity, rank1},
  numpages   = {26},
  publisher  = {Kluwer Academic Publishers},
  url        = {https://doi.org/10.1007/s10618-014-0377-7},
}

@Article{Xia2021,
  author        = {Kevin Xia and Kai{-}Zhan Lee and Yoshua Bengio and Elias Bareinboim},
  title         = {The Causal-Neural Connection: Expressiveness, Learnability, and Inference},
  journal       = {ArXiv},
  year          = {2021},
  volume        = {abs/2107.00793},
  __markedentry = {[lilli:1]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2107-00793.bib},
  eprint        = {2107.00793},
  eprinttype    = {arXiv},
  file          = {:Xia2021 - The Causal-Neural Connection_ Expressiveness, Learnability, and Inference.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  timestamp     = {Tue, 20 Jul 2021 08:32:19 +0200},
  url           = {https://arxiv.org/abs/2107.00793},
}

@Article{Xing2023,
  author   = {Xing, Jin and Sieber, Renee},
  title    = {The challenges of integrating explainable artificial intelligence into GeoAI},
  journal  = {Transactions in GIS},
  year     = {2023},
  volume   = {n/a},
  number   = {n/a},
  abstract = {Abstract Although explainable artificial intelligence (XAI) promises considerable progress in glassboxing deep learning models, there are challenges in applying XAI to geospatial artificial intelligence (GeoAI), specifically geospatial deep neural networks (DNNs). We summarize these as three major challenges, related generally to XAI computation, to GeoAI and geographic data handling, and to geosocial issues. XAI computation includes the difficulty of selecting reference data/models and the shortcomings of attributing explanatory power to gradients, as well as the difficulty in accommodating geographic scale, geovisualization, and underlying geographic data structures. Geosocial challenges encompass the limitations of knowledge scope—semantics and ontologies—in the explanation of GeoAI as well as the lack of integrating non-technical aspects in XAI, including processes that are not amenable to XAI. We illustrate these issues with a land use classification case study.},
  doi      = {https://doi.org/10.1111/tgis.13045},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.13045},
  keywords = {rank1},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.13045},
}

@Article{Lipton2018,
  author     = {Lipton, Zachary C.},
  title      = {The mythos of model interpretability},
  journal    = {Commun. ACM},
  year       = {2018},
  volume     = {61},
  number     = {10},
  pages      = {36–43},
  month      = {sep},
  issn       = {0001-0782},
  abstract   = {In machine learning, the concept of interpretability is both important and slippery.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3233231},
  issue_date = {October 2018},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3233231},
}

@Article{Pearl2019,
  author        = {Pearl, Judea},
  title         = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  journal       = {Commun. ACM},
  year          = {2019},
  volume        = {62},
  number        = {3},
  pages         = {54–60},
  month         = {feb},
  issn          = {0001-0782},
  __markedentry = {[lilli:1]},
  abstract      = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.},
  address       = {New York, NY, USA},
  doi           = {10.1145/3241036},
  groups        = {relevant},
  issue_date    = {March 2019},
  keywords      = {rank2},
  numpages      = {7},
  publisher     = {Association for Computing Machinery},
  url           = {https://doi.org/10.1145/3241036},
}

@InProceedings{Wilming2023,
  author        = {Wilming, Rick and Kieslich, Leo and Clark, Benedict and Haufe, Stefan},
  title         = {Theoretical Behavior of {XAI} Methods in the Presence of Suppressor Variables},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  year          = {2023},
  editor        = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume        = {202},
  series        = {Proceedings of Machine Learning Research},
  pages         = {37091--37107},
  month         = {23--29 Jul},
  publisher     = {PMLR},
  __markedentry = {[lilli:4]},
  abstract      = {In recent years, the community of ’explainable artificial intelligence’ (XAI) has created a vast body of methods to bridge a perceived gap between model ’complexity’ and ’interpretability’. However, a concrete problem to be solved by XAI methods has not yet been formally stated. As a result, XAI methods are lacking theoretical and empirical evidence for the ’correctness’ of their explanations, limiting their potential use for quality-control and transparency purposes. At the same time, Haufe et al. (2014) showed, using simple toy examples, that even standard interpretations of linear models can be highly misleading. Specifically, high importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target. This behavior has been confirmed empirically for a large array of XAI methods in Wilming et al. (2022). Here, we go one step further by deriving analytical expressions for the behavior of a variety of popular XAI methods on a simple two-dimensional binary classification problem involving Gaussian class-conditional distributions. We show that the majority of the studied approaches will attribute non-zero importance to a non-class-related suppressor feature in the presence of correlated noise. This poses important limitations on the interpretations and conclusions that the outputs of these XAI methods can afford.},
  file          = {:Wilming2023 - Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables.pdf:PDF;wilming23a.pdf:https\://proceedings.mlr.press/v202/wilming23a/wilming23a.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
  url           = {https://proceedings.mlr.press/v202/wilming23a.html},
}

@Book{Neumann1944,
  title     = {Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)},
  publisher = {Princeton University Press},
  year      = {1944},
  author    = {John von Neumann and Oskar Morgenstern and Ariel Rubinstein},
  isbn      = {9780691130613},
  abstract  = {This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press publishedTheory of Games and Economic Behavior. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences.This sixtieth anniversary edition includes not only the original text but also an introduction by Harold Kuhn, an afterword by Ariel Rubinstein, and reviews and articles on the book that appeared at the time of its original publication in theNew York Times, ttheAmerican Economic Review, and a variety of other publications. Together, these writings provide readers a matchless opportunity to more fully appreciate a work whose influence will yet resound for generations to come.},
  file      = {:Neumann1944 - Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition).pdf:PDF},
  groups    = {Application Fields, socialclimate},
  keywords  = {rank1},
  url       = {http://www.jstor.org/stable/j.ctt1r2gkx},
}

@Article{Lines2018,
  author     = {Lines, Jason and Taylor, Sarah and Bagnall, Anthony},
  title      = {Time Series Classification with HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles},
  journal    = {ACM Trans. Knowl. Discov. Data},
  year       = {2018},
  volume     = {12},
  number     = {5},
  month      = {jul},
  issn       = {1556-4681},
  abstract   = {A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC.We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.},
  address    = {New York, NY, USA},
  articleno  = {52},
  doi        = {10.1145/3182382},
  issue_date = {October 2018},
  keywords   = {Time series classification, meta ensembles, deep learning, heterogeneous ensembles, rank1},
  numpages   = {35},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3182382},
}

@Article{Bagnall2015,
  author   = {Bagnall, Anthony and Lines, Jason and Hills, Jon and Bostrom, Aaron},
  title    = {Time-Series Classification with COTE: The Collective of Transformation-Based Ensembles},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  year     = {2015},
  volume   = {27},
  number   = {9},
  pages    = {2522-2535},
  doi      = {10.1109/TKDE.2015.2416723},
  keywords = {rank1},
}

@InProceedings{Zhang2016,
  author    = {Zhang, Jianming and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  title     = {Top-Down Neural Attention by Excitation Backprop},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {543--559},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images.},
  isbn      = {978-3-319-46493-0},
}

@Article{Schoelkopf2021,
  author    = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  title     = {Toward Causal Representation Learning},
  journal   = {Proceedings of the IEEE},
  year      = {2021},
  volume    = {109},
  number    = {5},
  pages     = {612-634},
  abstract  = {The two fields of machine learning and graphical
causality arose and developed separately. However, there is now
cross-pollination and increasing interest in both fields to benefit
from the advances of the other. In the present paper, we review
fundamental concepts of causal inference and relate them to
crucial open problems of machine learning, including transfer
and generalization, thereby assaying how causality can contribute
to modern machine learning research. This also applies in the
opposite direction: we note that most work in causality starts
from the premise that the causal variables are given. A central
problem for AI and causality is, thus, causal representation
learning, the discovery of high-level causal variables from low-
level observations. Finally, we delineate some implications of
causality for machine learning and propose key research areas
at the intersection of both communities},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.1109/JPROC.2021.3058954},
  file      = {:Schoelkopf2021 - Towards Causal Representation Learning.pdf:PDF},
  groups    = {relevant},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, rank4},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2102.11107},
}

@InProceedings{Lopez-Paz2015,
  author    = {David Lopez-Paz and Krikamol Muandet and Bernhard Sch{\"o}lkopf and Ilya O. Tolstikhin},
  title     = {Towards a Learning Theory of Cause-Effect Inference},
  booktitle = {International Conference on Machine Learning},
  year      = {2015},
  file      = {:Lopez-Paz2015 - Towards a Learning Theory of Cause-Effect Inference.pdf:PDF},
  keywords  = {rank1},
}

@InBook{Ghorbani2019,
  title         = {Towards automatic concept-based explanations},
  publisher     = {Curran Associates Inc.},
  year          = {2019},
  author        = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
  address       = {Red Hook, NY, USA},
  __markedentry = {[lilli:2]},
  abstract      = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
  articleno     = {832},
  booktitle     = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  file          = {:Ghorbani2019 - Towards Automatic Concept-Based Explanations.pdf:PDF},
  keywords      = {rank5},
  numpages      = {10},
}

@InProceedings{Kohlbrenner2020,
  author        = {Kohlbrenner, Maximilian and Bauer, Alexander and Nakajima, Shinichi and Binder, Alexander and Samek, Wojciech and Lapuschkin, Sebastian},
  title         = {Towards Best Practice in Explaining Neural Network Decisions with LRP},
  booktitle     = {2020 International Joint Conference on Neural Networks (IJCNN)},
  year          = {2020},
  pages         = {1-7},
  __markedentry = {[lilli:2]},
  doi           = {10.1109/IJCNN48605.2020.9206975},
  file          = {:Kohlbrenner2020 - Towards Best Practice in Explaining Neural Network Decisions with LRP.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank4},
}

@Article{Krarup2020,
  author     = {Benjamin Krarup and Senka Krivic and Felix Lindner and Derek Long},
  title      = {Towards Contrastive Explanations for Comparing the Ethics of Plans},
  journal    = {CoRR},
  year       = {2020},
  volume     = {abs/2006.12632},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2006-12632.bib},
  eprint     = {2006.12632},
  eprinttype = {arXiv},
  keywords   = {rank1},
  timestamp  = {Thu, 14 Oct 2021 09:14:19 +0200},
  url        = {https://arxiv.org/abs/2006.12632},
}

@Misc{Rong2023,
  author        = {Yao Rong and Tobias Leemann and Thai-trang Nguyen and Lisa Fiedler and Peizhu Qian and Vaibhav Unhelkar and Tina Seidel and Gjergji Kasneci and Enkelejda Kasneci},
  title         = {Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations},
  year          = {2023},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2210.11584},
  file          = {:Rong2023 - Towards Human-centered Explainable AI_ A Survey of User Studies for Model Explanations.pdf:PDF},
  keywords      = {rank5},
  primaryclass  = {cs.AI},
}

@Article{Reimers2021b,
  author        = {Christian Reimers and Paul Bodesheim and Jakob Runge and Joachim Denzler},
  title         = {Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing},
  journal       = {ArXiv},
  year          = {2021},
  volume        = {abs/2103.06179},
  __markedentry = {[lilli:1]},
  abstract      = {Bias in classifiers is a severe issue of modern deep learning methods, especially for their application
in safety- and security-critical areas. Often, the bias of a classifier is a direct consequence of a bias in
the training dataset, frequently caused by the co-occurrence of relevant features and irrelevant ones.
To mitigate this issue, we require learning algorithms that prevent the propagation of bias from
the dataset into the classifier. We present a novel adversarial debiasing method, which addresses a
feature that is spuriously connected to the labels of training images but statistically independent of
the labels for test images. Thus, the automatic identification of relevant features during training is
perturbed by irrelevant features. This is the case in a wide range of bias-related problems for many
computer vision tasks, such as automatic skin cancer detection or driver assistance. We argue by
a mathematical proof that our approach is superior to existing techniques for the abovementioned
bias. Our experiments show that our approach performs better than state-of-the-art techniques on a
well-known benchmark dataset with real-world images of cats and dogs.},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2103-06179.bib},
  eprint        = {2103.06179},
  eprinttype    = {arXiv},
  file          = {:Reimers2021b - Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  timestamp     = {Tue, 16 Mar 2021 11:26:59 +0100},
  url           = {https://arxiv.org/abs/2103.06179},
}

@Article{Mueller-Hansen2017,
  author   = {M\"uller-Hansen, F. and Schl\"uter, M. and M\"as, M. and Donges, J. F. and Kolb, J. J. and Thonicke, K. and Heitzig, J.},
  title    = {Towards representing human behavior and decision making in Earth system models -- an overview of techniques and approaches},
  journal  = {Earth System Dynamics},
  year     = {2017},
  volume   = {8},
  number   = {4},
  pages    = {977--1007},
  doi      = {10.5194/esd-8-977-2017},
  file     = {:Mueller-Hansen2017 - Towards representing human behavior and decision making in Earth system models -- an overview of techniques and approaches.pdf:PDF},
  groups   = {Application Fields, socialclimate},
  keywords = {rank1},
  url      = {https://esd.copernicus.org/articles/8/977/2017/},
}

@Article{Dombrowski2022,
  author        = {Ann-Kathrin Dombrowski and Christopher J. Anders and Klaus-Robert Müller and Pan Kessel},
  title         = {Towards robust explanations for deep neural networks},
  journal       = {Pattern Recognition},
  year          = {2022},
  volume        = {121},
  pages         = {108194},
  issn          = {0031-3203},
  __markedentry = {[lilli:1]},
  abstract      = {Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.},
  doi           = {https://doi.org/10.1016/j.patcog.2021.108194},
  groups        = {relevant},
  keywords      = {Explanation method, Saliency map, Adversarial attacks, Manipulation, Neural networks, rank3},
  url           = {https://www.sciencedirect.com/science/article/pii/S0031320321003769},
}

@Misc{Zhang2023,
  author        = {Wenbo Zhang and Tong Wu and Yunlong Wang and Yong Cai and Hengrui Cai},
  title         = {Towards Trustworthy Explanation: On Causal Rationalization},
  year          = {2023},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2306.14115},
  file          = {:Zhang2023 - Towards Trustworthy Explanation_ On Causal Rationalization.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {cs.LG},
}

@Misc{Ahmed2022,
  author        = {Sabeen Ahmed and Ian E. Nielsen and Aakash Tripathi and Shamoon Siddiqui and Ghulam Rasool and Ravi P. Ramachandran},
  title         = {Transformers in Time-series Analysis: A Tutorial},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.01138},
  file          = {:Ahmed2022 - Transformers in Time-series Analysis_ A Tutorial.pdf:PDF},
  keywords      = {rank1},
  primaryclass  = {cs.LG},
}

@Misc{Singla2021,
  author        = {Sahil Singla and Besmira Nushi and Shital Shah and Ece Kamar and Eric Horvitz},
  title         = {Understanding Failures of Deep Networks via Robust Feature Extraction},
  year          = {2021},
  __markedentry = {[lilli:1]},
  archiveprefix = {arXiv},
  eprint        = {2012.01750},
  file          = {:Singla2021 - Understanding Failures of Deep Networks via Robust Feature Extraction.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank3},
  primaryclass  = {cs.CV},
}

@InProceedings{Gu2019,
  author    = {Gu, Jindong and Yang, Yinchong and Tresp, Volker},
  title     = {Understanding Individual Decisions of CNNs via Contrastive Backpropagation},
  booktitle = {Computer Vision -- ACCV 2018},
  year      = {2019},
  editor    = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
  pages     = {119--134},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.},
  isbn      = {978-3-030-20893-6},
}

@Article{Bau2020,
  author        = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  title         = {Understanding the role of individual units in a deep neural network},
  journal       = {Proceedings of the National Academy of Sciences},
  year          = {2020},
  volume        = {117},
  number        = {48},
  pages         = {30071-30078},
  issn          = {0027-8424},
  __markedentry = {[lilli:2]},
  abstract      = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
  doi           = {10.1073/pnas.1907375117},
  elocation-id  = {201907375},
  file          = {:Bau2020 - Understanding the role of individual units in a deep neural network.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  publisher     = {National Academy of Sciences},
  url           = {https://www.pnas.org/content/early/2020/08/31/1907375117},
}

@Article{Lapuschkin2019,
  author        = {Sebastian Lapuschkin and Stephan W\"{a}ldchen and Alexander Binder and Gr{\'{e}}goire Montavon and Wojciech Samek and Klaus-Robert M\"{u}ller},
  title         = {Unmasking Clever Hans predictors and assessing what machines really learn},
  journal       = {Nature Communications},
  year          = {2019},
  volume        = {10},
  number        = {1},
  month         = mar,
  __markedentry = {[lilli:1]},
  doi           = {10.1038/s41467-019-08987-4},
  groups        = {relevant},
  keywords      = {rank4},
  publisher     = {Springer Science and Business Media {LLC}},
  url           = {https://www.nature.com/articles/s41467-019-08987-4},
}

@Article{Tran2022,
  author        = {Tran, Thien Q and Fukuchi, Kazuto and Akimoto, Youhei and Sakuma, Jun},
  title         = {Unsupervised Causal Binary Concepts Discovery with VAE for Black-Box Model Explanation},
  journal       = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year          = {2022},
  volume        = {36},
  number        = {9},
  pages         = {9614-9622},
  month         = {Jun.},
  __markedentry = {[lilli:1]},
  abstract      = {We aim to explain a black-box classifier with the form: &quot;data X is classified as class Y because X has A, B and does not have C&quot; in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user’s prior knowledge to induce high interpretability of concepts. Finally, using multiple datasets, we demonstrate that the proposed method can discover useful concepts for explanation in this form.},
  doi           = {10.1609/aaai.v36i9.21195},
  file          = {:Tran2022 - Unsupervised Causal Binary Concepts Discovery with VAE for Black-Box Model Explanation.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://ojs.aaai.org/index.php/AAAI/article/view/21195},
}

@Article{Chalupka2016a,
  author      = {Krzysztof Chalupka and Tobias Bischoff and Pietro Perona and Frederick Eberhardt},
  title       = {Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data},
  abstract    = {We show that the climate phenomena of El Nino and La Nina arise naturally as states of macro-variables when our recent causal feature learning framework (Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind (ZW) and sea surface temperatures (SST) taken over the equatorial band of the Pacific Ocean. The method identifies these unusual climate states on the basis of the relation between ZW and SST patterns without any input about past occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering the SST fields while disregarding their relationship with ZW patterns, or (ii) clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches. Finally, we propose a new robust and scalable alternative to our original algorithm (Chalupka 2016), which circumvents the need for high-dimensional density learning.},
  date        = {2016-05-30},
  eprint      = {1605.09370v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Chalupka2016a - Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data.pdf:PDF;:bib/1605.09370v1.pdf:PDF},
  keywords    = {stat.ML, cs.AI, cs.LG, physics.ao-ph, rank1},
}

@Article{Kretschmer2016,
  author    = {Marlene Kretschmer and Dim Coumou and Jonathan F. Donges and Jakob Runge},
  title     = {Using Causal Effect Networks to Analyze Different Arctic Drivers of Midlatitude Winter Circulation},
  journal   = {Journal of Climate},
  year      = {2016},
  volume    = {29},
  number    = {11},
  pages     = {4069--4081},
  month     = {jun},
  address   = {Boston MA, USA},
  doi       = {10.1175/jcli-d-15-0654.1},
  file      = {:Kretschmer2016 - Using Causal Effect Networks to Analyze Different Arctic Drivers of Midlatitude Winter Circulation.pdf:PDF},
  groups    = {relevant},
  keywords  = {rank2},
  publisher = {American Meteorological Society},
  url       = {https://journals.ametsoc.org/view/journals/clim/29/11/jcli-d-15-0654.1.xml},
}

@InProceedings{Reimers2019,
  author        = {Reimers, Christian and Runge, Jakob and Denzler, Joachim},
  title         = {Using Causal Inference to Globally Understand Black Box Predictors beyond Saliency Maps},
  booktitle     = {International Workshop on Climate Informatics (CI)},
  year          = {2019},
  __markedentry = {[lilli:1]},
  abstract      = {State-of-the-art machine learning methods, which predictions can not be easily verified and, there-
especially deep neural networks, have reached impressive fore, we need domain experts to determine whether
results in many prediction and classification tasks. Rising a predicted values makes sense, for example, climate
complexity and automatic feature selection make the science.
resulting learned models hard to interpret and turns them
into black boxes. Advances into feature visualization have We will discuss some works that aim to make deep},
  file          = {:Reimers2019 - Using Causal Inference to Globally Understand Black Box Predictors beyond Saliency Maps.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
}

@Misc{Cristali2022,
  author        = {Irina Cristali and Victor Veitch},
  title         = {Using Embeddings for Causal Estimation of Peer Influence in Social Networks},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.08033},
  file          = {:Cristali2022 - Using Embeddings for Causal Estimation of Peer Influence in Social Networks.pdf:PDF},
  groups        = {Application Fields},
  keywords      = {rank1},
  primaryclass  = {cs.SI},
  url           = {https://arxiv.org/abs/2205.08033},
}

@InProceedings{Zintgraf2017,
  author    = {Zintgraf, Luisa and Cohen, Taco and Adel, Tameem and Welling, Max},
  title     = {Visualising Deep Neural Network Decisions: Prediction Difference Analysis},
  booktitle = {International Conference on Learning Representations},
  year      = {2017},
  month     = {04},
}

@Article{Zeiler2013,
  author     = {Matthew D. Zeiler and Rob Fergus},
  title      = {Visualizing and Understanding Convolutional Networks},
  journal    = {CoRR},
  year       = {2013},
  volume     = {abs/1311.2901},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/ZeilerF13.bib},
  eprint     = {1311.2901},
  eprinttype = {arXiv},
  timestamp  = {Mon, 13 Aug 2018 16:48:37 +0200},
  url        = {http://arxiv.org/abs/1311.2901},
}

@Article{Deng2014,
  author    = {Yi Deng and Imme Ebert-Uphoff},
  title     = {Weakening of atmospheric information flow in a warming climate in the Community Climate System Model},
  journal   = {Geophysical Research Letters},
  year      = {2014},
  volume    = {41},
  number    = {1},
  pages     = {193--200},
  month     = {jan},
  doi       = {10.1002/2013gl058646},
  file      = {:Deng2014 - Weakening of atmospheric information flow in a warming climate in the Community Climate System Model.pdf:PDF},
  keywords  = {rank1},
  publisher = {American Geophysical Union ({AGU})},
}

@InProceedings{Brehmer2022,
  author   = {Johann Brehmer and Pim de Haan and Qualcomm AI and Research† Qualcomm and AI Research},
  title    = {Weakly supervised causal representation learning},
  year     = {2022},
  file     = {:Brehmer2022 - Weakly supervised causal representation learning.pdf:PDF},
  groups   = {Recommended, relevant},
  keywords = {rank4},
}

@InProceedings{Leemann2023,
  author        = {Leemann, Tobias and Kirchhof, Michael and Rong, Yao and Kasneci, Enkelejda and Kasneci, Gjergji},
  title         = {When are post-hoc conceptual explanations identifiable?},
  booktitle     = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  year          = {2023},
  editor        = {Evans, Robin J. and Shpitser, Ilya},
  volume        = {216},
  series        = {Proceedings of Machine Learning Research},
  pages         = {1207--1218},
  month         = {31 Jul--04 Aug},
  publisher     = {PMLR},
  __markedentry = {[lilli:3]},
  abstract      = {Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts under non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outperform competitors on a battery of experiments including hundreds of trained models and dependent concepts, where they exhibit up to 29 % better alignment with the ground truth. Our results highlight the strict conditions under which reliable concept discovery without human labels can be guaranteed and provide a formal foundation for the domain. Our code is available online.},
  file          = {:Leemann2023 - When are Post-hoc Conceptual Explanations Identifiable_.pdf:PDF;leemann23a.pdf:https\://proceedings.mlr.press/v216/leemann23a/leemann23a.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  url           = {https://proceedings.mlr.press/v216/leemann23a.html},
}

@Article{Luo2020,
  author   = {Yunan Luo and Jian Peng and Jianzhu Ma},
  title    = {When causal inference meets deep learning},
  journal  = {Nature Machine Intelligence},
  year     = {2020},
  volume   = {2},
  pages    = {426 - 427},
  keywords = {rank1},
  url      = {https://www.nature.com/articles/s42256-020-0218-x},
}

@InProceedings{Sixt2020,
  author        = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
  title         = {When Explanations Lie: Why Many Modified BP Attributions Fail},
  booktitle     = {Proceedings of the 37th International Conference on Machine Learning},
  year          = {2020},
  series        = {ICML'20},
  publisher     = {JMLR.org},
  __markedentry = {[lilli:3]},
  abstract      = {Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.},
  articleno     = {839},
  file          = {:Sixt2020 - When Explanations Lie_ Why Many Modified BP Attributions Fail.pdf:PDF},
  groups        = {relevant},
  keywords      = {rank5},
  numpages      = {12},
  primaryclass  = {cs.LG},
}

@InProceedings{Simic2021,
  author   = {Ilija Šimić and 1 Vedran and Sabol and Eduardo Veas and 2},
  title    = {XAI Methods for Neural Time Series Classification: A Brief Review},
  year     = {2021},
  abstract = {might have a substantial impact on the well-being of a
person, as well as considerable financial and legal conse-},
  file     = {:Simic2021 - XAI Methods for Neural Time Series Classification_ A Brief Review.pdf:PDF},
  groups   = {Recommended, relevant},
  keywords = {rank4},
}

@Article{Clark2023,
  author        = {Clark, Benedict and Wilming, Rick and Haufe, Stefan},
  title         = {XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance},
  journal       = {ArXiv},
  year          = {2023},
  __markedentry = {[lilli:4]},
  file          = {:Clark2023 - XAI-TRIS_ Non-linear benchmarks to quantify ML explanation performance.pdf:PDF},
  keywords      = {rank5},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:/home/lilli/Desktop/MASTER/bib;}

@Comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Application Fields\;0\;;
1 ExplicitGroup:socialclimate\;0\;;
1 ExplicitGroup:Recommended\;0\;;
1 ExplicitGroup:relevant\;0\;;
}

@Comment{jabref-meta: saveOrderConfig:specified;title;false;year;false;author;false;}
