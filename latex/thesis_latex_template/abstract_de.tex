% ---------------------------------------------------
% ----- Abstract (German) of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing. 
%
\pagestyle{empty}

\subsection*{Zusammenfassung}
Methoden der erklärbaren künstlichen Intelligenz (Explainable Artificial Intelligence, XAI), insbesondere lokale Wichtigkeitszuordnung, sind ein beliebtes Instrument zur Visualisierung der Bedeutung von Eingangsmerkmalen, wie zum Beispiel Pixeln, für ein neuronales Netz.
Folglich gibt es eine wachsende Zahl von Arbeiten, die diese Wichtigkeitszuordnung und ihre Visualisierung sowie im speziellen konzeptbasierte Methoden genauer evaluieren und ihre Grenzen aufzeigen. In diesem Bereich nimmt die Anwendung von Kausalen Inferenzmethoden und Effektabschätzungen zu, da Erklärungen ein inhärent kausales Konstrukt sind: Wenn die Entscheidung eines Modells erklärt werden soll, ist das gleichzusetzen damit, die kausalen Ursachen dieser Entscheidung zu identifizieren und zu quantifizieren. Die Evaluation von XAI mit kausalen Methoden wird häufig von künstlich generierten Datensätzen, in denen die Generierungsvariablen bekannt sind, begleitet, um die tatsächliche Funktionsweise eines neuronalen Netzes ermitteln zu können. 
Die neuere Erklärungsmethode Konzeptbedingte Relevanzpropagation (Concept Relevance Propagation, CRP) scheint, basierend auf einer kleinen Nutzerstudie und qualitativen Untersuchungen, den Menschen besser als reine Saliency-Visualisierungen (oder Wärmebildkarten) in die Lage zu versetzen, zu erkennen, ob ein Modell unerwünschte Korrelationen gelernt hat. Bisher wurde dieses Potenzial aber nicht quantitativ evaluiert, vermutlich, weil die relative Wichtigkeit von unerwünschten Merkmalen schwerer formal zu testen ist als die grundsätzliche Nähe einer Erklärung zum Verhalten eines trainierten Modells. 
Mit Hilfe eines kausal generierten Benchmark-Datensatzes untersuchen wir daher die Reaktion von CRP auf eine Intervention innerhalb eines kausalen Modells. 
Unser kausaler Prozess führt eine Scheinkorrelation ein, mit einem Merkmal das keine direkte Beziehung zum wirklich wichtigen Merkmal hat, sondern durch eine Störgröße mit ihm gekoppelt ist. Indem wir die Stärke dieser Kopplung kontinuierlich erhöhen, können wir analysieren, ob die Reaktion der Erklärung auf ein solches falsch korreliertes Merkmal der tatsächlichen Reaktion des Modells folgt. Zu diesem Zweck konstruieren wir eine Reihe von Metriken, die den Einfluss einer solchen Verzerrung sowohl auf die Erklärung als auch auf das Modell quantifizieren. Wir stellen fest, dass der gemessene kausale Effekt auf die Entscheidung des Modells und die Erklärung zum großen Teil übereinstimmen. Je abstrakter und menschlich interpretierbarer wir jedoch die Wichtigkeit eines unerwünscht korrelierten Merkmals innerhalb der Erklärung messen, desto weniger genau folgt sie der tatsächlichen Wichtigkeit für das Modell. 
Einerseits unterstreichen unsere Experimente und Ergebnisse die Erkenntnis, dass eine Übereinstimmung mit der Datenverteilung im Datensatz nicht mit der Korrektheit gegenüber einem trainierten Modell verwechselt werden sollte. Andererseits zeigen sie, dass die Wichtigkeit eines Merkmals innerhalb einer Erklärung stark davon abhängt, was gemessen wird und welche Annahmen über das menschliche Verständnis und die Definition der Erklärung getroffen werden.


