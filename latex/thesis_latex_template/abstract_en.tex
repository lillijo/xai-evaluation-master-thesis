\pagestyle{empty}
\subsection*{Abstract}


Explainable Artificial Intelligence (XAI) methods, specifically local attribution methods, are a popular tool to visualize the importance of input features for a neural network.
Consequently, there is a growing corpus of work evaluating saliency map and concept-based methods rigorously and finding best practices as well as limitations and disadvantages. In this realm the application of the causal inference and estimation framework has gained traction as explanations are an intrinsically causal concept: When a model ought to be explained, the goal is to identify and quantify the causes that led to its decisions. The evaluation of XAI with causal methods is often accompanied by artificially constructed benchmark datasets with known ground truth, to be able to establish the true workings of a neural network. 
The recent explanation method Concept Relevance Propagation (CRP) seems to enable humans better than pure heatmaps to discover whether and how strongly a model is biased based on a small user study and qualitative investigations. So far, this potential has not been evaluated quantitatively, presumably since the fidelity of relative feature importance is not as straight-forward to quantify as the general fidelity towards a model. 
With the help of a causal benchmark dataset we thus investigate the reaction of CRP to an intervention on a causal model constructing the dataset and guiding the experiment. Our causal process introduces a spurious feature which has no direct relationship with the truly important feature but is coupled through a confounding variable. By intervening on their coupling ratio, we can analyze whether the reaction to such a spuriously correlated feature of the explanation follows the model's true importance. For this, we construct a set of measures which quantify the influence of such a bias on both the model and the explanation. We find the measured causal effect on the models predictions and explanation to mostly coincide. However, the more abstracted and human-interpretable we measure the explanation importance, the less closely it follows the true importance. 
On the one hand our experiments and results emphasize the view that coherence to the data distribution should not be confused with correctness of an explanation towards a trained model. On the other, they show that a feature's importance within an explanation strongly depends on how the explanation itself is defined and what assumptions about human understanding of the explanation are made.
