\chapter{Experimental Results}\label{chapter:results}

{ \color{red}
10-20 pages 

    \begin{itemize}
        \item (1/3 of thesis)
        \item whatever you have done, you must comment it, compare it to other systems, evaluate it
        \item usually, adequate graphs help to show the benefits of your approach
        \item caution: each result/graph must be discussed! what's the reason for this peak or why have you observed this effect
    \end{itemize}
}

\section{The Ground Truth Trained Models}
Here we report the general results of training 816 models with 16 different random initializations and 51 different values of $\rho$ (in 0.02 steps from 0 to 1).
The details on the model architecture and training process can be found in the \cref{appendix:model}. 
As already noted in the method section, the model is fairly small but still has high performance for the simple benchmark data we test it on. 

\begin{itemize}
    \item trained on 
\end{itemize}


\begin{figure}
    \centering
    \includegraphics{thesis_latex_template/pics/basic_accuracy.png}
    \caption[Accuracy]{Mean accuracy for models with the same coupling ratio $\rho$ (16 per bucket)}
    \label{fig:basic_accuracy}
\end{figure}


\begin{figure}
    \centering
    \includegraphics{thesis_latex_template/pics/accuracy_intervened.png}
    \caption[Accuracy for intervened Subsets]{Accuracy for models with the same coupling ratio $\rho$ (16 per bucket). As expected, for ellipses without and rectangles with the watermark, the accuracy drops as $\rho$ raises.}
    \label{fig:basic_accuracy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/unnormalized_some_measures.png}
    \caption{Comparison of Measures Unnormalized}
    \label{fig:unnormalized}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/normalized_some_measures.png}
    \caption{Comparison of Measures Normalized to [0,1]}
    \label{fig:normalized}
\end{figure}


Interesting things i wanna say
\begin{itemize}
    \item Firstly: The reaction to the "bias" is way less strong than one might expect. Only at super high rates it actually becomes worth it to learn the spurious feature.
    \item However, that might be expected as that is what deep neural networks are specifically designed for
    \item in general, it is relieving to see that the importance of the biased feature for the model and the explanation is not completely departing from each other
    \item as the models feature importance rises with $\rho$, so does the explanation importance
    \item but it becomes clear that depending on the way the explanation is read / measured it can over- or under-emphasize the wm effect. 
    \item the seed makes a huge difference (maybe this is just because my model is not stable and it has to be recomputed???) 
    \item 
\end{itemize}

\section{Experiments}
\begin{itemize}
    \item what have I tried out with the different methods?
    \item list in concise order the possible measures
    \item ground-truth feature importance: mean logit change for output, phi correlation (= prediction flip)
    \item baseline explanation feature importance - thats what we compare to e.g. watermark bounding-box importance for summary heatmap
    \item special concept explanations feature importance
    \item how are the experiments set up, how do i make sure they are all well comparable
    \item to which other baseline could my measures be compared to?
\end{itemize}

\section{Results}
lots of plots!
\begin{itemize}
    \item what have I tried out with the different methods?
    \item what works and what doesn't
    \item plot for each experiment/possible method?
          \begin{itemize}
              \item watermark bounding box average relevance for different subgroups, somehow get difference
              \item variance of latent factors in relevance maximization image set - low variance means it encodes the concept
              \item naive: total relevance for watermark image region
              \item total activation + relevance of neuron given just watermark image
              \item one idea: take masked/bounding box approach again for neurons individual heatmaps
              \item nmf idea: somehow try to reduce the latent space to Watermark/Shape axis and measure variance in either direction
              \item centroids idea: use random DR algorithm and calculate ratio of centroid distances (needs latent factors again)
              \item causal idea??? somehow measure causal effect? - the other things are kind of causal or?
          \end{itemize}
\end{itemize}



\section{Evaluation}
\begin{itemize}
    \item evaluation of evaluation criteria:
    \begin{itemize}
        \item takes into consideration the whole latent space spanned by the concepts
        \item orients itself on known human cognition, user studies in this field would suggest this??? 
        \item performs similar to baseline watermark bounding box importance? 
        \item ...? \todo{define success criteria of finding a good measure}
    \end{itemize}
    \item which measure is the best according to those criteria
    \item which measure is the closest to ground truth
    \item which is the furthest from ground truth
    \item does measure find \textit{more information} than CRP itself and could possibly be used as a method on top of CRP for disentanglement/ spurious-core relation explanation?
    \item 
\end{itemize}


\section{Evaluating Method on Other Problems}\label{section:further_problems}

\section{Discussion}
\begin{itemize}
    \item do measures work
    \item what does causality help us with
    \item is CRP better for constant vector shift stuff or does it still suffer from it?
    \item can the application of those measures further explain/inform the explanation?
    \item what failed miserably
\end{itemize}
