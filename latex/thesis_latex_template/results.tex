\chapter{Experimental Results}\label{chapter:results}

{ \color{red}
10-20 pages 

    \begin{itemize}
        \item (1/3 of thesis)
        \item whatever you have done, you must comment it, compare it to other systems, evaluate it
        \item usually, adequate graphs help to show the benefits of your approach
        \item caution: each result/graph must be discussed! what's the reason for this peak or why have you observed this effect
    \end{itemize}
}

\section{Trained Models}
Here we report the general results of training 816 models with 16 different random initializations and 51 different values of $\rho$ (in 0.02 steps from 0 to 1).
The details on the model architecture and training process can be found in the \cref{appendix:model}. 
As already noted in the method section, the model architecture is fairly minimal but still has high performance for the simple benchmark problems we test it on. 
For the shape vs. watermark scenario the average performance is above 95\% and seems to not be dependent on the coupling ration $\rho$ as is visible in \cref{fig:basic_accuracy}.
% Almost identical results are achieved for the non-localized and suppressor variable benchmark problems.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{thesis_latex_template/pics/basic_accuracy.png}
    \caption[Accuracy]{Average accuracy for models with the same coupling ratio $\rho$ (16 per bucket)}
    \label{fig:basic_accuracy}
\end{figure}


A first impression on how much the coupling ratio (i.e. the biasedness of the model) affects the performance for instances that go against this bias can be seen in \cref{fig:accuracy_intervened}. We will in the following examine this \textit{ground truth importance} of the model in more detail with the measures $m_1$ discussed in the method \cref{section:gt_measure}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{thesis_latex_template/pics/accuracy_intervened.png}
    \caption[Accuracy for intervened Subsets]{Average accuracy for the 16 models per coupling ratio $\rho$. As expected, for ellipses without and rectangles with the watermark, the accuracy drops as $\rho$ raises.}
    \label{fig:accuracy_intervened}
\end{figure}

Like expected, the spurious feature becomes more important to the model as its coupling with the target feature gets stronger. Both the mean logit change and the prediction flip (i.e. $\phi$-coefficient) reflect this (\cref{fig:m1_results}). It is interesting to note the variance of importance increasing the stronger the coupling gets. While all models seem to learn to ignore the spurious feature up until a correlation in the data distribution of about 0.7, some continue to mostly dismiss it as the correlation gets even stronger, while others start using the watermark for prediction. Therefore we found it important to also show each set of models trained with the same random initialization separately (\cref{fig:gt_over_seeds}). We speculate this robustness to spurious features some instances of initial weights and biases seem to display to be, because the local minimum concentrating around the spurious feature is far from the initial point in the gradient function. It certainly is a phenomenon worth studying and multiple authors have set to explicitly de-biasing networks using e.g. adapted cost functions, adversarial methods or human-in-the-loop correction \cite{Anders2022,Pahde2023,Reimers2021, Reimers2021b, Dreyer2023a}.

A second noteworthy observation one can make from this individual view is that some seeds seem to be more unstable in performance than others (i.e. 1, 3, 6, 14). Nevertheless no relationship between this instability and the models reaction to the spurious feature is apparent so we do not remove these seeds from our analysis.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/m1_phi_mlc.png}
    \caption[Ground Truth Importance $m_1$]{Ground Truth Importance of watermark feature $W$, plotted against coupling ratio $\rho$.}
    \label{fig:m1_results}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/compare_seeds_w_accuracy.png}
    \caption[Comparing Seeds]{Mean Logit Change when intervening on watermark over 16 random seeds (red)
    compared to $\rho$ (green dashed), correlation in data distribution (blue dash-dotted) and training accuracy (gray dotted).
    }
    \label{fig:gt_over_seeds}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/unnormalized_some_measures.png}
    \caption{Comparison of Measures Unnormalized}
    \label{fig:unnormalized}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{thesis_latex_template/pics/normalized_some_measures.png}
    \caption{Comparison of Measures Normalized to [0,1]}
    \label{fig:normalized}
\end{figure}


Interesting things i wanna say
\begin{itemize}
    \item Firstly: The reaction to the "bias" is way less strong than one might expect. Only at super high rates it actually becomes worth it to learn the spurious feature.
    \item However, that might be expected as that is what deep neural networks are specifically designed for
    \item in general, it is relieving to see that the importance of the biased feature for the model and the explanation is not completely departing from each other
    \item as the models feature importance rises with $\rho$, so does the explanation importance
    \item but it becomes clear that depending on the way the explanation is read / measured it can over- or under-emphasize the wm effect. 
    \item the seed makes a huge difference (maybe this is just because my model is not stable and it has to be recomputed???) 
    \item 
\end{itemize}

\section{Experiments}
\begin{itemize}
    \item what have I tried out with the different methods?
    \item list in concise order the possible measures
    \item ground-truth feature importance: mean logit change for output, phi correlation (= prediction flip)
    \item baseline explanation feature importance - thats what we compare to e.g. watermark bounding-box importance for summary heatmap
    \item special concept explanations feature importance
    \item how are the experiments set up, how do i make sure they are all well comparable
    \item to which other baseline could my measures be compared to?
\end{itemize}

\section{Results}
lots of plots!
\begin{itemize}
    \item what have I tried out with the different methods?
    \item what works and what doesn't
    \item plot for each experiment/possible method?
          \begin{itemize}
              \item watermark bounding box average relevance for different subgroups, somehow get difference
              \item variance of latent factors in relevance maximization image set - low variance means it encodes the concept
              \item naive: total relevance for watermark image region
              \item total activation + relevance of neuron given just watermark image
              \item one idea: take masked/bounding box approach again for neurons individual heatmaps
              \item nmf idea: somehow try to reduce the latent space to Watermark/Shape axis and measure variance in either direction
              \item centroids idea: use random DR algorithm and calculate ratio of centroid distances (needs latent factors again)
              \item causal idea??? somehow measure causal effect? - the other things are kind of causal or?
          \end{itemize}
\end{itemize}



\section{Evaluation}
\begin{itemize}
    \item evaluation of evaluation criteria:
    \begin{itemize}
        \item takes into consideration the whole latent space spanned by the concepts
        \item orients itself on known human cognition, user studies in this field would suggest this??? 
        \item performs similar to baseline watermark bounding box importance? 
        \item ...? \todo{define success criteria of finding a good measure}
    \end{itemize}
    \item which measure is the best according to those criteria
    \item which measure is the closest to ground truth
    \item which is the furthest from ground truth
    \item does measure find \textit{more information} than CRP itself and could possibly be used as a method on top of CRP for disentanglement/ spurious-core relation explanation?
    \item 
\end{itemize}


\section{Evaluating Method on Other Problems}\label{section:further_problems}

\section{Discussion}
\begin{itemize}
    \item do measures work
    \item what does causality help us with
    \item is CRP better for constant vector shift stuff or does it still suffer from it?
    \item can the application of those measures further explain/inform the explanation?
    \item what failed miserably
\end{itemize}
