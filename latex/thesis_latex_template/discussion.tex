\chapter{Discussion}\label{chapter:discussion}
In the following we discuss how our measures evaluate an explanation's fidelity to trained neural networks. This discussion includes both evaluating how well the measures embody the actual explanation as well as how close their values are to the true model importance. 

\section{Fidelity of Relative Feature Importance}
We recall the question central to our experiments:
\begin{quote}
    \textit{Is the relative feature importance of a spurious feature explained by CRP in correspondence with the true importance assigned to it by the model or is it more closely aligned to the associations within the data distribution?}
\end{quote}

In summary our results show that the relative feature importance explained by CRP follows the model's true importance more closely than the linear coupling of the data distribution. As the models feature importance rises non-linearly to $\rho$, so does the explanation importance for most measures. From the experiments we have conducted, we can conclude that \textit{we are explaining the model and not the data}. This result might not be surprising for the concept-based method CRP which deterministically depends on the prediction of the model. But it relates to the criticism of \cite{Nauta2023} towards recent XAI evaluation work, which often conflates plausibility of an explanation with its correctness. If the explained importance was more akin to the data distribution, i.e., estimated the spurious feature just as strongly as it is present in the data, this might be plausible, but not a correct explanation of the inner workings of the model. 
We want to stress again that in this thesis we did not claim to find one, unified method for evaluating how explanation methods react to spurious features. Instead, we adapted existing and constructed new ways to measure relative feature importance for the high-dimensional concept-based explanation given by CRP. This approach emphasizes the multitude of quantifiable criteria for an explanation's correctness. There is not \textit{one way} of measuring relative feature importance in an explanation, because there is not only one criterion for a good explanation. 
Not surprisingly, fidelity varies depending on the complexity of the measurement and the kind of spurious feature (watermark or pattern) to be evaluated. Overall, our experiments show that depending on how the relative spurious feature importance is defined, one can come to different conclusions about the explanation's fidelity. 

Attempting to measure the fidelity of relative feature importance in this way also highlights the weakness of quantitative evaluation approaches. 
Although we have defined a rigorous causal framework to 
measure true feature importance, finally the explained feature importance depends on how we define what is part of the explanation and what humans interpret from it. Such a definition then enables a quantitative evaluation but without user studies we can not confirm the suitability of this definition. Evidently, interpreting how CRP explains the feature importance can even differ from case to case, like the watermark and pattern benchmarks studied here show. 
We are hence missing information on the causal pathways from the explanation output to the human brain. To measure how well CRP uncovers the true workings of a model, one would need to fully understand these pathways of how humans perceive concept-based explanations.

\section{Fidelity, Complexity and Interpretability of Explanations Seen Through Measures}
Our analysis suggests that there is a trade-off between how closely the effect on the explanation follows the effect on the models and the complexity of measurement. 

\subsection{Relevance Vectors and Attribution Maps}
When we measure the pixel- or neuron-wise differences, the effect of the intervention on our spurious feature is most accurately measured in both scenarios. 
The causal effect on the relevance vectors unsurprisingly follows the causal effect on the output vectors most closely of all measures. Because the attribution onto the neurons within one layer is determined by the output values, they inevitably have a strong relationship. The attribution maps  have higher dimensionality (here, 8x64x64), which is most likely why noise and rounding errors slightly inflate the causal effect measured on them but overall they also follow the ground truth well.   
In the watermark scenario the spatial separation of spurious and core feature still somewhat enables an identification of which feature is more important when looking at attribution maps. Adding to that, when the watermark is missing, no relevance is usually assigned to the area, so the images without watermark act as a clean baseline for measuring the causal effect. 

For the pattern scenario the pixel- or neuron-wise measurements still have relatively high fidelity to the ground truth importance. 
This is mostly because a concept that encodes, for example, blurryness will have high relevance within the shape when the shape is blurry and low relevance otherwise. Hence, the effect of the intervention is high. Despite that, whether neurons encode blurry or noisy is not visible from the attribution maps which misleadingly emphasize the shape for either. Because both features overlap, the attribution maps can not convey whether the pattern or the shape are used by the model at all.

The relevances and their attribution maps do also to a degree overestimate the pattern's importance for low values of $\rho$ and underestimate it for high values. We suppose that this is as a result of the strong entanglement between shape and pattern. Changing the pixels within the shape to a different pattern also has an effect on neurons which only encode the shape and not its pattern. Vice versa, for high $\rho$ the difference between the two values of the pattern $W$ is not maximal, in comparison to the other scenario where the watermark is completely missing when $W=0$. 

We hypothesized that using relevance maximization sets to show which concept is actually encoded by a neuron would help measure the spurious feature importance in a more interpretable way. Yet, our results on these further expose how important proper disentanglement of features is for this task. 

\subsection{Region-Specific Measures}
As laid out before, the complete measured effect on relevances and attribution maps is not necessarily interpretable by humans. In the watermark case, the intensity of attribution pixels within differently sized features is difficult to compare. For this the region-specific measures were studied as a potential remedy. 
Although they do follow the general trend of the ground truth for the watermark case, they do so less closely. 
$\RMA$ and $\RRA$ underestimate the importance of the watermark, because some relevance (be it positive or negative) is still assigned to the shape.
\citeauthor{Arras2022} have been questioning whether the score of 1 for $\RMA$ is attainable or even desirable. Especially in the setting where two features are present this is a valid question to pose. We suppose that the relevance within the bounding box should still follow the ground truth importance as closely as possible for the watermark, i.e., spatially separated, scenario. 

The pointing game method on the other hand overestimates the importance of the watermark at all times. This is most likely owing to its reduced complexity, it however represents an important finding with regard to the concept-based explanation: it means that some neurons which have non-zero relevance do indeed early on attribute the watermark highly. Whether these are ``nonsensical'' neurons, i.e., ones that have not learned any recognizable feature, or whether they actually encode the watermark can not be concluded with this low-complexity measurement. 

When we compare the weighted sum of neurons versus the most relevant neuron for the watermark scenario, this effect becomes more clear: While the other metrics underestimate the spurious feature importance more when all neurons are taken into account, the pointing game method overestimates the importance more. 
This implies that uninformative or random neurons have enough relevance assigned within the watermark boundary to have an effect on the explained importance of the spurious feature. 

Interestingly, the region specific measures still show some effect of intervention for high values of $\rho$ in the \textit{pattern} scenario. 
Our conclusion is that while all encoded concepts relevance lies mostly within the boundary of the shape for this case, the relevance of individual neurons still differs strongly. So a neuron that encodes the blurred pattern will assign high relevance to pixels within a blurred shape and low relevance to the same shape boundary with noisy pattern. 
Another possible explanation is less desirable: For some examples we found that attribution maps seemed to assign high relevance to pixels outside of the shape, when the pattern feature was not ''matching'' the neurons concept. Through further inspection we assume the artifact to occur, for example, when \textit{noisy} concepts attribute the noisy background and \textit{blurry} neurons do not fire at all. This highly misleading artifact then ''positively'' influences the measured region-specific importance. Relevance Mass Accuracy is also overestimated when the pattern feature has not actually been learned, presumably again due to the entanglement of shape and pattern.
In contrast to $\RMA$ and $\RRA$ the binary measure does not perform well on the pattern benchmark. Here, the most highly relevant pixel usually lies within the shape, even though the effect can be measured for the overall image. 

To further study the applicability of region-specific measures, we not only looked at the weighted aggregation over all concepts within a layer, but also examined the difference between the top neuron for the image where $W=1$ to the one image where $W=0$. This approach shows, why region-specific measure are misleading for scenarios like our pattern benchmark, where the spurious and the target feature are overlapping. In such cases, the majority of the relevance will always lie within the boundary irrespective of which pattern is present in the given instance for the most relevant neuron. The attribution maps of the top neurons for an image therefore do not differ noticeably depending on which pattern is present and it is not possible to gauge the patterns importance from them. 

\subsection{Prototype Reference Sets}
Because we expected exactly this to happen for the pattern case, we constructed another measure using the relevance maximization reference sets. We furthermore propose that the prototypical explanation they embody is most in line with desiderata like \textit{interpretability} and \textit{compactness}, also due to the confirmation from the small subject study of \cite{Achtibat2023}.
The general, i.e., not class-specific, measure achieves comparable results to the other measures for both scenarios, although it seems to be more noisy. This is likely due to the small size of the reference sets (15 images). 
In contrast to that, the class-specific measure is not working as expected for relevance maximization. We suspect that this has to do with the way relevance sets are selected. Because the shape and the pattern feature are so deeply entangled for high values of $\rho$, a concept encoding, e.g., the blurry pattern, will still put rectangles in the context they are most often seen in, which is with a noisy pattern. An example of such an artifact can be seen in \cref{fig:class_reference_sets}. But even more generally the class-specific reference sets seem to not encode the spurious feature well. They are more likely to include images with varying values of $W$, even when the effect on the relevance of that neuron suggests that the spurious feature is most important for the respective neuron.  

Another experiment we conducted for the reference sets compares relevance with activation maximization. Instead of using the ratio of images having a certain value for the spurious feature to the total number of prototypes, we include the distribution of the shape feature. As described, relevance maximization aims to put the concept within the context it is most often used in, which creates undesirable results in this scenario. We note that even though this entanglement is preventing interpretability in the given example when importance is measured in this particular way, it might still give insights and be desirable in other applications.
Activation maximization on the other hand works well for this measure, because the diversity in shape is higher for the reference sets. Thereby it is possible to conclude that a concept encodes a pattern regardless of which shape the pattern is found on.
We therefore propose that when using prototypical examples one would actually need to look at both the relevance and activation sets, yet again bloating up the complexity of the explanation. 
Of course, the strong entanglement and lack of diversity in the reference sets studied here might be less pronounced for more complex realistic image datasets. However, we postulate that for more complex datasets the disentanglement of features is harder and even less interpretable as \cite{Traeuble2021} have found.

\section{Ground-Truth Relative Feature Importance}
Although the ground-truth relative feature importance is merely a side-product necessary for our analysis, we want to comment on some of the observations we have made about it. 
In other work about explaining relative feature importance we have not found thorough descriptions of the ground-truth dependency of models on the coupling of a spurious feature. Even if the observed robustness to spurious features can be expected for neural networks due to their non-linearities, it is still interesting to see how it differs depending on the kind of spurious feature.
It is also noteworthy that the importance seems to go through a \textit{state change} at the point where learning the spurious feature becomes easier than learning the core feature. From that point on, the model seems to only depend on the spurious features value. 
Nonetheless, even before this state change a slow raise of importance is visible. Our assumption is that there are two (or even three) strategies the models apply:
When the watermark is only weakly biased, the model completely ignores its value. The stronger the bias gets, the more the relative feature importance of the spurious feature is mixed with the core feature. In the last step the core feature is completely ignored. 
For realistic applications as \citeauthor{Achtibat2022}'s case of a real watermark, its explained relevance is often significantly below the relevance of target features. One might also add that the prevalence of spurious features like watermarks is likely lower and not limited to only one class but the core feature likely orders of magnitude more difficult to learn than the watermark in realistic settings.
We therefore hypothesize that many real biased problems are located in the second stage, where both spurious and core features are important to some degree.
While it is not possible to differentiate these strategies even for the ground-truth importance, we consider their demarcation in explanations an important direction for further research.
If our measures indeed prove to be a reliable way to yield explained feature importance through further experiments, they could possible act as a hint towards which state the model is in with regards to feature importance. For this to work in real applications it would be necessary to extract well disentangled features in an automatic way, which is an unsolved problem. 


\section{Limitations of the Experiment and Future Work}
All in all, the causal framework we have constructed is apt for the comparison of ground truth feature importance and explained feature importance. The causal model for the data generation represents one of many causal pathways for how spurious features can get introduced in realistic image datasets. Its simple structure makes the analysis straight-forward, while producing a equivalent data distribution as many other realistic causal models one can think of.
A possible limitation for the comparability with realistic image data is the simplicity of the shape images. Additionally, only differentiating two classes, where one is positively and the other negatively associated with the spurious feature is a drastic reduction. We are not convinced that it is still in line with realistic datasets where only few of many classes have watermarks.
In the future, additional experiments with varying, more realistic, datasets are necessary to confirm the applicability of our results to real cases. The difficulty of this undertaking lies in the lack of ground-truth features for such data. While human labelling is expensive, causal experiments using automatically extracted variables (e.g. by Variational Autoencoders) have been conducted by others \citep{Reimers2020, Goyal2019, Tran2022}. Despite that, work on finding and unlearning spurious features has shown that their discovery and disentanglement from core features is an unsolved task especially for less localizable features \citep{Dreyer2023a}.

Besides more realistic datasets, variants of the causal model should also be studied more extensively. The suppressor variable model of \cite{Clark2023} and \cite{Wilming2022} would lend itself as a first candidate, as it has already been studied in a similar setting but without comparing the explained feature importance to the ground truth feature importance of the model. Likewise, our hypothesis that neural networks are not able to differentiate selection bias from confounder bias owing to their equivalent data distribution needs to be tested. 
Furthermore, instead of only analyzing CRP it would be interesting to also compare it to either its general form LRP or to other concept-based methods. 

On another note, we were not fully successful in setting all measures in relation to each other. It is not clear whether it is reasonable to compare outcomes from high-dimensional measures like the mean attribution change, with binary measures like the pointing game. As the explanation method CRP is not explicitly relating the spurious to the core feature and we do not have a ground truth of what 100 percent of attribution ought to look like, interpreting measures as percentages was not always possible. The experiment we have conducted on the spurious feature should therefore be repeated with all other latent factors as well as the core feature in its full extent. A glimpse of what results might be can be found in the preliminary experiment in \cref{appendix:dsprites}.


\section{Conclusion}
This thesis analyzed the fidelity of explained relative feature importance for a spurious feature. 
Our goal was to establish ways to measure the relative feature importance conveyed by the concept-based explanation method CRP. This goal was inspired by the lack of quantitative evaluation for concept-based methods and in particular CRP. Especially their potential to convey relative feature importance had been so far only studied qualitatively. 
Towards this aim we devised a causal framework of how a spurious feature can be coupled with a core feature in a data distribution. This enabled us to assess the ground-truth reaction of an array of neural networks to such a coupling. Having yielded this ground-truth, we were able to compare it to the measured relative feature importance within the explanation. 
Overall, we found the concept-based explanation to react to our intervention on the spurious feature similarly to how the model reacts. This is evidence that the explanation indeed follows what the model has learned and is not merely a reflection of the used data's distribution. 
How well the model importance and explanation importance coincide, however, strongly depends on how one interprets what is part of the explanation and one measures it.
On a curve from most complete to most interpretable, the explanations fidelity with regard to the spurious feature's importance suffers the less complex the explanation is defined. While it is possible to construct a formalized causal framework for the computation of spurious feature importance to the model, this stringent approach is limited due to the informal definition of how a \textit{good} explanation should look. Nevertheless, the candidates we have constructed are if not a complete at least a diverse set of possibilities for how relative feature importance can be measured in concept-based explanations.


