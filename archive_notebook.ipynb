{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404105c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_to_strings(d):\n",
    "    newmetadata = {}\n",
    "    for k,v in d.items():\n",
    "        newk,newv = k,v\n",
    "        if isinstance(v, bytes):\n",
    "            newv = v.decode(\"utf-8\") \n",
    "        if isinstance(v, dict):\n",
    "            newv = decode_to_strings(v)\n",
    "        if isinstance(k, bytes):\n",
    "            newk = k.decode(\"utf-8\") \n",
    "        newmetadata[newk] = newv\n",
    "    return newmetadata\n",
    "\n",
    "with np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',encoding='latin1', allow_pickle=True) as data:\n",
    "    labels = data['latents_values']\n",
    "    with open('latents_values.pickle', 'wb') as handle:\n",
    "        pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622333",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dsprites.pickle', 'rb') as f:\n",
    "    pdata = pickle.load(f)\n",
    "    fname_template='dsprites-dataset/images/{cap}.npy'\n",
    "    for i in range(len(pdata[\"dataset\"])):\n",
    "        with open(fname_template.format(cap=i) , 'wb') as f:\n",
    "            np.save(f, pdata[\"dataset\"][i] )\n",
    "    with open('labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(pdata[\"labels\"], handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#im, la = next(dataiter)\n",
    "\n",
    "def draw_img(index, images, labels):\n",
    "    images = images.view((BATCH_SIZE, 1,64,64))\n",
    "    sample = images[index].view(1,1,64,64)\n",
    "    model.train()\n",
    "    pred = model(sample)\n",
    "    model.eval()\n",
    "    img_grid = torchvision.utils.make_grid(sample, nrow=16)\n",
    "    print(f\"({pred.tolist()}, {labels[index]})\" )\n",
    "    matplotlib_imshow(img_grid, one_channel=True)\n",
    "    \n",
    "#draw_img(5, im, la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img):\n",
    "    img = img[0]\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.view((BATCH_SIZE, 1,64,64))\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images, nrow=16)\n",
    "matplotlib_imshow(img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25bc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeConvolutionalNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=8, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e00294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# big network for orientation\n",
    "class OrientationConvolutionalNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OrientationConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Conv2d(32, 32, kernel_size=8, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(1600, 160),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(160, 40),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small network for orientation\n",
    "class OrientationConvolutionalNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OrientationConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=8, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Conv2d(16, 16, kernel_size=8, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(64, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef91c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for orientation (40 classes)\n",
    "\n",
    "class DSpritesImageDataset(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.img_dir = \"dsprites-dataset/images/\"\n",
    "        with open('labels.pickle', 'rb') as f:\n",
    "            labels = pickle.load(f)\n",
    "            self.labels = labels\n",
    "        #dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',encoding='bytes', allow_pickle=True)\n",
    "        #self.metadata = dataset_zip['metadata'][()]\n",
    "        #self.latents_values = dataset_zip['latents_values']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, f'{index}.npy')\n",
    "        image = np.load(img_path)\n",
    "        image = torch.from_numpy(np.asarray(image, dtype=np.float32)).view(1,64,64)\n",
    "        target = self.labels[index][3]\n",
    "        # use actual numerical value instead\n",
    "        # target = float(metadata[\"latents_possible_values\"][\"orientation\"][target])\n",
    "        # use one-hot-encoding\n",
    "        # target = one_hot(target)\n",
    "        if not self.train:\n",
    "            return (image, target, index)\n",
    "        return (image, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find class distribution\n",
    "cond_layer = 'linear_layers.2'\n",
    "labelcount = []\n",
    "for blub in tqdm(range(100)):\n",
    "    dataiter = iter(training_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.view((BATCH_SIZE, 1,64,64))\n",
    "    output = model(images)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    preds = pred.flatten().tolist()\n",
    "    res = [f'{preds[i]}_{labels[i]}' for i in range(BATCH_SIZE)]\n",
    "    labelcount += res\n",
    "collections.Counter(labelcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca67243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if biased \n",
    "is_biased = []\n",
    "for k, data in tqdm(enumerate(training_loader)):\n",
    "    inputs, labels, indices = data\n",
    "    latents = dsprites_dataset_test.labels\n",
    "    is_biased += [latents[i][4] < 16 and latents[i][5] < 16 for i in indices]\n",
    "print(collections.Counter(is_biased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335e1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210c4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
